<!doctype html>
<html class="theme-next use-motion ">
<head>
  

<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>


<script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
<link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet">
<style>
    .pace .pace-progress {
        background: #1E92FB; /*进度条颜色*/
        height: 3px;
    }
    .pace .pace-progress-inner {
         box-shadow: 0 0 10px #1E92FB, 0 0 5px     #1E92FB; /*阴影颜色*/
    }
    .pace .pace-activity {
        border-top-color: #1E92FB;    /*上边框颜色*/
        border-left-color: #1E92FB;    /*左边框颜色*/
    }
</style>




<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />








  <link rel="stylesheet" type="text/css" href="/others/fancybox/source/jquery.fancybox.css?v=2.1.5"/>






  <link href="/vendors/googleapis/css/Lato.css" rel="stylesheet" type="text/css">




<link rel="stylesheet" type="text/css" href="/others/font-awesome/css/font-awesome.min.css?v=4.4.0" />

<link rel="stylesheet" type="text/css" href="/css/main.css?v=0.4.5.2"/>


    <meta name="description" content="向上，向阳！" />



  <meta name="keywords" content="Machine Learning,Deeping Learning,job," />





  <link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico?v=0.4.5.2" />


<meta name="description" content="code{white-space: pre;}">
<meta name="keywords" content="Machine Learning,Deeping Learning,job">
<meta property="og:type" content="article">
<meta property="og:title" content="加油！一定能找到好工作！">
<meta property="og:url" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/index.html">
<meta property="og:site_name" content="SmileLingyong">
<meta property="og:description" content="code{white-space: pre;}">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/blue.jpg">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/TIM截图20180817192259.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/TIM截图20180817214034.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/2.16.4.1.jpg">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/2.16.4.2.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/2.16.4.3.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/TIM截图20180608171710.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/TIM截图20180608172312.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/k-折交叉验证.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/precision_recall.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/mAP.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/result.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/sigmod_01.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/sigmod_02.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/tanh_01.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/tanh_02.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/ReLU_01.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/ReLU_02.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/Leaky_ReLU_01.png">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=y-f%28x%29%3D0">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/maxresdefault.jpg">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/范数.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/TIM截图20180710112848.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/常见的核函数.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/预剪枝.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/后剪枝.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/2-5.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/条件独立.jpg">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/全概率公式.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/贝叶斯公式.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/2.19.1.1.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/k-means.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/K-means.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/层次聚类.jpg">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/LDA.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/2.20.1.jpg">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/梯度下降.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/梯度检验.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/BN.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/BN_02.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/GN.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/BN_croped.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/2.jpg">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/3.jpg">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/4.jpg">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/ResNet1.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/ResNet2.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/ResNet_other.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/ResNet3.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/ResNet4.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/ResNet5.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/caffe_conv_02.jpg">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/caffe_conv_01.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/Layer.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/CUDA架构_01.jpg">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/CUDA架构_02.jpg">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/Kernel线程层级结构.jpg">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/2D卷积.jpg">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/转置卷积3.jpeg">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/转置卷积4.jpeg">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/转置卷积1.gif">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/转置卷积2.gif">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/标准离散卷积.gif">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/扩张卷积.gif">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/扩张卷积2.jpeg">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/分组卷积.jpg">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/分组卷积2.jpg">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/深度可分离卷积.jpg">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/深度可分离卷积_0.jpg">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/感受野.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/mean_pooling.jpeg">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/max_pooling.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/roi_max_pooling1.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/roi_max_pooling2.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/NMS.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/KMP.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/红黑树.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/二叉树定义1.jpg">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/二叉树定义2.jpg">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/二叉排序树1.jpg">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/二叉排序树插入.jpg">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/二叉排序树删除.jpg">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/二叉排序树查找效率.jpg">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/平衡二叉树.jpg">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/平衡二叉树插入.jpg">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/平衡二叉树查找+哈夫曼树.jpg">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/哈夫曼树.jpg">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/哈夫曼编码.jpg">
<meta property="og:image" content="http://simtalk.cn/img/%E7%89%B9%E5%BE%81%E4%B8%8E%E5%8C%B9%E9%85%8D/corner.png">
<meta property="og:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/Trie.png">
<meta property="og:updated_time" content="2019-09-14T12:48:39.165Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="加油！一定能找到好工作！">
<meta name="twitter:description" content="code{white-space: pre;}">
<meta name="twitter:image" content="http://yoursite.com/2018/04/07/Try-your-best!You-will-find-the-job/blue.jpg">


<script type="text/javascript" id="hexo.configuration">
  var CONFIG = {
    scheme: '',
    sidebar: 'always'
  };
</script>



  <title> 加油！一定能找到好工作！ | SmileLingyong </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="">

  <!--[if lte IE 8]>
  <div style=' clear: both; height: 59px; padding:0 0 0 15px; position: relative;margin:0 auto;'>
    <a href="http://windows.microsoft.com/en-US/internet-explorer/products/ie/home?ocid=ie6_countdown_bannercode">
      <img src="http://7u2nvr.com1.z0.glb.clouddn.com/picouterie.jpg" border="0" height="42" width="820"
           alt="You are using an outdated browser. For a faster, safer browsing experience, upgrade for free today or use other browser ,like chrome firefox safari."
           style='margin-left:auto;margin-right:auto;display: block;'/>
    </a>
  </div>
<![endif]-->
  






  <div id="container" class="container one-column page-post-detail">

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  
  
    <ul id="menu" class="menu ">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-user"></i> <br />
            
            About
          </a>
        </li>
      

      
      
	  	<span style="font-size:14px;float:right;padding:39px 40px 0 0;">——穷则独善其身，达则兼济天下.</span>
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">

        	<div id="content" class="content"> 

  <div id="posts" class="posts-expand">
    

  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                加油！一定能找到好工作！
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            Posted on
            <time itemprop="dateCreated" datetime="2018-04-07T23:59:59+08:00" content="2018-04-07">
              2018-04-07 23:59
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp; In
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/ML/" itemprop="url" rel="index">
                    <span itemprop="name">ML</span>
                  </a>
                </span>

                
                
                  , 
                

              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/DL/" itemprop="url" rel="index">
                    <span itemprop="name">DL</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
            <span id="/2018/04/07/Try-your-best!You-will-find-the-job/"class="leancloud_visitors"  data-flag-title="加油！一定能找到好工作！">
            &nbsp; | &nbsp;   
            views
            </span>
          
        </div>
      </header>
    


    <div class="post-body">

      
      

      
        <span itemprop="articleBody"><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <meta http-equiv="Content-Style-Type" content="text/css">
  <meta name="generator" content="pandoc">
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pygments-css@1.0.0/github.min.css" type="text/css">
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
</head>
<body>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/blue.jpg">

</div>
<a id="more"></a>
<h3 id="模型评估">模型评估</h3>
<h4 id="模型评估常用方法">模型评估常用方法</h4>
<p>一般情况来说，单一评分标准无法完全评估一个机器学习模型。只用good和bad偏离真实场景去评估某个模型，都是一种欠妥的评估方式。下面介绍常用的分类模型和回归模型评估方法。</p>
<p><strong>分类模型常用评估方法：</strong></p>
<table>
<thead>
<tr class="header">
<th align="center">指标</th>
<th align="center">描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Accuracy</td>
<td align="center">准确率</td>
</tr>
<tr class="even">
<td align="center">Precision</td>
<td align="center">精准度/查准率</td>
</tr>
<tr class="odd">
<td align="center">Recall</td>
<td align="center">召回率/查全率</td>
</tr>
<tr class="even">
<td align="center">P-R曲线</td>
<td align="center">查准率为纵轴，查全率为横轴，作图</td>
</tr>
<tr class="odd">
<td align="center">F1</td>
<td align="center">F1值</td>
</tr>
<tr class="even">
<td align="center">Confusion Matrix</td>
<td align="center">混淆矩阵</td>
</tr>
<tr class="odd">
<td align="center">ROC</td>
<td align="center">ROC曲线</td>
</tr>
<tr class="even">
<td align="center">AUC</td>
<td align="center">ROC曲线下的面积</td>
</tr>
</tbody>
</table>
<p>F1 是基于查准率与查全率的调和平均(harmonic mean)定义的： <span class="math inline">\(\frac{1}{F 1}=\frac{1}{2} \cdot\left(\frac{1}{P}+\frac{1}{R}\right)\)</span><br>
<span class="math display">\[
F 1=\frac{2 \times P \times R}{P+R}
 = =\frac{2 \times TP }{样例总数 + TP - TN}
\]</span><br>
用于综合考虑查准率、查全率的性能度量。</p>
<p><strong>回归模型常用评估方法：</strong></p>
<table>
<thead>
<tr class="header">
<th align="center">指标</th>
<th align="center">描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Mean Square Error (MSE, RMSE)</td>
<td align="center">均方误差</td>
</tr>
<tr class="even">
<td align="center">Absolute Error (MAE, RAE)</td>
<td align="center">绝对误差</td>
</tr>
<tr class="odd">
<td align="center">R-Squared</td>
<td align="center">R平方值</td>
</tr>
</tbody>
</table>
<h4 id="机器学习中的bias和variance有什么区别和联系">机器学习中的Bias和Variance有什么区别和联系</h4>
<p><strong>Bias(偏差)</strong>与<strong>Variance(方差)</strong>分别是用于衡量一个模型<strong>泛化误差</strong>的两个方面<br>
- <strong>Bias(偏差)：</strong> 指的是模型预测的<strong>期望值</strong>与<strong>真实值</strong>之间的差（描述模型的<strong>拟合能力</strong>）<br>
- <strong>Variance(方差)：</strong> 指的是模型预测的<strong>期望值</strong>与<strong>预测值</strong>之间的差的平方和（描述模型的<strong>稳定性</strong>）</p>
<ul>
<li>在<strong>监督学习</strong>中，模型的<strong>泛化误差</strong>可<strong>分解</strong>为偏差、方差与噪声之和。</li>
</ul>
<p><span class="math display">\[
Err(x) = Bias^2 + Variance + Irreducible\ Error
\]</span></p>
<ul>
<li><strong>噪声：</strong>表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度</li>
</ul>
<p><img src="/2018/04/07/Try-your-best!You-will-find-the-job/TIM截图20180817192259.png" height></p>
<h4 id="经验误差与泛化误差">经验误差与泛化误差</h4>
<ul>
<li><strong>经验误差</strong>（empirical error）：也叫训练误差（training error），模型在训练集上的误差。</li>
<li><strong>泛化误差</strong>（generalization error）：模型在新样本集（测试集）上的误差。</li>
</ul>
<h4 id="深度学习中的偏差与方差">深度学习中的偏差与方差</h4>
<ul>
<li>神经网络的拟合能力非常强，因此它的<strong>训练误差</strong>（偏差）通常较小；</li>
<li>但是过强的拟合能力会导致较大的方差，使模型的测试误差（泛化误差）增大；</li>
<li>因此深度学习的核心工作之一就是研究如何降低模型的泛化误差，这类方法统称为<strong>正则化方法</strong>。</li>
</ul>
<h4 id="偏差方差与boosting和bagging联系"><a href="#集成学习">偏差方差与Boosting和Bagging联系</a></h4>
<ol style="list-style-type: decimal">
<li><strong>Boosting</strong> 能提升弱分类器性能的原因是降低了<strong>偏差</strong></li>
<li><strong>Bagging</strong> 则是降低了<strong>方差</strong></li>
</ol>
<ul>
<li><strong>Boosting</strong> 方法：</li>
<li>Boosting 的<strong>基本思路</strong>就是在不断减小模型的<strong>训练误差</strong>（拟合残差或者加大错类的权重），加强模型的学习能力，从而减小偏差；</li>
<li>但 Boosting 不会显著降低方差，因为其训练过程中各基学习器是强相关的，缺少独立性。</li>
<li><strong>Bagging</strong> 方法：</li>
<li>对 <code>n</code> 个<strong>独立不相关的模型</strong>预测结果取平均，方差是原来的 <code>1/n</code>；</li>
<li>假设所有基分类器出错的概率是独立的，<strong>超过半数</strong>基分类器出错的概率会随着基分类器的数量增加而下降。</li>
</ul>
<h4 id="偏差与方差的计算公式">偏差与方差的计算公式</h4>
<ul>
<li><p>记在<strong>训练集 D</strong> 上学得的模型为<br>
<span class="math display">\[
  f(\boldsymbol{x} ; D)
  \]</span><br>
模型的<strong>期望预测</strong>为<br>
<span class="math display">\[
  \hat{f}(\boldsymbol{x})=\mathbb{E}_{D}[f(\boldsymbol{x} ; D)]
  \]</span></p></li>
<li><p><strong>偏差</strong>（Bias）<br>
<span class="math display">\[
  \operatorname{bias}^{2}(\boldsymbol{x})=(\hat{f}(\boldsymbol{x})-y)^{2}
  \]</span></p></li>
</ul>
<blockquote>
<p><strong>偏差</strong>度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力；</p>
</blockquote>
<ul>
<li><strong>方差</strong>（Variance）<br>
<span class="math display">\[
  \operatorname{var}(\boldsymbol{x})=\mathbb{E}_{D}\left[(f(\boldsymbol{x} ; D)-\hat{f}(\boldsymbol{x}))^{2}\right]
  \]</span></li>
</ul>
<blockquote>
<p><strong>方差</strong>度量了同样大小的<strong>训练集的变动</strong>所导致的学习性能的变化，即刻画了数据扰动所造成的影响（模型的稳定性）；</p>
</blockquote>
<ul>
<li><strong>噪声</strong><br>
<span class="math display">\[
  \varepsilon^{2}=\mathbb{E}_{D}\left[\left(y_{D}-y\right)^{2}\right]
  \]</span></li>
</ul>
<blockquote>
<p><strong>噪声</strong>则表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度。</p>
</blockquote>
<ul>
<li>“<strong>偏差-方差分解</strong>”表明模型的泛化能力是由算法的能力、数据的充分性、任务本身的难度共同决定的</li>
</ul>
<h4 id="偏差与方差的权衡过拟合与模型复杂度的权衡">偏差与方差的权衡（过拟合与模型复杂度的权衡）</h4>
<ul>
<li>给定学习任务，</li>
<li>当训练不足时，模型的<strong>拟合能力不够</strong>（数据的扰动不足以使模型产生显著的变化），此时<strong>偏差</strong>主导模型的泛化误差；</li>
<li>随着训练的进行，模型的<strong>拟合能力增强</strong>（模型能够学习数据发生的扰动），此时<strong>方差</strong>逐渐主导模型的泛化误差；</li>
<li>当训练充足后，模型的<strong>拟合能力过强</strong>（数据的轻微扰动都会导致模型产生显著的变化），此时即发生<strong>过拟合</strong>（训练数据自身的、非全局的特征也被模型学习了）</li>
<li>偏差和方差的关系和<strong>模型容量</strong>（模型复杂度）、<strong>欠拟合</strong>和<strong>过拟合</strong>的概念紧密相联<br>
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/TIM截图20180817214034.png" height></li>
<li>当模型的容量增大（x 轴）时， 偏差（用点表示）随之减小，而方差（虚线）随之增大</li>
<li>沿着 x 轴存在<strong>最佳容量</strong>，<strong>小于最佳容量会呈现欠拟合</strong>，<strong>大于最佳容量会导致过拟合</strong>。</li>
</ul>
<h4 id="欠拟合与过拟合">欠拟合与过拟合</h4>
<ul>
<li><strong>欠拟合</strong>指模型不能在<strong>训练集</strong>上获得足够低的<strong>训练误差</strong></li>
<li><strong>过拟合</strong>指模型的<strong>训练误差</strong>与<strong>测试误差</strong>（泛化误差）之间差距过大</li>
<li>反映在<strong>评价指标</strong>上，就是模型在训练集上表现良好，但是在测试集和新数据上表现一般（<strong>泛化能力差</strong>）</li>
</ul>
<hr>
<h4 id="根据不同的坐标方式图解欠拟合与过拟合">根据不同的坐标方式，图解欠拟合与过拟合</h4>
<ol style="list-style-type: decimal">
<li><strong>横轴为训练样本数量，纵轴为误差</strong></li>
</ol>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/2.16.4.1.jpg">

</div>
<ul>
<li>模型欠拟合：在训练集以及测试集上同时具有较高的误差，此时模型的偏差较大；</li>
<li>模型过拟合：在训练集上具有较低的误差，在测试集上具有较高的误差，此时模型的方差较大。</li>
<li>模型正常：在训练集以及测试集上，同时具有相对较低的偏差以及方差。</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li><strong>横轴为模型复杂程度，纵轴为误差</strong></li>
</ol>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/2.16.4.2.png">

</div>
<p>​ 红线为测试集上的Error, 蓝线为训练集上的Error</p>
<ul>
<li>模型欠拟合：模型在点A处，在训练集以及测试集上同时具有较高的误差，此时模型的偏差较大。</li>
<li>模型过拟合：模型在点C处，在训练集上具有较低的误差，在测试集上具有较高的误差，此时模型的方差较大。</li>
<li>模型正常：模型复杂程度控制在点B处为最优。</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li><strong>横轴为正则项系数，纵轴为误差</strong></li>
</ol>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/2.16.4.3.png">

</div>
<p>​ 红线为测试集上的Error,蓝线为训练集上的Error</p>
<ul>
<li>模型欠拟合：模型在点C处，在训练集以及测试集上同时具有较高的误差，此时模型的偏差较大。</li>
<li>模型过拟合：模型在点A处，在训练集上具有较低的误差，在测试集上具有较高的误差，此时模型的方差较大。</li>
<li>模型正常：模型复杂程度控制在点B处为最优。</li>
</ul>
<h4 id="降低-过拟合-的方法">降低 过拟合 的方法</h4>
<ul>
<li><strong>数据增强</strong></li>
<li>图像：平移、旋转、缩放</li>
<li>利用<strong>生成对抗网络</strong>（GAN）生成新数据</li>
<li>NLP：利用机器翻译生成新数据</li>
<li><strong>增加正则化项</strong>（权值约束）</li>
<li>L1 正则化</li>
<li>L2 正则化</li>
<li><strong>增大正则化项系数</strong></li>
<li><strong>降低模型复杂度</strong></li>
<li>神经网络：减少网络层、神经元个数</li>
<li>决策树：降低树的深度、剪枝</li>
<li><strong>采用Dropout方法</strong></li>
<li>Dropout方法，通俗的讲就是在训练的时候让神经元以一定的概率不工作</li>
<li><strong>提前终止 early stopping</strong></li>
<li><strong>集成学习</strong></li>
<li>神经网络：Dropout</li>
<li>决策树：随机森林、GBDT</li>
</ul>
<h4 id="降低-欠拟合-的方法">降低 欠拟合 的方法</h4>
<ul>
<li><strong>加入新的特征</strong></li>
<li>交叉特征、多项式特征、…</li>
<li>深度学习：因子分解机、Deep-Crossing、自编码器</li>
<li><strong>增加模型复杂度</strong></li>
<li>线性模型：添加高次项</li>
<li>神经网络：增加网络层数、神经元个数</li>
<li><strong>减小正则化项的系数</strong></li>
<li>添加正则化项是为了限制模型的学习能力，减小正则化项的系数则可以放宽这个限制</li>
<li>模型通常更倾向于更大的权重，更大的权重可以使模型更好的拟合数据</li>
</ul>
<h4 id="l1l2-范数正则化">L1/L2 范数正则化</h4>
<h4 id="l1l2-范数的作用异同">L1/L2 范数的作用、异同</h4>
<p><strong>相同点</strong></p>
<ul>
<li>限制模型的学习能力——通过限制参数的规模，使模型偏好于<strong>权值较小</strong>的目标函数，防止过拟合。</li>
</ul>
<p><strong>不同点</strong></p>
<ul>
<li><strong>L1 正则化</strong>可以产生更<strong>稀疏</strong>的权值矩阵，可以用于特征选择，同时一定程度上防止过拟合；<strong>L2 正则化</strong>主要用于防止模型过拟合</li>
<li><strong>L1 正则化</strong>适用于特征之间有关联的情况；<strong>L2 正则化</strong>适用于特征之间没有关联的情况。</li>
</ul>
<h4 id="为什么-l1-和-l2-正则化可以防止过拟合">为什么 L1 和 L2 正则化可以防止过拟合？</h4>
<ul>
<li>L1 &amp; L2 正则化会使模型偏好于更小的权值。</li>
<li>更小的权值意味着<strong>更低的模型复杂度</strong>；添加 L1 &amp; L2 正则化相当于为模型添加了某种<strong>先验</strong>，限制了参数的分布，从而降低了模型的复杂度。</li>
<li>模型的复杂度降低，意味着模型对于噪声与异常点的抗干扰性的能力增强，从而提高模型的泛化能力。——直观来说，就是对训练数据的拟合刚刚好，不会过分拟合训练数据（比如异常点，噪声）——<strong>奥卡姆剃刀原理</strong></li>
</ul>
<h4 id="为什么-l1-正则化可以产生稀疏权值而-l2-不会">为什么 L1 正则化可以产生稀疏权值，而 L2 不会？</h4>
<ul>
<li><p>对目标函数添加范数正则化，训练时相当于在范数的约束下求目标函数 <code>J</code> 的最小值</p></li>
<li><p>带有<strong>L1 范数</strong>（左）和<strong>L2 范数</strong>（右）约束的二维图示</p></li>
</ul>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/TIM截图20180608171710.png">

</div>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/TIM截图20180608172312.png">

</div>
<ul>
<li>图中 <code>J</code> 与 <code>L1</code> 首次相交的点即是最优解。<code>L1</code> 在和每个坐标轴相交的地方都会有“<strong>顶点</strong>”出现，多维的情况下，这些顶点会更多；在顶点的位置就会产生稀疏的解。而 <code>J</code> 与这些“顶点”相交的机会远大于其他点，因此 <code>L1</code> 正则化会产生稀疏的解。</li>
<li><code>L2</code> 不会产生“<strong>顶点</strong>”，因此 <code>J</code> 与 <code>L2</code> 相交的点具有稀疏性的概率就会变得非常小。</li>
</ul>
<h4 id="交叉验证的主要作用">交叉验证的主要作用</h4>
<p>为了得到更为稳健可靠的模型，使用验证集对模型的泛化误差进行评估，得到模型泛化误差的近似值。当有多个模型可以选择时，我们通常选择“泛化误差”最小的模型。</p>
<p>常用的交叉验证方法：<strong>留一交叉验证</strong>、<strong>k折交叉验证</strong></p>
<h4 id="k-折交叉验证"><span class="math inline">\(k\)</span> 折交叉验证</h4>
<p><strong><span class="math inline">\(k\)</span> 折交叉验证：</strong>将含有 <span class="math inline">\(N\)</span> 个样本的数据集，分成 <span class="math inline">\(k\)</span> 份，每份含有 <span class="math inline">\(N/k\)</span> 个样本。选择其中1份作为测试集，另外 <span class="math inline">\(k-1\)</span> 份作为训练集。这样就可以获得 <span class="math inline">\(K\)</span> 组训练/测试集，从而可以进行 <span class="math inline">\(k\)</span> 次训练和测试，最终返回这 <span class="math inline">\(k\)</span> 个测试结果的均值，做为模型最终的泛化误差。一般 <span class="math inline">\(2 \leq k \leq 10\)</span> ，<span class="math inline">\(k\)</span> 最常用的取值是 10，此时称为<strong>10折交叉验证</strong>：</p>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/k-折交叉验证.png">

</div>
<p><strong>10次10折交叉验证：</strong> 则是如上重复做了10次，每次的10折交叉验证随机使用不同的划分。</p>
<blockquote>
<p>训练集和测试集必须从完整的数据集中均匀取样。均匀取样的目的是希望减少训练集、测试集与原数据集之间的偏差。当样本数量足够多时，通过随机取样，便可以实现均匀取样的效果。</p>
</blockquote>
<h4 id="precision和recall">Precision和Recall</h4>
<p>对于二分类问题，可将样例 根据其真实类别与学习器预测类别的组合划分为：</p>
<ul>
<li><strong>TP</strong> (True Positive)： 预测为真，实际为真</li>
<li><strong>FP</strong> (False Positive)： 预测为真，实际为假</li>
<li><strong>TN </strong>(True Negative)： 预测为假，实际为假</li>
<li><strong>FN</strong> (False Negative)：预测为假，实际为真</li>
</ul>
<p>令 TP、FP、TN、FN分别表示其对应的样例数，则显然有 <strong>TP + FP + TN + FN = 样例总数</strong>分类结果的 <strong>“混淆矩阵”</strong> 如下：</p>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center">实际为真 T</th>
<th align="center">实际为假 F</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>预测为正例 P</strong></td>
<td align="center"><strong>TP</strong> (预测为1，实际为1)</td>
<td align="center"><strong>FP</strong> (预测为1，实际为0)</td>
</tr>
<tr class="even">
<td align="center"><strong>预测为负例 N</strong></td>
<td align="center"><strong>FN</strong> (预测为0，实际为1)</td>
<td align="center"><strong>TN</strong> (预测为0，实际为0)</td>
</tr>
</tbody>
</table>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/precision_recall.png">

</div>
<p><span class="math display">\[
（查准率）Precision = \frac{TP}{TP  + FP} \\ \\ \\
{\color{Purple}{（预测的好瓜中有多少是真的好瓜）}}
\]</span></p>
<p><span class="math display">\[
（查全率）Recall  = \frac{TP}{TP + FN} \\ \\
{\color{Purple}{（所有真正的好瓜中有多少被真的挑出来了）}}
\]</span></p>
<h4 id="p-r曲线">P-R曲线</h4>
<p>一般来说，查准率高时，查全率往往偏低，而查全率高时，查准率往往偏低。通常只有在一些简单任务中，才可能使得查全率和查准率都很高。在很多情况，我们可以根据学习器的预测结果，得到对应预测的 confidence scores 得分(有多大的概率是正例)，按照得分对样例进行排序，排在前面的是学习器认为”最可能“是正例的样本，排在最后的则是学习器认为“最不可能”是正例的样本。每次选择当前第 <span class="math inline">\(i\)</span> 个样例的得分作为阈值 <span class="math inline">\((1 \leq i \leq 样例个数)\)</span>，计算当前预测的前 <span class="math inline">\(i\)</span> 为正例的查全率和查准率。然后以<strong>查全率为横坐标</strong>，<strong>查准率为纵坐标</strong>作图，就得到了我们的查准率-查全率曲线: <strong>P-R曲线</strong></p>
<h4 id="roc与auc">ROC与AUC</h4>
<p><strong>ROC</strong> 全称是“受试者工作特征”（Receiver Operating Characteristic）。ROC 曲线下的面积就是 <strong>AUC</strong>（Area Under the Curve）。AUC用于衡量“二分类问题”机器学习算法性能（泛化能力）。</p>
<blockquote>
<p>思想：和计算 P-R 曲线方法基本一致，只是这里计算的是 真正率(True Positive rate) 和 假正率(False Positive rate)，以 FPR 为横轴，TPR 为纵轴，绘制的曲线就是 ROC 曲线，ROC 曲线下的面积，即为 AUC</p>
</blockquote>
<p><span class="math display">\[
（真正率）TPR = \frac{TP}{TP + FN}
\]</span></p>
<p><span class="math display">\[
（假正率）FPR = \frac{FP}{FP + TN}
\]</span></p>
<h4 id="map">mAP</h4>
<p>接下来说说 <strong>AP</strong> 的计算，此处参考的是 <code>PASCAL  VOC  CHALLENGE</code> 的计算方法。首先按照 P-R曲线 计算方式计算 Precision 和 Recall。然后设定一组阈值，[0, 0.1, 0.2, …, 1]，计算 Recall 大于第 <span class="math inline">\(i\)</span> 个阈值的R-P集合中，对应的 Precision 的最大值。这样，我们就计算出了11个 Precision。<strong>mAP</strong> 即为这11个 Precision 的平均值。这种方法英文叫做 <code>11-point interpolated average precision</code><br>
相应的Precision-Recall曲线（这条曲线是单调递减的）如下：</p>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/mAP.png">

</div>
<p><strong>AP</strong> 衡量的是学出来的模型在每个类别上的好坏，<strong>mAP</strong> 衡量的是学出的模型在所有类别上的好坏，得到 AP 后 mAP 的计算就变得很简单了，就是取所有 AP 的平均值。</p>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/result.png">

</div>
<h4 id="为什么要设置单一数字评估指标设置指标的意义">为什么要设置单一数字评估指标，设置指标的意义</h4>
<p>在训练模型时，无论是调整超参数，还是调整不同的模型算法，我们都需要一个有效的评价指标，这个评价标准能帮助我们快速了解新的尝试后模型的性能是否更优。例如在分类时，我们通常会选择选择准确率，当样本不平衡时，查准率和查全率又会是更好的评价指标。所以在训练模型时，如果设置了单一数字的评估指标通常能很快的反应出我们模型的改进是否直接产生了收益，从而加速我们的算法改进过程。若在训练过程中，发现优化目标进一步深入，现有指标无法完全反应进一步的目标时，就需要重新选择评估指标了。</p>
<h4 id="训练验证测试集的定义及划分">训练/验证/测试集的定义及划分</h4>
<p>训练、验证、测试集在机器学习领域是非常重要的三个内容。三者共同组成了整个项目的性能的上限和走向。</p>
<ul>
<li><strong>训练集</strong>：用于模型训练的样本集合，样本占用量是最大的；</li>
<li><strong>验证集</strong>：用于训练过程中的模型性能评价，跟着性能评价才能更好的调参；</li>
<li><strong>测试集</strong>：用于最终模型的一次最终评价，直接反应了模型的性能。</li>
</ul>
<p>在划分上，可以分两种情况：</p>
<p>1、在样本量有限的情况下，有时候会把验证集和测试集合并。实际中，若划分为三类，那么训练集：验证集：测试集=6:2:2；若是两类，则训练集：验证集=7:3。这里需要主要在数据量不够多的情况，验证集和测试集需要占的数据比例比较多，以充分了解模型的泛化性。</p>
<p>2、在海量样本的情况下，这种情况在目前深度学习中会比较常见。此时由于数据量巨大，我们不需要将过多的数据用于验证和测试集。例如拥有1百万样本时，我们按训练集：验证集：测试集=98:1:1的比例划分，1%的验证和1%的测试集都已经拥有了1万个样本。这已足够验证模型性能了。</p>
<p>此外，三个数据集的划分不是一次就可以的，若调试过程中发现，三者得到的性能评价差异很大时，可以重新划分以确定是数据集划分的问题导致还是由模型本身导致的。其次，若评价指标发生变化，而导致模型性能差异在三者上很大时，同样可重新划分确认排除数据问题，以方便进一步的优化。</p>
<h4 id="什么是top5错误率">什么是TOP5错误率</h4>
<p>在很多情况，我们可以根据学习器的预测结果，得到对应预测的 confidence scores 得分(有多大的概率是正例)，按照得分对样例进行排序，取得分最高的前5个，计算这前5个都不是正例的概率，即为TOP5错误率。</p>
<h4 id="如何通过模型重新观察数据">如何通过模型重新观察数据</h4>
<p>通过模型分析错误数据，推断是否存在错误标注或者漏标注等问题。此外，对于出现一些过拟合的情况，我们也可以通过观察来了解模型。例如分类任务，样本严重不平衡时，模型全预测到了一边时，其正确率仍然很高，但显然模型已经出现了问题。</p>
<h4 id="有哪些改善模型的思路">有哪些改善模型的思路</h4>
<p>改善模型本质是如何优化模型，这本身是个很宽泛的问题。也是目前学界一直探索的目的，而从目前常规的手段上来说，一般可取如下几点。</p>
<ol style="list-style-type: decimal">
<li><strong>数据角度</strong></li>
</ol>
<p>增强数据集。无论是有监督还是无监督学习，数据永远是最重要的驱动力。更多的类型数据对良好的模型能带来更好的稳定性和对未知数据的可预见性。对模型来说，“看到过的总比没看到的更具有判别的信心”。但增大数据并不是盲目的，模型容限能力不高的情况下即使增大数据也对模型毫无意义。而从数据获取的成本角度，对现有数据进行有效的扩充也是个非常有效且实际的方式。良好的数据处理，常见的处理方式如数据缩放、归一化和标准化等。</p>
<ol start="2" style="list-style-type: decimal">
<li><strong>模型角度</strong></li>
</ol>
<p>模型的容限能力决定着模型可优化的空间。在数据量充足的前提下，对同类型的模型，增大模型规模来提升容限无疑是最直接和有效的手段。但越大的参数模型优化也会越难，所以需要在合理的范围内对模型进行参数规模的修改。而不同类型的模型，在不同数据上的优化成本都可能不一样，所以在探索模型时需要尽可能挑选优化简单，训练效率更高的模型进行训练。</p>
<ol start="3" style="list-style-type: decimal">
<li><strong>调参优化角度</strong></li>
</ol>
<p>如果你知道模型的性能为什么不再提高了，那已经向提升性能跨出了一大步。 超参数调整本身是一个比较大的问题。一般可以包含<strong>模型初始化的配置，优化算法的选取、学习率的策略</strong>以及<strong>如何配置正则和损失函数</strong>等等。这里需要提出的是对于同一优化算法，相近参数规模的前提下，不同类型的模型总能表现出不同的性能。这实际上就是模型优化成本。从这个角度的反方向来考虑，同一模型也总能找到一种比较适合的优化算法。所以确定了模型后选择一个适合模型的优化算法也是非常重要的手段。</p>
<ol start="4" style="list-style-type: decimal">
<li><strong>训练角度</strong></li>
</ol>
<p>很多时候我们会把优化和训练放一起。但这里我们分开来讲，主要是为了强调充分的训练。在越大规模的数据集或者模型上，诚然一个好的优化算法总能加速收敛。但你在未探索到模型的上限之前，永远不知道训练多久算训练完成。所以在改善模型上充分训练永远是最必要的过程。充分训练的含义不仅仅只是增大训练轮数。有效的学习率衰减和正则同样是充分训练中非常必要的手段。</p>
<hr>
<h3 id="激活函数">激活函数</h3>
<h4 id="为什么需要激活函数">为什么需要激活函数？</h4>
<ul>
<li>激活函数对模型学习、理解非常复杂的、和非线性的函数具有重要作用</li>
<li>使用<strong>激活函数</strong>的目的是为了向网络中加入<strong>非线性因素</strong> 。从而加强网络的表示能力，解决<strong>线性模型</strong>无法解决的问题</li>
</ul>
<h4 id="为什么要使用非线性激活函数">为什么要使用非线性激活函数？</h4>
<blockquote>
<p><strong>神经网络的万能近似定理</strong>认为，神经网络具有至少一个非线性隐藏层，那么只要给予网络足够数量的隐藏单元，它就可以以任意的精度来近似拟合任何<strong>从一个有限维空间到另一个有限维空间</strong>的函数。</p>
</blockquote>
<ul>
<li>如果不使用非线性激活函数，那么每一层输出都是上层输入的<strong>线性组合</strong>；此时无论网络有多少层，其整体也将是线性的，就做不到用非线性来逼近任意函数，导致失去万能近似的性质</li>
<li>使用非线性激活函数 ，可以增强网络的表示能力，使它可以学习从输入到输出之间复杂的非线性的映射。而且，仅<strong>部分层是纯线性</strong>是可以接受的，这有助于<strong>减少网络中的参数</strong>。</li>
</ul>
<h4 id="什么时候可以用线性激活函数">什么时候可以用线性激活函数</h4>
<ul>
<li>输出层，大多使用线性激活函数</li>
<li>在隐含层可能会使用一些线性激活函数</li>
<li>一般用到的线性激活函数很少</li>
</ul>
<h4 id="常见的激活函数">常见的激活函数</h4>
<ul>
<li><span class="math inline">\(Sigmoid\)</span></li>
</ul>
<p><span class="math inline">\(Sigmod\)</span> 又叫作 <strong><span class="math inline">\(Logistic\)</span> 激活函数</strong>，它将实数值压缩进 0 到 1 的区间内，还可以在预测概率的输出层中使用。该函数将大的负数转换成 0，将大的正数转换成 1<br>
数学公式为：<br>
<span class="math display">\[
y = \sigma(x) = \frac{1}{1 + e^{-x}}   \\
y&#39; = y * (1 - y)
\]</span><br>
下图展示了 Sigmoid 函数及其导数：</p>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/sigmod_01.png" alt="Sigmoid激活函数">
<p class="caption">Sigmoid激活函数</p>
</div>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/sigmod_02.png" alt="Sigmoid激活函数导数">
<p class="caption">Sigmoid激活函数导数</p>
</div>
<ul>
<li><span class="math inline">\(Sigmoid\)</span> 函数的三个主要缺陷：</li>
<li><strong>梯度消失</strong>：<span class="math inline">\(Sigmoid\)</span> 函数在输入取绝对值非常大的正值或负值时会出现 <strong>饱和</strong> 现象，在图像上表现为变得很平缓，此时函数会对输入的微小变化不敏感，即此时的梯度趋近于0，造成梯度消失，网络权重更新缓慢或不更新。</li>
<li><strong>计算成本高昂</strong>：<span class="math inline">\(exp()\)</span> 函数与其他非线性激活函数相比，计算成本高昂</li>
<li><strong>不以零为中心</strong>：Sigmoid 输出不以零为中心的</li>
</ul>
<hr>
<ul>
<li><span class="math inline">\(Tanh\)</span> 函数</li>
</ul>
<p><span class="math display">\[
\tanh (x)=2 \sigma(2 x)-1=\frac{\mathrm{e}^{x}-\mathrm{e}^{-x}}{\mathrm{e}^{x}+\mathrm{e}^{-x}} \\
\tanh ^{\prime}(x)=1-\tanh ^{2}(x)
\]</span></p>
<p><span class="math inline">\(Tanh\)</span> 函数又叫作<strong>双曲正切激活函数</strong>。与 <span class="math inline">\(Sigmoid\)</span> 函数类似，区别是值域为 <span class="math inline">\((-1, 1)\)</span> ，且 <span class="math inline">\(Tanh\)</span> 函数的输出以零为中心，因为区间在 <span class="math inline">\(-1\)</span> 到 <span class="math inline">\(1\)</span> 之间。</p>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/tanh_01.png" alt="Tanh函数">
<p class="caption">Tanh函数</p>
</div>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/tanh_02.png" alt="Tanh函数导数">
<p class="caption">Tanh函数导数</p>
</div>
<ul>
<li>缺点：</li>
<li><strong>梯度消失</strong>： <span class="math inline">\(Tanh\)</span> 函数也会有梯度消失的问题，因此在饱和时也会「杀死」梯度。</li>
<li><strong>计算成本高昂</strong>：<span class="math inline">\(exp()\)</span> 函数与其他非线性激活函数相比，计算成本高昂</li>
</ul>
<h4 id="为什么tanh收敛速度比sigmoid快">为什么Tanh收敛速度比Sigmoid快</h4>
<ul>
<li><span class="math inline">\(tanh^{&#39;}(x)=1-tanh(x)^{2}\in (0,1)\)</span></li>
<li><span class="math inline">\(s^{&#39;}(x)=s(x)*(1-s(x))\in (0,\frac{1}{4}]\)</span></li>
</ul>
<p>由上面两个公式可知 <span class="math inline">\(Tanh\)</span> 梯度消失的问题比 <span class="math inline">\(Sigmoid\)</span> 轻，所以 <span class="math inline">\(Tanh\)</span> 收敛速度比 <span class="math inline">\(Sigmoid\)</span> 快。</p>
<hr>
<ul>
<li><span class="math inline">\(Relu\)</span> 函数</li>
</ul>
<p><span class="math inline">\(ReLU\)</span> 是从底部开始半修正的一种函数，数学公式为：<br>
<span class="math display">\[
f(x) = max(0, x)
\]</span><br>
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/ReLU_01.png"><br>
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/ReLU_02.png"></p>
<ul>
<li>优点：</li>
<li><strong>加速网络训练：</strong>当输入 <span class="math inline">\(x &lt; 0\)</span> 时，输出为 0，当 <span class="math inline">\(x &gt; 0\)</span> 时，输出为 <span class="math inline">\(x\)</span>。该激活函数使网络更快速地收敛。</li>
<li><strong>避免梯度消失</strong>： <span class="math inline">\(ReLU\)</span> 的导数始终是一个常数，负半区为 0，正半区为 1，所以不会发生梯度消失现象。而 <span class="math inline">\(Sigmoid\)</span> 函数在输入取绝对值非常大的正值或负值时会出现<strong>饱和</strong>现象.</li>
<li><strong>减缓过拟合</strong>：<span class="math inline">\(ReLU\)</span> 在负半区的输出为 0。一旦神经元的激活值进入负半区，那么该激活值就不会产生梯度/不会被训练，造成了网络的稀疏性——<strong>稀疏激活</strong> 。这有助于减少参数的相互依赖，缓解过拟合问题的发生</li>
<li><strong>加速计算</strong>：<span class="math inline">\(ReLU\)</span> 的求导不涉及浮点运算，所以速度更快</li>
<li>缺点：</li>
<li><strong>不以零为中心</strong>：和 <span class="math inline">\(Sigmoid\)</span> 激活函数类似，<span class="math inline">\(ReLU\)</span> 函数的输出不以零为中心。</li>
<li>容易造成神经元死亡现象Dead ReLU <a href="https://blog.csdn.net/disiwei1012/article/details/79204243" target="_blank" rel="noopener">Reference</a>：比如对于一个神经元，当有一个比较大的梯度传递过来，导致该神经元的参数分布发生比较大的变化，变成一个低方差，中心在-0.1的高斯分布，这样以后，大部分数据输入该神经元之后为0，不能通过反向传播更新其参数，造成神经元死亡。这于是就引入了 <span class="math inline">\(Leaky\ ReLU\)</span> 来解决该问题。</li>
</ul>
<hr>
<ul>
<li><span class="math inline">\(Leaky\ ReLU\)</span></li>
</ul>
<p>该函数试图缓解 <code>dead ReLU</code> 问题。数学公式为：<br>
<span class="math display">\[
f(x) = max(0.1x, x)
\]</span><br>
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/Leaky_ReLU_01.png"></p>
<p><span class="math inline">\(Leaky\ ReLU\)</span> 的概念是：当 <span class="math inline">\(x &lt; 0\)</span> 时，它得到 0.1 的正梯度。该函数一定程度上缓解了 <code>dead ReLU</code> 问题，但是使用该函数的结果并不连贯。尽管它具备 <span class="math inline">\(ReLU\)</span> 激活函数的所有特征，如计算高效、快速收敛、在正区域内不会饱和。</p>
<p><span class="math inline">\(Leaky\ ReLU\)</span> 可以得到更多扩展。不让 <span class="math inline">\(x\)</span> 乘常数项，而是让 <span class="math inline">\(x\)</span> 乘超参数，这看起来比 <span class="math inline">\(Leaky\ ReLU\)</span> 效果要好。该扩展就是 <span class="math inline">\(Parametric\ ReLU\)</span>。</p>
<hr>
<ul>
<li><span class="math inline">\(Parametric\ ReLU\)</span></li>
</ul>
<p><span class="math display">\[
f(x) = max(ax, x)
\]</span></p>
<p>其中 <span class="math inline">\(\alpha​\)</span> 是一个可以学习的参数，因为你可以对它进行反向传播。这使神经元能够选择负区域最好的梯度，有了这种能力，它们可以变成 ReLU 或 Leaky ReLU。</p>
<h4 id="怎样理解-relu-0-时是非线性激活函数">怎样理解 ReLU（&lt; 0 时）是非线性激活函数</h4>
<p>从 <span class="math inline">\(ReLU\)</span> 的图像可以看出具有一下特点：</p>
<ul>
<li>单侧抑制</li>
<li>相对宽阔的兴奋边界</li>
<li>稀疏激活性</li>
</ul>
<p><span class="math inline">\(ReLU\)</span> 函数从图像上看，是一个分段线性函数，把所有的负值都变为 0，而正值不变，这样就成为单侧抑制。因为有了这单侧抑制，才使得神经网络中的神经元也具有了稀疏激活性。</p>
<hr>
<h3 id="损失函数代价函数">损失函数/代价函数</h3>
<blockquote>
<ol style="list-style-type: decimal">
<li>损失函数 <span class="math inline">\(| y_i-f(x_i)|\)</span> ，一般是针对单个样本 <span class="math inline">\(i\)</span></li>
<li>代价函数 <span class="math inline">\(\frac{1}{N}\sum_{i=1}^{N}{| y_i-f(x_i)}|\)</span> ，一般是针对总体<span class="math inline">\(N\)</span></li>
<li>目标函数 <span class="math inline">\(\frac{1}{N}\sum_{i=1}^{N}{| y_i-f(x_i)}| + 正则化项\)</span></li>
</ol>
<p><strong>(其实 <code>损失函数</code> 和 <code>代价函数</code> 可以理解成同一个东西)</strong></p>
</blockquote>
<p><strong>损失函数</strong>（loss function）是用来估量模型的预测值 <span class="math inline">\(f(X)\)</span> 与真实值 <span class="math inline">\(Y\)</span> 的不一致程度，它是一个非负实值函数，通常使用 <span class="math inline">\(L(Y, f(X))\)</span> 来表示，损失函数越小，模型的鲁棒性就越好。训练数据集的平均损失称为<strong>经验风险</strong> 。<strong>结构风险最小化</strong> 是为了防止过拟合而提出来的策略，结构风险在经验风险基础上加上正则化项(regularizer)或惩罚项(penalty term)，[李航P8] 通常可以表示为如下形式：<br>
<span class="math display">\[
\theta^{*}=\arg \min _{\theta} \frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i} ; \theta\right)\right)+\lambda \Phi(\theta)
\]</span><br>
其中，前面的均值函数表示的是经验风险函数，<span class="math inline">\(L\)</span> 代表的是损失函数，后面的 <span class="math inline">\(Φ\)</span> 是正则化项，它可以是 <span class="math inline">\(L1\)</span>，也可以是 <span class="math inline">\(L2\)</span>，或者其他的正则函数。整个式子表示的意思是<strong>找到使目标函数最小时的 <span class="math inline">\(θ\)</span> 值</strong>。</p>
<h4 id="为什么需要损失代价函数">为什么需要损失/代价函数</h4>
<p>用于找到最优解的目标/假设函数（用于衡量假设函数的准确性）比如：为了得到逻辑回归模型的参数，需要一个代价函数，通过训练代价函数来得到参数。</p>
<h4 id="损失代价函数的作用及原理">损失/代价函数的作用及原理</h4>
<p>使用线性回归的例子说明，用一条直线，拟合给定的数据，使用均方误差损失函数，梯度下降法来拟合。</p>
<h4 id="为什么损失代价函数要非负">为什么损失/代价函数要非负</h4>
<p>目标函数存在一个下界，在优化过程当中，如果优化算法能够使目标函数不断减小，根据单调有界准则，这个优化算法就能证明是收敛有效的。只要设计的目标函数有下界，基本上都可以，代价函数非负更为方便。</p>
<h4 id="常用的损失函数">常用的损失函数</h4>
<p><strong>（1）0-1 损失函数</strong></p>
<p>如果 预测值 <span class="math inline">\(f(X)\)</span> 和目标值 <span class="math inline">\(Y\)</span> 相等，值为 0，如果不相等，值为 1：<br>
<span class="math display">\[
L(Y, f(X)) =
\begin{cases}
1,&amp; Y\ne f(X)\\
0,&amp; Y = f(X)
\end{cases}
\]</span><br>
一般的在实际使用中，相等的条件过于严格，可适当放宽条件：<br>
<span class="math display">\[
L(Y, f(X)) =
\begin{cases}
1,&amp; |Y-f(X)|\geqslant T\\
0,&amp; |Y-f(X)|&lt; T
\end{cases}
\]</span><br>
<strong>（2）绝对值损失函数（L1 Loss、LAE）</strong></p>
<p>L1范数损失函数，也称作<strong>最小绝对值偏差</strong>( least absolute deviations, <strong>LAD</strong>)，<strong>最小绝对值误差</strong>(least absolute errors, <strong>LAE</strong>)。目的是将估计值 <span class="math inline">\(f(X)\)</span> 和 目标值 <span class="math inline">\(Y\)</span> 的绝对差值的总和 <span class="math inline">\(L\)</span> 最小化：<br>
<span class="math display">\[
L(Y, f(X)) = |Y-f(X)|  \\ \color{green}{（此时的 \ X \ 是只有一个样本的输入）}
\]</span><br>
在实际应用中，<span class="math inline">\(X\)</span> 样本个数为 <span class="math inline">\(m\)</span> ，通常会使用 <strong>平均最小绝对值误差 (MAE)</strong> 做为 L1 Loss 形式 ：<br>
<span class="math display">\[
L(Y, f(X)) = \frac{1}{m}\sum_{i=1}^m|Y-f(X)|   \\ \color{green}{（此时的 \ X \ 有 \ m \ 个样本的输入）}
\]</span><br>
<strong>（3）平方损失函数 (L2 Loss、LSE、最小二乘法OLS)</strong></p>
<p>基于均方误差最小化(平方损失函数)来进行模型求解的方法称为: <span class="math inline">\(\mathbf{\color{green}{最小二乘法}}\)</span>. 在线性回归中，最小二乘法就是试图找到一条直线，使得所有样本到直线上的欧氏距离之和最小. (其假设样本和噪声都<strong>服从<span class="math inline">\(\mathbf{\color{blue}{高斯分布}}\)</span></strong>，然后通过极大似然估计(MLE)可以推导出最小二乘式子)<br>
即 <strong>OLS</strong> 是<strong>基于距离的</strong>，而这个距离就是我们用的最多的欧几里得距离。为什么它会选择使用欧式距离作为误差度量呢？ (即Mean squared error， MSE)，主要有以下几个原因：</p>
<blockquote>
<ol style="list-style-type: decimal">
<li>简单，计算方便</li>
<li>欧氏距离是一种很好的相似性度量标准</li>
<li>在不同的表示域变换后特征性质不变</li>
</ol>
</blockquote>
<ul>
<li>平方损失（Square loss）的标准形式如下：</li>
</ul>
<p><span class="math display">\[
L(Y,f(X))=(Y-f(X))^2.
\]</span><br>
- 当样本个数为 <span class="math inline">\(m\)</span> 时，此时损失函数变为：</p>
<p><span class="math display">\[
L(Y, f(X)) = \sum_{i = 1}^{m}(Y - f(X))
\]</span></p>
<p><span class="math inline">\(Y-f(X)\)</span> 表示的是 <strong>残差</strong>，整个式子表示的是 <strong>残差的平方和</strong>，而我们的目的就是最小化这个目标函数值 (注：该式子未加入正则项)，也就是<u>最小化残差的平方和</u>（residual sum of squares, RSS）<br>
而在实际应用中，通常会使用 <strong>均方误差 (MSE)</strong> 作为一项衡量指标，公式如下：<br>
<span class="math display">\[
MSE = \frac{1}{m}\sum_{i = 1}^{m}(Y - f(X))^2
\]</span><br>
&gt; 最常用的是平方损失，然而其缺点是对于异常点会施以较大的惩罚，因而不够robust。如果有较多异常点，则绝对值损失表现较好，但绝对值损失的缺点是在 <img src="https://www.zhihu.com/equation?tex=y-f%28x%29%3D0" alt="[公式]"> 处不连续可导，因而不容易优化。</p>
<p><strong>（4）对数损失函数（逻辑回归）</strong><br>
<span class="math display">\[
L(Y,P(Y|X))=-logP(Y|X).
\]</span><br>
常见的<u>逻辑回归使用的就是对数损失函数</u>，而不是平方损失。逻辑回归它假设样本服从 <strong><span class="math inline">\(\mathbf{\color{blue}{伯努利分布（0-1分布）}}\)</span></strong>，进而求得满足该分布的似然函数，接着取对数求极值等。而逻辑回归并没有求似然函数的极值，而是把极大化当做是一种思想，进而推导出它的经验风险函数为：最小化负的似然函数，从损失函数的角度看， 就是 <span class="math inline">\(log\)</span> 损失函数。</p>
<p>损失函数 <span class="math inline">\(L(Y, P(Y|X))\)</span> 表达的是样本 <span class="math inline">\(X\)</span> 在分类 <span class="math inline">\(Y\)</span> 的情况下，使概率 <span class="math inline">\(P(Y|X)\)</span> 达到最大值（换言之，<strong>就是利用已知的样本分布，找到最有可能（即最大概率）导致这种分布的参数值；或者说什么样的参数才能使我们观测到目前这组数据的概率最大</strong>）因为 <span class="math inline">\(log\)</span> 函数是单调递增的，所以 <span class="math inline">\(logP(Y|X)\)</span> 也会达到最大值，因此在前面加上负号之后，最大化 <span class="math inline">\(P(Y|X)\)</span> 就等价于最小化 <span class="math inline">\(L\)</span> 了。</p>
<p><strong>逻辑回归</strong> 的 <span class="math inline">\(P(Y=y|x)\)</span> 表达式如下（为了将类别标签 <span class="math inline">\(y\)</span> 统一为1和0，下面将表达式分开表示）：<br>
<span class="math display">\[
P(Y=y | x)=\left\{\begin{array}{cc}{h_{\theta}(x)=g(f(x))=\frac{1}{1+\exp (-f(x)\}}} &amp; {, y=1} \\ {1-h_{\theta}(x)=1-g(f(x))=\frac{1}{1+\exp (f(x)\}}} &amp; {, y=0}\end{array}\right.
\]</span><br>
将它带入到上式，通过推导可以得到logistic的损失函数表达式，如下：<br>
<span class="math display">\[
L(y, P(Y=y | x))=\left\{\begin{array}{cc}{\log (1+\exp \{-f(x)\})} &amp; {, y=1} \\ {\log (1+\exp \{f(x)\})} &amp; {, y=0}\end{array}\right.
\]</span><br>
逻辑回归最后得到的目标式子如下：<br>
<span class="math display">\[
J(\theta)=-\frac{1}{m} \sum_{i=1}^{m}\left[y^{(i)} \log h_{\theta}\left(x^{(i)}\right)+\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right]
\]</span><br>
<strong>（5）指数损失函数</strong><br>
  <br>
指数损失函数的标准形式为：<br>
<span class="math display">\[
L(y|f(x))=exp[-yf(x)].
\]</span><br>
例如 <code>AdaBoost</code> 就是以指数损失函数为损失函数。</p>
<p><strong>（6）Hinge 损失函数</strong><br>
  <br>
Hinge 损失函数的标准形式如下：<br>
<span class="math display">\[
L(y)=max(0, 1-ty).
\]</span><br>
其中 <span class="math inline">\(y\)</span> 是预测值，范围为 $(-1,1), $ <span class="math inline">\(t\)</span> 为目标值，其为 <span class="math inline">\(-1 或 1\)</span>。<br>
在线性支持向量机中，最优化问题可等价于：<br>
<span class="math display">\[
\underset{w,b}{min}\sum_{i=1}^{N}(1-y_i(wx_i+b))+\lambda \lVert w^2 \rVert
\]</span></p>
<p><span class="math display">\[
\frac{1}{m}\sum_{i=1}^{N}l(wx_i+by_i))+\lVert w^2 \rVert
\]</span></p>
<p>其中 <span class="math inline">\(l(wx_i+by_i))\)</span> 是Hinge损失函数，<span class="math inline">\(\lVert w^2 \rVert\)</span> 可看做为正则化项。</p>
<blockquote>
<p>Reference: <a href="http://www.csuldw.com/2016/03/26/2016-03-26-loss-function/" target="_blank" rel="noopener">机器学习-损失函数</a></p>
</blockquote>
<h4 id="softmax损失函数">Softmax损失函数</h4>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/maxresdefault.jpg">

</div>
<p><strong>Softmax</strong> 函数 <span class="math inline">\(\sigma(z)=\left({\color{Red}{\sigma_{1}(z)}}, \ldots, \sigma_{m}(z)\right)\)</span> 定义如下：<br>
<span class="math display">\[
{\color{Green}{o_i}} = {\color{Red}{\sigma_{i}(z)}}
=\frac{\exp \left(z_{i}\right)}{\sum_{j=1}^{m} \exp \left(z_{j}\right)}, \quad i=1, \ldots, m  \\
{\color{Green}{\small{【观察到的数据 \ x \ (或z)\ 属于类别\ i\ 的概率，或者称作似然 (Likelihood)】}}}
\]</span><br>
它在 Logistic Regression 里其到的作用是将线性预测值转化为类别概率：<span class="math inline">\(m\)</span> 代表类别数，假设 <span class="math inline">\(z_{i}=\theta_{i}^{T} x\)</span> 是第 <span class="math inline">\(i\)</span> 个类别的线性预测结果，带入 <span class="math inline">\(Softmax\)</span> 的结果其实就是先对每一个 <span class="math inline">\(z_i\)</span> 取 exponential 变成非负，然后除以所有项之和进行归一化，现在每个 <span class="math inline">\(o_{i}=\sigma_{i}(z)\)</span> 就可以解释成：<strong>观察到的数据 <span class="math inline">\(x\)</span> 属于类别 <span class="math inline">\(i\)</span> 的概率，或者称作似然 (Likelihood)</strong>。</p>
<p>然后 Logistic Regression 的目标函数是根据最大似然原则来建立的，假设数据 <span class="math inline">\(x\)</span> 所对应的类别为 <span class="math inline">\(y\)</span>，则根据我们刚才的<strong>计算最大似然就是要最大化 <span class="math inline">\(o_y\)</span> 的值</strong> (通常是使用 <strong>negative log-likelihood</strong> 而不是 likelihood，也就是说<strong>最小化 <span class="math inline">\(-log(o_y)\)</span> 的值</strong>，这两者结果在数学上是等价的)。后面这个操作就是 caffe 文档里说的 <strong>Multinomial Logistic Loss</strong>，具体写出来是这个样子：<br>
<span class="math display">\[
\ell(y, o)=-\log \left(o_{y}\right)
\]</span><br>
<strong>Softmax 函数梯度计算</strong></p>
<p><code>Softmax层</code> 输出为 <span class="math inline">\(o_i = \sigma_i(z)\)</span> ，其对输入 <span class="math inline">\(z_k\)</span> 的梯度为:<br>
<span class="math display">\[
\begin{aligned} 
\frac{\partial o_{i}}{\partial z_{k}} 
&amp; = \frac{\partial}{\partial z_{k}} \frac{\exp \left(z_{i}\right)}{\sum_{j=1}^{m} \exp \left(z_{j}\right)} \\
&amp; =\frac{\delta_{i k} e^{z_{i}}\left(\sum_{j=1}^{m} e^{z_{j}}\right)-e^{z_{i}} e^{z_{k}}}{\left(\sum_{j=1}^{m} e^{z_{j}}\right)^{2}} \\ 
&amp; =\delta_{i k} o_{i}-o_{i} o_{k} 
\end{aligned}
\]</span><br>
即 <strong>Softmax 层的梯度为：</strong><br>
<span class="math display">\[
\begin{aligned} 
\frac{\partial o_{i}}{\partial z_{k}} =
\begin{cases}  
o_i(1 - o_k), &amp; k = i \\
- o_i o_k, &amp; k \ne i
\end{cases}
\end{aligned}
\]</span></p>
<h4 id="softmax-loss损失函数">Softmax-Loss损失函数</h4>
<p>而 <strong>Softmax-Loss</strong> 其实就是把两者结合到一起，只要把 <span class="math inline">\(o_y\)</span> 的定义展开即可：<br>
<span class="math display">\[
\tilde{\ell}(y, z)=-\log \left(\frac{e^{z_{y}}}{\sum_{j=1}^{m} e^{z_{j}}}\right)=\log \left(\sum_{j=1}^{m} e^{z_{j}}\right)-z_{y} \\
{\color{Green}{\small{【将观察到的数据 \ x \ (或z)\ 属于类别 \ y \ 的概率做最大似然估计，或最小\ negative\  log\ 似然】}}}
\]</span><br>
<strong>梯度计算：</strong></p>
<p>计算输入数据 <span class="math inline">\(z_k\)</span> 属于类别 <span class="math inline">\(y\)</span> 的概率的极大似然估计。由于 <code>Softmax-Loss</code> 层是最顶层的输出层，则可以直接用最终输出 (loss): <span class="math inline">\(\tilde{\ell}(y, z)\)</span> 求对输入 <span class="math inline">\(z_k\)</span> 的偏导数：<br>
<span class="math display">\[
\begin{aligned}
\frac{\partial \tilde{\ell}(y, z)}{\partial z_{k}} 
&amp; = \frac{\partial}{\partial z_{k}} \left(\log \left(\sum_{j=1}^{m} e^{z_{j}}\right)-z_{y} \right)\\
&amp; = \frac{\exp \left(z_{k}\right)}{\sum_{j=1}^{m} \exp \left(z_{j}\right)}-\delta_{k y}=\sigma_{k}(z)-\delta_{k y}
\end{aligned}
\]</span><br>
其中 <span class="math inline">\(\sigma_k(z)\)</span> 是 <code>Softmax-Loss</code> 的中间步骤 <code>Softmax</code> 在 Forward Pass 的计算结果，而<br>
<span class="math display">\[
\delta_{k y}=\left\{\begin{array}{ll}{1} &amp; {k=y} \\ {0} &amp; {k \neq y}\end{array}\right.
\]</span><br>
即 <strong>Softmax-Loss 层的梯度为</strong>：<br>
<span class="math display">\[
\begin{aligned}
\frac{\partial \tilde{\ell}(y, z)}{\partial z_{k}}  = 
\begin{cases} 
\sigma_{k}(z) - 1 , &amp; k = y \\
\sigma_{k}(z) , &amp;k \ne y
\end{cases}
\end{aligned}
\]</span></p>
<hr>
<h4 id="熵条件熵kl散度交叉熵">熵、条件熵、KL散度、交叉熵</h4>
<blockquote>
<p>信息论背后的原理是：从不太可能发生的事件中能学到更多的有用信息。</p>
<ul>
<li>发生可能性较大的事件包含较少的信息。</li>
<li>发生可能性较小的事件包含较多的信息。</li>
<li>独立事件包含额外的信息 。</li>
</ul>
</blockquote>
<blockquote>
<p><strong>【通俗理解】</strong></p>
<ol style="list-style-type: decimal">
<li><strong>熵</strong>：可以表示一个事件A的自信息量，也就是A包含多少信息（表示随机变量不确定性的度量，所有可能发生的事件产生的信息量的期望）</li>
<li><strong>KL散度</strong>：可以用来表示从事件B的角度来看，事件A有多大不同</li>
<li><strong>交叉熵</strong>：可以用来表示从事件B的角度来看，如何描述事件A</li>
<li><strong>条件熵</strong>： H(Y|X) 表示已知随机变量X的条件下，随机变量Y的不确定性</li>
</ol>
<p><strong>【概括】</strong>：KL散度可以被用于计算代价，而在特定情况下最小化KL散度等价于最小化交叉熵。而交叉熵的运算更简单，所以用交叉熵来当做代价</p>
</blockquote>
<ol style="list-style-type: decimal">
<li><strong>熵(entropy)</strong>： <strong><code>在信息论中表示一个事件所包含的信息量，所有可能发生事件产生的信息量的期望 (对A时间中的随机变量进行编码所需要的最小字节数)</code></strong></li>
</ol>
<ul>
<li><strong>越不可能发生的事件信息量越大</strong>，比如“我不会死”这句话信息量就很大。<strong>而确定事件的信息量就很低</strong>， 比如“我是我妈生的”，信息量就很低甚至为0</li>
<li><strong>独立事件的信息量可叠加。</strong>比如“a. 张三今天喝了阿萨姆红茶，b. 李四前天喝了英式早茶”的信息量就应该恰好等于a+b的信息量，如果张三李四喝什么茶是两个独立事件。</li>
</ul>
<p><strong>自信息</strong>：对于事件 <span class="math inline">\(X = x\)</span> ，定义自信息 <code>self-information</code> 为： <span class="math inline">\(I(x) = -logP(x)\)</span> 自信息仅仅处理单个输出，但是如果计算自信息的期望，它就是熵.</p>
<p><strong>熵的定义</strong>：<br>
<span class="math display">\[
   S(x)=-\sum_{i= 1}^{n} P\left(x_{i}\right) \log P\left(x_{i}\right)
   \]</span><br>
<span class="math inline">\(x\)</span> 指不同的事件，比如喝茶。<span class="math inline">\(P(x_i)\)</span> 指的是某个事件发生的概率，比如喝红茶的概率。对于一个一定会发生的事件，其发生概率为1，<span class="math inline">\(S(x)=-\log (1) * 1=-0 * 1=0\)</span> ，信息量为0.</p>
<ol start="2" style="list-style-type: decimal">
<li><strong>KL散度/相对熵(Kullback-Leibler Divergence)：</strong> <strong><code>衡量两个事件(分布)之间的差异 (刻画了非真实分布B编码真实分布A带来的平均编码长度的增量)</code></strong></li>
</ol>
<p>对于给定的随机变量 <span class="math inline">\(X\)</span>，它的两个单独的概率分布函数 <span class="math inline">\(A(X) 和 B(X)\)</span> 的区别，可以用 <code>KL</code> 散度来度量。<strong>KL散度，有时候也叫KL距离，一般被用于计算两个分布之间的差异</strong>。看名字似乎跟计算两个点之间的距离也很像，但实则不然，因为KL散度<strong>不具备有对称性</strong>。在距离上的对称性指的是A到B的距离等于B到A的距离。</p>
<p><strong>KL散度的数学定义：</strong></p>
<ul>
<li>对于<strong>离散事件</strong>我们可以定义事件A和B的差别为(2.1)：</li>
</ul>
<p><span class="math display">\[
   D_{K L}(A \| B)=\sum_{i} P_{A}\left(x_{i}\right) \log \left(\frac{P_{A}\left(x_{i}\right)}{P_{B}\left(x_{i}\right)}\right)=\sum_{i} P_{A}\left(x_{i}\right) \log \left(P_{A}\left(x_{i}\right)\right)-P_{A}\left(x_{i}\right) \log \left(P_{B}\left(x_{i}\right)\right)
   \\ {\color{blue}{事件 A 与 B之间的对数差 \ 在 A上的期望}}
   \]</span></p>
<ul>
<li>对于<strong>连续事件</strong>，那么我们只是把求和改为求积分而已(2.2)：</li>
</ul>
<p><span class="math display">\[
   D_{K L}(A \| B)=\int a(x) \log \left(\frac{a(x)}{b(x)}\right)
   \]</span></p>
<p>从公式中可以看出：</p>
<ul>
<li><strong>如果</strong> <span class="math inline">\(P_A=P_B\)</span> ，即两个事件/分布完全相同，那么KL散度等于0</li>
<li><strong>其中 <span class="math inline">\(-\sum_{i} P_{A}\left(x_{i}\right) \log \left(P_{A}\left(x_{i}\right)\right)\)</span> 就是事件A的熵</strong></li>
<li>如果颠倒一下顺序求 <span class="math inline">\(D_{KL}(B||A)\)</span> ，那么就需要使用B的熵，答案就不一样了。<strong>所以KL散度来计算两个分布A与B的时候是不是对称的，有“坐标系”的问题</strong>，<span class="math inline">\(D_{K L}(A \| B) \neq D_{K L}(B \| A)\)</span></li>
</ul>
<blockquote>
<p>换句话说，KL散度由A自己的熵与B在A上的期望共同决定。当使用KL散度来衡量两个事件(连续或离散)，上面的公式意义就是<strong>求 A与B之间的对数差 在 A上的期望值</strong>。</p>
</blockquote>
<ol start="3" style="list-style-type: decimal">
<li><strong>交叉熵(cross entropy) = KL散度 + 熵</strong> <strong><code>描述两个事件之间的相互关系 (刻画了用非真实分布B来表示真实分布A中的样本的平均编码长度)</code></strong></li>
</ol>
<p>如果我们默认了用KL散度来计算两个分布间的不同，那还要交叉熵做什么？</p>
<p>事实上交叉熵和KL散度的公式非常相近，其实就是KL散度的后半部分(公式2.1)：A和B的交叉熵 = A与B的KL散度 + A的熵。<span class="math inline">\(D_{K L}(A \| B)=-S(A)+H(A, B)\)</span></p>
<ul>
<li><strong>KL散度的公式</strong></li>
</ul>
<p><span class="math display">\[
   D_{K L}(A \| B)=\sum_{i} P_{A}\left(x_{i}\right) \log \left(\frac{P_{A}\left(x_{i}\right)}{P_{B}\left(x_{i}\right)}\right)=\sum_{i} P_{A}\left(x_{i}\right) \log \left(P_{A}\left(x_{i}\right)\right)-P_{A}\left(x_{i}\right) \log \left(P_{B}\left(x_{i}\right)\right)
   \]</span></p>
<ul>
<li><strong>熵的公式</strong></li>
</ul>
<p><span class="math display">\[
   S(A)=-\sum_{i} P_{A}\left(x_{i}\right) \log P_{A}\left(x_{i}\right)
   \]</span></p>
<ul>
<li><strong>交叉熵公式</strong></li>
</ul>
<p><span class="math display">\[
   H(A, B)=-\sum_{i} P_{A}\left(x_{i}\right) \log \left(P_{B}\left(x_{i}\right)\right)
   \]</span></p>
<blockquote>
<p><strong>如果</strong> <span class="math inline">\(S(A)\)</span> <strong>是一个常量，那么</strong> <span class="math inline">\(D_{KL}(A||B) = H(A,B)\)</span> ，<strong>也就是说KL散度和交叉熵在特定条件下等价。</strong> <strong><code>比如我们的A为真实分布，B为非真实分布，即KL散度前者是真实分布的熵，后者是交叉熵，由于真实分布是固定的，所以信息熵的值是固定的。此时的KL散度 和 交叉熵是等价的。</code></strong></p>
</blockquote>
<ul>
<li><strong>交叉熵性质</strong>
<ul>
<li>和KL散度相同，交叉熵也不具备对称性：<span class="math inline">\(H(A, B) \neq H(B, A)\)</span></li>
<li>从名字上来看，Cross(交叉)主要是用于<strong>描述两个事件之间的相互关系</strong>，对自己求交叉熵等于熵。即 <span class="math inline">\(H(A, A)=S(A)\)</span> ，注意只是非负而不一定等于0。</li>
</ul></li>
<li><strong>交叉熵和KL散度的联系</strong>
<ul>
<li>不同之处：交叉熵中不包括“熵”的部分</li>
<li>相同之处：a. 都不具备对称性 b. 都是非负的</li>
<li>等价条件：当 A 固定不变时，那么最小化KL散度 $D_{KL}(A||B) $ 等价于最小化交叉熵 <span class="math inline">\(H(A,B)\)</span> 。<span class="math inline">\(D_{KL}(A||B) = H(A,B)\)</span></li>
</ul></li>
</ul>
<ol start="4" style="list-style-type: decimal">
<li><strong>条件熵(conditional entropy)</strong>： <strong><code>H(Y|X):表示已知随机变量X的条件下，随机变量Y的不确定性.</code></strong></li>
</ol>
<ul>
<li><strong>条件熵的公式</strong></li>
<li><strong><a href="https://zhuanlan.zhihu.com/p/26551798" target="_blank" rel="noopener">实例理解</a></strong></li>
</ul>
<p><span class="math display">\[
   \begin{aligned} H(Y | X) &amp;=\sum_{x \in X} p(x) H(Y | X=x) \\ &amp;=-\sum_{x \in X} p(x) \sum_{y \in Y} p(y | x) \log p(y | x) \\ &amp;=-\sum_{x \in X} \sum_{y \in Y} p(x, y) \log p(y | x) \end{aligned}
\]</span></p>
<ul>
<li><p><strong>总结</strong></p>
<ul>
<li><u>其实条件熵意思是按一个新的变量的每个值对原变量进行分类，</u>(比如上面这个题把嫁与不嫁按帅，不帅分成了俩类) <u>然后在每一个小类里面，都计算一个小熵，然后每一个小熵乘以各个类别的概率，然后求和。</u></li>
<li>我们用另一个变量对原变量分类后，原变量的不确定性就会减小了，因为新增了X的信息，可以感受一下。不确定程度减少了多少就是信息的增益。</li>
</ul></li>
<li><p><strong>性质</strong></p></li>
</ul>
<p><span class="math display">\[
H(X, Y)=H(Y | X)+H(X)
   \\ 即：描述 X 和 Y 所需要的信息是：描述 X 所需要的信息加上给定 X 条件下描述 Y 所需的额外信息。
\]</span></p>
<ol start="5" style="list-style-type: decimal">
<li><strong>机器如何“学习”</strong></li>
</ol>
<p>在机器学习中，我们希望在<strong>训练数据上模型学到的分布 <span class="math inline">\(P(model)\)</span> 和真实数据的分布 <span class="math inline">\(P(real)\)</span> 越接近越好，所以我们可以使其相对熵最小</strong>。但是我们没有真实数据的分布，所以只能希望模型学到的分布 <span class="math inline">\(P(model)\)</span> 和训练数据的分布 <span class="math inline">\(P(train)\)</span> 尽量相同。假设训练数据是从总体中独立同分布采样的，那么我们可以通过最小化训练数据的经验误差来降低模型的泛化误差。即：</p>
<ul>
<li>希望学到的模型的分布和真实分布一致，<span class="math inline">\(P(model)≃P(real)\)</span></li>
<li>但是真实分布不可知，假设训练数据是从真实数据中独立同分布采样的，<span class="math inline">\(P(train)≃P(real)\)</span></li>
<li>因此，我们希望学到的模型分布至少和训练数据的分布一致，<span class="math inline">\(P(train)≃P(model)\)</span></li>
</ul>
<p>由此非常理想化的看法是如果模型(左)能够学到训练数据(中)的分布，那么应该近似的学到了真数据(右)的分布：<span class="math inline">\(P(model)≃P(train)≃P(real)\)</span></p>
<ol start="6" style="list-style-type: decimal">
<li><strong>为什么交叉熵可以用作代价？</strong></li>
</ol>
<p>最小化模型分布 <span class="math inline">\(P(model)\)</span> 与训练数据上的分布 <span class="math inline">\(P(train)\)</span> 的差异，等价于最小化这两个分布间的KL散度，也就是最小化 <span class="math inline">\(D_{KL}(P(train)||P(model))\)</span></p>
<p>对比上面的KL散度公式</p>
<ul>
<li>A就是数据的真实分布：<span class="math inline">\(P(train)\)</span></li>
<li>B就是模型从训练数据上学到的分布：<span class="math inline">\(P(model)\)</span></li>
</ul>
<p>而训练数据的分布A是给定的，是固定的，即A的信息熵是固定的，此时求 <span class="math inline">\(D_KL(A||B)\)</span> 等价于求 <span class="math inline">\(H(A, B)\)</span> ，也就是 A 与 B 的交叉熵。<strong>得证，交叉熵可以用于计算“学习模型的分布”与“训练数据分布”之间的不同。当交叉熵最低时(等于训练数据分布的熵)，我们学到了“最好的模型”。</strong></p>
<blockquote>
<p>Reference: <a href="https://www.cnblogs.com/kyrieng/p/8694705.html" target="_blank" rel="noopener">参考一</a> <a href="https://www.zhihu.com/question/65288314/answer/244557337" target="_blank" rel="noopener">参考二</a> <a href="https://zhuanlan.zhihu.com/p/26486223" target="_blank" rel="noopener">参考三</a> <a href="https://zhuanlan.zhihu.com/p/32985487" target="_blank" rel="noopener">参考四</a> <a href="https://zhuanlan.zhihu.com/p/26551798" target="_blank" rel="noopener">参考五</a></p>
</blockquote>
<hr>
<h4 id="为什么用交叉熵代替二次代价均方误差函数">为什么用交叉熵代替二次代价(均方误差)函数</h4>
<p><strong>二次代价函数（quadratic cost）</strong></p>
<p>当使用 <code>sigmoid</code> 函数作为激活函数：<span class="math inline">\(\sigma(z)=\frac{1}{1+e^{-z}}\)</span> ，此时的二次代价函数为<br>
<span class="math display">\[
J(\theta) = \frac{1}{2m}\sum_{i = 1}^{m}(h(x^{(i)} - y^{(i)})^2)
\]</span><br>
假如使用梯度下降法（Gradient descent）来调整权值的参数大小，为了说明方便，我们用单个样本为例，此时二次代价函数为：<br>
<span class="math display">\[
J = \frac{(a - y)^2}{2}
\]</span><br>
<code>a = σ(z)</code> 表示该神经元的输入，<span class="math inline">\(y\)</span> 表示真实值，参数调整需要求损失函数的偏导：<br>
<span class="math display">\[
\begin{aligned} \frac{\partial J}{\partial w} &amp;=(a-y) \sigma^{\prime}(z) x \\ \frac{\partial J}{\partial b} &amp;=(a-y) \sigma^{\prime}(z) \end{aligned}
\]</span><br>
参数沿着梯度方向调整参数大小，不足的地方在于，当初始的误差越大，收敛得越缓慢 (sigmoid输入绝对值非常大的值，是饱和区域，梯度很小 <a href="#常见的激活函数">Reference</a>)</p>
<p><strong>交叉熵代价函数（cross-entropy）</strong><br>
<span class="math display">\[
J = -\frac{1}{n}\sum_x[y\ln a + (1-y)\ln{(1-a)}]
\]</span></p>
<p>其中, <code>a = σ(z)</code>最终求导得到更新权重时的偏导:<br>
<span class="math display">\[
\begin{aligned} \frac{\partial J}{\partial w} &amp;=\frac{1}{n} \sum_{x}(\sigma(z)-y)  x_{j} \\ \frac{\partial J}{\partial b} &amp;=\frac{1}{n} \sum_{x}(\sigma(z)-y) \end{aligned}
\]</span><br>
当误差越大时，梯度就越大，权值 <span class="math inline">\(w\)</span> 和偏置 <span class="math inline">\(b\)</span> 调整就越快，训练的速度也就越快，从而达到更快收敛的目。</p>
<hr>
<h3 id="范数">范数</h3>
<blockquote>
<p><strong>什么是范数？</strong></p>
</blockquote>
<p>我们知道距离的定义是一个宽泛的概念，只要满足非负、自反、三角不等式就可以称之为距离。范数是一种强化了的距离概念，它在定义上比距离多了一条数乘的运算法则。有时候为了便于理解，我们可以把范数当作距离来理解。</p>
<p>在数学上，范数包括 <strong>向量范数</strong> 和 <strong>矩阵范数</strong>，向量范数表征向量空间中向量的大小，矩阵范数表征矩阵引起变化的大小。一种非严密的解释就是，对应向量范数，向量空间中的向量都是有大小的，这个大小如何度量，就是用范数来度量的，不同的范数都可以来度量这个大小，就好比米和尺都可以来度量远近一样；对于矩阵范数，学过线性代数，我们知道，通过运算 <span class="math inline">\(AX=B\)</span>，可以将向量 <span class="math inline">\(X\)</span> 变化为 <span class="math inline">\(B\)</span>，矩阵范数就是来度量这个变化大小的。这里简单地介绍以下几种向量范数的定义和含义</p>
<h4 id="l-p范数">L-P范数</h4>
<p>与闵可夫斯基距离的定义一样，L-P范数不是一个范数，而是一组范数，其定义如下：<br>
<span class="math display">\[
Lp=\sqrt[p]{\sum\limits_{i = 1}^n  x_i^p}，x=(x_1,x_2,\cdots,x_n)
\]</span><br>
根据 <span class="math inline">\(p\)</span> 的变化，范数也有着不同的变化，一个经典的有关 <span class="math inline">\(p\)</span> 范数的变化图如下：</p>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/范数.png">

</div>
<p>上图表示了 <span class="math inline">\(p\)</span> 从无穷到0变化时，三维空间中到原点的距离（范数）为1的点构成的图形的变化情况。以常见的 L-2范数(p=2)为例，此时的范数也即欧氏距离，空间中到原点的欧氏距离为1的点构成了一个球面。<br>
实际上，在 <span class="math inline">\(0≤p&lt;1\)</span> 时，<span class="math inline">\(L_p\)</span> 并不满足三角不等式的性质，也就不是严格意义下的范数。以<span class="math inline">\(p=0.5\)</span> ，二维坐标 <span class="math inline">\((1,4)、(4,1)、(1,9)\)</span> 为例，<span class="math inline">\(\sqrt[0.5]{(1+\sqrt{4})}+\sqrt[0.5]{(\sqrt{4}+1)}&lt;\sqrt[0.5]{(1+\sqrt{9})}\)</span> ，因此这里的 L-P 范数只是一个概念上的宽泛说法。</p>
<h4 id="l0范数">L0范数</h4>
<p>当 <span class="math inline">\(p=0\)</span> 时，也就是 L0 范数，由上面可知，L0范数并不是一个真正的范数，它<u><strong>主要被用来度量向量中非零元素的个数</strong></u>。用上面的L-P定义可以得到的L-0的定义为：<br>
<span class="math display">\[
||x||_{0}=\sqrt[0]{\sum\limits_1^nx_i^0}，x=(x_1,x_2,\cdots,x_n)
\]</span><br>
这里就有点问题了，我们知道非零元素的零次方为1，但零的零次方，非零数开零次方都是什么鬼，很不好说明 L0 的意义，所以在通常情况下，大家都用的是：</p>
<p>​ <span class="math inline">\(||x_0||\)</span> = # <span class="math inline">\(( i|x_i\neq 0)\)</span></p>
<blockquote>
<p>表示向量 <span class="math inline">\(x\)</span> 中非零元素的个数。</p>
</blockquote>
<p>对于L0范数，其优化问题为：<br>
<span class="math display">\[
min||x||_0 \\
s.t. Ax=b
\]</span><br>
在实际应用中，由于 L0 范数本身不容易有一个好的数学表示形式，给出上面问题的形式化表示是一个很难的问题，故被人认为是一个NP难问题。所以在实际情况中，L0的最优问题会被放宽到 L1 或 L2 下的最优化。</p>
<h4 id="l1范数">L1范数</h4>
<p>L1范数是我们经常见到的一种范数，它的定义如下：<br>
<span class="math display">\[
||x||_1=\sum_i|x_i|
\]</span></p>
<blockquote>
<p>表示向量 <span class="math inline">\(x\)</span> 中非零元素的绝对值之和</p>
</blockquote>
<p>L1范数有很多的名字，例如我们熟悉的<strong>曼哈顿距离</strong>、<strong>最小绝对误差</strong>等。使用L1范数可以度量两个向量间的差异，如绝对误差和（Sum of Absolute Difference）：<br>
<span class="math display">\[
SAD(x_1,x_2)=\sum_i|x_{1i}-x_{2i}|
\]</span><br>
对于 L1 范数，它的优化问题如下：<br>
<span class="math display">\[
min||x||_1 \\
s.t.Ax=b
\]</span><br>
由于 L1 范数的天然性质，对 L1 优化的解是一个稀疏解，因此 L1范数 也被叫做<strong>稀疏规则算子</strong>。通过L1 可以实现特征的稀疏，去掉一些没有信息的特征，例如在对用户的电影爱好做分类的时候，用户有100个特征，可能只有十几个特征是对分类有用的，大部分特征如身高体重等可能都是无用的，利用 L1 范数就可以过滤掉。</p>
<h4 id="l2范数">L2范数</h4>
<p>L2范数是我们最常见最常用的范数了，我们用的最多的度量距离<strong>欧氏距离</strong>就是一种 L2 范数，它的定义如下：<br>
<span class="math display">\[
||x||_2=\sqrt{\sum_ix_i^2}
\]</span></p>
<blockquote>
<p>表示向量元素的平方和 再开平方</p>
</blockquote>
<p>像 L1 范数一样，L2 也可以度量两个向量间的差异，如平方差和（Sum of Squared Difference）:<br>
<span class="math display">\[
SSD(x_1,x_2)=\sum_i(x_{1i}-x_{2i})^2
\]</span><br>
对于L2范数，它的优化问题如下：<br>
<span class="math display">\[
min||x||_2 \\
s.t.Ax=b
\]</span><br>
L2 范数通常会被用来做优化目标函数的正则化项，防止模型为了迎合训练集而过于复杂造成过拟合的情况，从而提高模型的泛化能力。</p>
<h4 id="l-范数">L-∞范数</h4>
<p>当 <span class="math inline">\(P=∞\)</span> 时，也就是 <span class="math inline">\(L∞\)</span> 范数，它主要被用来度量向量元素的最大值。用上面的 L-P 定义可以得到的 <span class="math inline">\(L∞\)</span> 的定义为：<br>
<span class="math display">\[
||x||_\infty=\sqrt[\infty]{\sum\limits_1^nx_i^\infty}，x=(x_1,x_2,\cdots,x_n)
\]</span><br>
与 L0 一样，在通常情况下，大家都用的是：<br>
<span class="math display">\[
||x||_\infty=max(|x_i|)
\]</span><br>
来表示 <span class="math inline">\(L∞\)</span></p>
<h4 id="欧式距离-与-余弦相似度">欧式距离 与 余弦相似度</h4>
<ol style="list-style-type: decimal">
<li><strong>区别</strong></li>
</ol>
<p>假设 2人对三部电影的评分分别是 <code>A = [3, 3, 3]</code> 和 <code>B = [5, 5, 5]</code><br>
那么2人的欧式距离是 <span class="math inline">\(\sqrt{12} = 3.46\)</span>， A、B的余弦相似度是1（方向完全一致）</p>
<blockquote>
<p>余弦值的范围是[-1, 1], 越接近于1，说明2个向量的方向越相近</p>
</blockquote>
<p>欧式距离和余弦相似度都能度量2个向量之间的相似度，但是欧式距离从2点之间的距离去考量，余弦相似从2个向量之间的夹角去考量。 从上例可以发出，2人对三部电影的评价趋势是一致的，但是欧式距离并不能反映出这一点，余弦相似则能够很好地反应。余弦相似可以很好地规避指标刻度的差异，最常见的应用是计算 <strong>文本的相似度</strong> 。</p>
<ol start="2" style="list-style-type: decimal">
<li><strong>联系</strong></li>
</ol>
<p>（1）欧式距离和余弦相似度都能度量 2 个向量之间的相似度</p>
<p>（2）放到向量空间中看，欧式距离衡量两点之间的<strong>直线距离</strong>，而余弦相似度计算的是两个向量之间的<strong>夹角</strong></p>
<p>（3）<strong>没有归一化时</strong>，欧式距离的范围是 [0, +∞]，而余弦相似度的范围是 [-1, 1]；余弦距离是计算<strong>相似程度</strong>，而欧氏距离计算的是<strong>相同程度</strong>（对应值的相同程度）</p>
<p>（4）<strong>归一化的情况下</strong>，可以将空间想象成一个超球面（三维），欧氏距离就是球面上两点的直线距离，而向量余弦值等价于两点的球面距离，本质是一样。</p>
<ol start="3" style="list-style-type: decimal">
<li><strong>例题解释</strong></li>
</ol>
<p>假设二维空间两个点： <span class="math inline">\(A(x_1, y_1),\ B(x_2,y_2)\)</span></p>
<ul>
<li><strong>欧氏距离（Euclidean Distance）</strong></li>
</ul>
<p><span class="math display">\[
euc = \sqrt{(x_1 - y_1)^2 + (x_2-y_2)^2}
\]</span></p>
<ul>
<li><strong>余弦定义</strong></li>
</ul>
<p><span class="math display">\[
\cos (\theta)=\frac{&lt;A, B&gt;}{|A||B|}     \ \ \ \ (分子为乡里的内积)
\]</span></p>
<p>然后归一化为<strong>单位向量，</strong> <span class="math inline">\(A\left(\frac{x_{1}}{\sqrt{x_{1}^{2}+y_{1}^{2}}}, \frac{y_{1}}{\sqrt{x_{1}^{2}+y_{1}^{2}}}\right), B\left(\frac{x_{2}}{\sqrt{x_{2}^{2}+y_{2}^{2}}}, \frac{y_{2}}{\sqrt{x_{2}^{2}+y_{2}^{2}}}\right)\)</span></p>
<p><strong>余弦相似度</strong>为：<span class="math inline">\(\cos =\frac{x_{1}}{\sqrt{x_{1}^{2}+y_{1}^{2}}} \times \frac{x_{2}}{\sqrt{x_{2}^{2}+y_{2}^{2}}}+\frac{y_{1}}{\sqrt{x_{1}^{2}+y_{1}^{2}}} \times \frac{y_{2}}{\sqrt{x_{2}^{2}+y_{2}^{2}}}，(分母是1省略了)\)</span></p>
<p><strong>欧式距离</strong>为：<span class="math inline">\(e u c=\sqrt{\left(\frac{x_{1}}{\sqrt{x_{1}^{2}+y_{1}^{2}}}-\frac{x_{2}}{\sqrt{x_{2}^{2}+y_{2}^{2}}}\right)^{2}+\left(\frac{y_{1}}{\sqrt{x_{1}^{2}+y_{1}^{2}}}-\frac{y_{2}}{\sqrt{x_{2}^{2}+y_{2}^{2}}}\right)^{2}}\)</span></p>
<p>简化后就是：<span class="math inline">\(e u c=\sqrt{2-2 \times \cos }\)</span> ，即化简后计算的欧式距离是关于余弦相似度的单调函数，可以认为归一化后，余弦相似与欧式距离效果是一致的（欧式距离越小等价于余弦相似度越大）</p>
<p>因此可以将 <strong>求余弦相似转为求欧式距离</strong> ，余弦相似的计算复杂度过高（需要两两比较 <span class="math inline">\(O(n^2)\)</span>），转为求欧式距离后，可以借助 <code>KDTree</code>（KNN算法用到）或者 <code>BallTree</code>（对高维向量友好）来降低复杂度。</p>
<blockquote>
<p><a href="https://www.zhihu.com/question/19640394" target="_blank" rel="noopener">欧氏距离和余弦相似度的区别是什么？</a> <a href="https://juejin.im/entry/5b14121fe51d4506d33ce9f4" target="_blank" rel="noopener">另一个总结</a> <a href="https://my.oschina.net/hunglish/blog/787596" target="_blank" rel="noopener">几种距离度量方法比较</a></p>
</blockquote>
<hr>
<h3 id="线性回归">线性回归</h3>
<p><a href="https://blog.csdn.net/weixin_41500849/article/details/80447501" target="_blank" rel="noopener">岭回归、Lasso回归 和 弹性网络回归</a></p>
<h3 id="逻辑回归">逻辑回归</h3>
<h4 id="回归划分">回归划分</h4>
<p>广义线性模型家族里，依据因变量不同，可以有如下划分：</p>
<ul>
<li>如果是连续的，就是<strong>多重线性回归</strong></li>
<li>如果是二项分布(0-1分布)，就是<strong>逻辑回归</strong></li>
<li>如果是泊松(Poisson)分布，就是<strong>泊松回归</strong></li>
<li>如果是负二项分布，就是<strong>负二项回归</strong></li>
<li>逻辑回归的因变量可以是二分类的，也可以是多分类的，但是二分类的更为常用，也更加容易解释。所以实际中最常用的就是<strong>二分类的逻辑回归</strong>。</li>
</ul>
<h4 id="逻辑回归适用性">逻辑回归适用性</h4>
<ul>
<li><strong>用于概率预测</strong>。用于可能性预测时，得到的结果有可比性。比如根据模型进而预测在不同的自变量情况下，发生某病或某种情况的概率有多大。</li>
<li><strong>用于分类</strong>。实际上跟预测有些类似，也是根据模型，判断某人属于某病或属于某种情况的概率有多大，也就是看一下这个人有多大的可能性是属于某病。进行分类时，仅需要设定一个阈值即可，可能性高于阈值是一类，低于阈值是另一类。</li>
<li>寻找危险因素。寻找某一疾病的危险因素等。</li>
<li><strong>仅能用于线性问题</strong>。<strong>只有当目标和特征是线性关系时，才能用逻辑回归</strong>。在应用逻辑回归时注意两点：一是当知道模型是非线性时，不适用逻辑回归；二是当使用逻辑回归时，应注意选择和目标为线性关系的特征。</li>
<li>各特征之间不需要满足条件独立假设，但各个特征的贡献独立计算。</li>
</ul>
<h4 id="逻辑回归与朴素贝叶斯的区别">逻辑回归与朴素贝叶斯的区别</h4>
<ol style="list-style-type: decimal">
<li>逻辑回归是判别模型， 朴素贝叶斯是生成模型，所以生成和判别的所有区别它们都有。</li>
</ol>
<ul>
<li><strong>判别模型</strong>：由数据直接学习决策函数 <span class="math inline">\(f(X)\)</span> 或者条件概率分布 <span class="math inline">\(P(Y|X)\)</span> 做为预测的模型，即判别模型。判别方法关系的是对给定的输入 <span class="math inline">\(X\)</span>，应该预测什么样的输出 <span class="math inline">\(Y\)</span></li>
<li><strong>生成模型</strong>：由数据学习联合概率分布 <span class="math inline">\(P(X, Y)\)</span>，然后求出条件概率分布 <span class="math inline">\(P(Y|X)\)</span> 作为预测的模型，即生成模型：　<span class="math inline">\(P(Y|X) = \frac{P(X, Y)}{P(X)}\)</span></li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>朴素贝叶斯属于贝叶斯，逻辑回归是极大似然，两种概率哲学间的区别。</li>
<li>朴素贝叶斯需要条件独立假设。</li>
<li>逻辑回归需要求特征参数间是线性的。</li>
</ol>
<h4 id="逻辑回归-与-线性回归-的联系与区别">逻辑回归 与 线性回归 的联系与区别</h4>
<p><strong>联系：</strong> 线性回归与逻辑回归都是广义的线性回归</p>
<table>
<colgroup>
<col width="5%">
<col width="47%">
<col width="47%">
</colgroup>
<thead>
<tr class="header">
<th align="center">区别</th>
<th align="center">线性回归（liner regression）</th>
<th align="center">LR（logistics regression）</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">构建方法</td>
<td align="center">最小二乘法</td>
<td align="center">极大似然函数</td>
</tr>
<tr class="even">
<td align="center">解决问题</td>
<td align="center">主要用于预测，解决<strong>回归问题</strong><br>也可以用来分类，但是鲁棒性差</td>
<td align="center">解决<strong>分类问题</strong></td>
</tr>
<tr class="odd">
<td align="center">输出</td>
<td align="center">输出实数域上连续值</td>
<td align="center">输出值被S型函数映射到[0,1]<br>通过设置阈值转换成分类类别</td>
</tr>
<tr class="even">
<td align="center">拟合函数</td>
<td align="center"><span class="math inline">\(h_{\theta}(x)=\theta ^{T}x=\theta _{1}x _{1}+\theta _{2}x _{2}+...+\theta _{n}x _{n}\)</span></td>
<td align="center">$h_{}(x)=P(y=1</td>
</tr>
</tbody>
</table>
<ul>
<li>线性回归的拟合函数，是对 <span class="math inline">\(h_{\theta}(x)\)</span> 的输出变量 <span class="math inline">\(y\)</span> 的拟合（通过房屋大小，预测房价）</li>
<li>逻辑回归的拟合函数，通过已确定的参数，计算出输入变量 <span class="math inline">\(y = 1\)</span> 的可能性，即是对为1类样本的概率的拟合（疾病预测）</li>
</ul>
<h4 id="逻辑回归-与-svm-的联系与区别">逻辑回归 与 SVM 的联系与区别 <a href="https://www.cnblogs.com/zhizhan/p/5038747.html" target="_blank" rel="noopener">*</a></h4>
<p><strong>相同点：</strong></p>
<ul>
<li>都是<strong>分类算法</strong></li>
<li>都是<strong>监督学习算法</strong></li>
<li>都是<strong>判别模型</strong></li>
<li>如果不考虑核函数，LR 与 SVM 都是线性分类算法，也就是说他们的分类决策面都是线性的</li>
</ul>
<p><strong>不同点：</strong></p>
<ul>
<li><p>LR 和 SVM 的<strong>本质区别</strong>来自于 loss function的不同</p></li>
<li><p>LR 使用的是<strong>交叉熵损失函数/对数损失函数</strong> (cross entropy / log loss)</p></li>
</ul>
<p><span class="math display">\[
  J(\theta)=-\frac{1}{m} \sum_{i=1}^{m}\left[y^{(i)} \log h_{\theta}\left(x^{(i)}\right)+\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right]
  \]</span></p>
<ul>
<li>SVM 是 <strong>合页损失函数</strong> (hinge loss)</li>
</ul>
<p><span class="math display">\[
  \mathcal{L}(w, b, \alpha)=\frac{1}{2}\|w\|^{2}-\sum_{i=1}^{n} \alpha_{i}\left(y_{i}\left(w^{T} x_{i}+b\right)-1\right)
  \]</span></p>
<ul>
<li>LR 基于<u>概率理论</u>，假设样本为1的概率可以用sigmoid函数来表示，然后通过极大似然估计的方法估计出参数的值。SVM 基于<u>几何间隔最大化原理</u>，认为存在最大几何间隔的分类面为最优分类面。线性SVM依赖数据表达的距离测度，所以需要对数据先做normalization，LR不受其影响</li>
<li>LR 可以产生概率，SVM 不能产生概率</li>
<li>SVM 只考虑局部的边界线附近的点，而逻辑回归考虑全局（远离的点对边界线的确定也起作用）</li>
<li>SVM的损失函数自带正则（结构风险最小化），而LR需要额外在损失函数上添加正则项</li>
</ul>
<h4 id="逻辑回归-与-随机森林-区别">逻辑回归 与 随机森林 区别</h4>
<p>随机森林等树算法都是非线性的，而LR是线性的。LR更侧重全局优化，而树模型主要是局部的优化。</p>
<hr>
<h3 id="svm">SVM</h3>
<p><strong>支持向量机（Support Vector Machines, SVM）</strong>是一种二分类模型。思想：间隔最大化来得到最优的分离超平面。方法：是将这个问题形式化为一个凸二次规划问题，也等价于一个正则化的合页损失函数最小化问题。其基本模型是定义在特征空间上的间隔最大的线性分类器。还可以通过使用核技巧，使其成为实质上的非线性分类器。SVM 的最优化算法是求解凸二次规划的最优化算法。</p>
<h4 id="什么是支持向量">什么是支持向量</h4>
<ul>
<li><p>训练数据集中与分离超平面距离最近的样本点的实例称为支持向量</p></li>
<li><p>更通俗的解释：</p></li>
<li>数据集中的某些点，位置比较特殊。比如 <code>x+y-2=0</code> 这条直线，假设出现在直线上方的样本记为 A 类，下方的记为 B 类。</li>
<li>在寻找找这条直线的时候，一般只需看两类数据，它们各自最靠近划分直线的那些点，而其他的点起不了决定作用。</li>
<li><p>这些点就是所谓的“支持点”，在数学中，这些点称为<strong>向量</strong>，所以更正式的名称为“<strong>支持向量</strong>”。</p></li>
</ul>
<blockquote>
<p><a href="https://blog.csdn.net/AerisIceBear/article/details/79588583" target="_blank" rel="noopener">SVM中支持向量的通俗解释</a> <a href="https://blog.csdn.net/woaidapaopao/article/details/77806273" target="_blank" rel="noopener">！！！要结合这个一起看</a></p>
</blockquote>
<h4 id="支持向量机的分类">支持向量机的分类</h4>
<ul>
<li>线性可分支持向量机</li>
<li>当训练数据<strong>线性可分</strong>时，通过<strong>硬间隔最大化</strong>，学习一个线性分类器，即线性可分支持向量机，又称<strong>硬间隔支持向量机</strong>。</li>
<li>线性支持向量机</li>
<li>当训练数据<strong>接近线性可分</strong>时，通过<strong>软间隔最大化</strong>，学习一个线性分类器，即线性支持向量机，又称<strong>软间隔支持向量机</strong>。</li>
<li>非线性支持向量机</li>
<li>当训练数据<strong>线性不可分</strong>时，通过使用<strong>核技巧</strong>及软间隔最大化，学习非线性支持向量机。</li>
</ul>
<h4 id="最大间隔超平面背后的原理">最大间隔超平面背后的原理</h4>
<blockquote>
<p>机器学习技法 (1-5) - 林轩田</p>
</blockquote>
<ul>
<li>相当于在<strong>最小化权重</strong>时对训练误差进行了约束——对比 L2 范数正则化，则是在最小化训练误差时，对权重进行约束</li>
</ul>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/TIM截图20180710112848.png">

</div>
<blockquote>
<p>与 L2 正则化的区别</p>
</blockquote>
<ul>
<li>相当于<strong>限制了模型复杂度</strong>——在一定程度上防止过拟合，具有更强的泛化能力</li>
</ul>
<h4 id="支持向量机推导">支持向量机推导</h4>
<ul>
<li>SVM 由简至繁包括：<strong>线性可分支持向量机</strong>、<strong>线性支持向量机</strong>以及<strong>非线性支持向量机</strong></li>
</ul>
<h4 id="一线性可分支持向量机推导">一、线性可分支持向量机推导</h4>
<blockquote>
<p>《统计学习方法》 &amp; <a href="https://blog.csdn.net/american199062/article/details/51322852#commentBox" target="_blank" rel="noopener">支持向量机SVM推导及求解过程</a> - CSDN博客</p>
</blockquote>
<ul>
<li>当训练数据<strong>线性可分</strong>时，通过<strong>硬间隔最大化</strong>，学习一个线性分类器，即线性可分支持向量机，又称<strong>硬间隔支持向量机</strong>。</li>
<li>线性 SVM 的推导分为两部分</li>
</ul>
<ol style="list-style-type: decimal">
<li>如何根据<strong>间隔最大化</strong>的目标导出 SVM 的<strong>标准问题</strong>；</li>
<li>拉格朗日乘子法对偶问题的求解过程.</li>
</ol>
<blockquote>
<p><strong>符号定义</strong></p>
</blockquote>
<ul>
<li>训练集 <span class="math inline">\(T\)</span></li>
</ul>
<p><span class="math display">\[
T= \{(x_{1}, y_{1}),(x_{2}, y_{2}), \cdots,(x_{N}, y_{N})\}
\]</span></p>
<ul>
<li><strong>分离超平面</strong> <span class="math inline">\((w, b)\)</span></li>
</ul>
<p><span class="math display">\[
w^{*} \cdot x+b^{*}=0
\]</span></p>
<p>如果使用映射函数，那么分离超平面为<br>
<span class="math display">\[
w^{*} \cdot \Phi(x)+b^{*}=0
\]</span></p>
<blockquote>
<p>映射函数 <span class="math inline">\(Φ(x)\)</span> 定义了从输入空间到特征空间的变换，特征空间通常是更高维的，甚至无穷维；方便起见，这里假设 <span class="math inline">\(Φ(x)\)</span> 做的是恒等变换。</p>
</blockquote>
<ul>
<li>分类决策函数 <span class="math inline">\(f(x)\)</span></li>
</ul>
<p><span class="math display">\[
f(x)=\operatorname{sign}\left(w^{*} \cdot x+b^{*}\right)
\]</span></p>
<h4 id="svm-标准问题的推导"><strong>SVM 标准问题的推导</strong></h4>
<ol style="list-style-type: decimal">
<li><strong>从“函数间隔”到“几何间隔”</strong></li>
</ol>
<blockquote>
<p>在分离超平面 <span class="math inline">\(w \cdot x+b =0\)</span> 确定的情况下，<span class="math inline">\(|w \cdot x+b|\)</span> 能够相对地表示点 <span class="math inline">\(x\)</span> 距离超平面的远近. 而 <span class="math inline">\(w \cdot x+b\)</span> 的符号与类别标记 <span class="math inline">\(y\)</span> 的符号一致时表示分类正确。所以可以用 <span class="math inline">\(y(w^{*} \cdot x+b^{*})\)</span> 来表示<strong>分类的正确性</strong>，这就是 <strong>函数间隔</strong></p>
</blockquote>
<p>给定训练集 <span class="math inline">\(T\)</span> 和 超平面 <span class="math inline">\((w, b)\)</span></p>
<ul>
<li>定义超平面关于样本点 <span class="math inline">\((x_i, y_i)\)</span> 的 函数间隔 <span class="math inline">\(\hat{\gamma}_i\)</span></li>
</ul>
<p><span class="math display">\[
   \hat{\gamma}_{i}=y_{i}\left(w \cdot x_{i}+b\right)
   \]</span></p>
<ul>
<li>定义超平面 <span class="math inline">\((w, b)\)</span> 关于数据集 <span class="math inline">\(T\)</span> 的 <span class="math inline">\(\mathbf{\color{red}{函数间隔 \ \hat{\gamma}}}\)</span> 为，超平面 <span class="math inline">\((w, b)\)</span> 关于 <span class="math inline">\(T\)</span> 中所有样本点 <span class="math inline">\((x_i, y_i)\)</span> 的函数间隔之最小值，即</li>
</ul>
<p><span class="math display">\[
   \begin{aligned} \hat{\gamma} &amp;=\min _{i=1, \cdots, N} y_{i}\left(w \cdot x_{i}+b\right) \\ &amp;=\min _{i=1, \cdots, N} \hat{\gamma}_{i} \end{aligned}
   \]</span></p>
<blockquote>
<p>函数间隔如果成比例的改变 <span class="math inline">\(w\)</span> 和 <span class="math inline">\(b\)</span> (比如都扩大2倍)，此时超平面没有改变，但是函数间隔却成了原来的 2 倍。为此，可以对超平面的法向量 <span class="math inline">\(w\)</span> 进行规范化， <span class="math inline">\(||w|| = 1\)</span>，使得间隔是确定的，这时的函数间隔就转换成为 <strong>几何间隔</strong>。（几何间隔最大的分离超平面是唯一的）</p>
</blockquote>
<p>对 <span class="math inline">\(w\)</span> 作规范化，使函数间隔成为<span class="math inline">\(\mathbf{\color{red}{几何间隔} \  \gamma}\)</span><br>
<span class="math display">\[
   \begin{aligned} \gamma &amp;=\min _{i=1, \cdots, N} y_{i}\left(\frac{w}{\color{red}{\|w\|}} x_{i}+\frac{b}{\color{red}{\|w\|}}\right) \\ &amp;=\min _{i=1, \cdots, N} \frac{\gamma_{i}}{\color{red}{\|w\|}} \end{aligned}
   \]</span></p>
<blockquote>
<p>超平面关于样本点<span class="math inline">\((x_i, y_i)\)</span> 的几何间隔，是实例点到超平面的带符号的距离，分类正确时，就是距离。</p>
</blockquote>
<p>函数间隔 <span class="math inline">\(\hat{\gamma}\)</span> 和几何间隔 <span class="math inline">\(\gamma\)</span> 的关系：<br>
<span class="math display">\[
   \gamma=\frac{\hat{\gamma}}{\|w\|}
   \]</span></p>
<ol start="2" style="list-style-type: decimal">
<li><strong>最大化几何间隔</strong></li>
</ol>
<p>要得到最大间隔的分离超平面，即几何间隔最大的分离超平面。即可将问题表示为下面的约束最优化问题：<br>
<span class="math display">\[
   \begin{array} {ll}
   &amp;{\color{Red}{\underset{w,b}{\max}}} \quad \gamma \\
   &amp; {\mathrm{s.t.} \quad\ y_i(\frac{w}{\color{Red} {\| w \|}}x_i+\frac{b}{\color{Red} {|| w ||}}) \geq \gamma,\quad i=1,2,\cdots,N}
   \end{array}
   \]</span><br>
即我们希望最大化超平面 <span class="math inline">\((w, b)\)</span> 关于训练数据集的几何间隔 <span class="math inline">\(\gamma\)</span> ，约束条件表示的是超平面 <span class="math inline">\((w, b)\)</span> 关于每个训练样本的几何间隔至少是 <span class="math inline">\(\gamma\)</span></p>
<p>由函数间隔与几何间隔的关系，等价于<br>
<span class="math display">\[
   \begin{array}{ll}
   {\color{Red}{\underset{w,b}{\max}}}  &amp; {\frac{\hat{\gamma}}{\color{Red}{\|w\|}}} \\ 
   {\text { s.t. }}  &amp; {y_{i}\left(w \cdot x_{i}+b\right) \geqslant \hat{\gamma}, \quad i=1,2, \cdots, N}\end{array}
   \]</span><br>
函数间隔 <span class="math inline">\(\hat{\gamma}\)</span> 的取值不会影响最终的超平面 <span class="math inline">\((w, b)\)</span> ，比如将 <span class="math inline">\(w\)</span> 和 <span class="math inline">\(b\)</span> 按比例改变为 <span class="math inline">\(\lambda w\)</span> 和 <span class="math inline">\(\lambda b\)</span>，此时函数间隔为 <span class="math inline">\(\lambda \hat{\gamma}\)</span>，此时的不等式约束没有影响，对目标函数的优化也没有影响，所以可取 <span class="math inline">\(\hat{\gamma} = 1\)</span></p>
<p>又最大化 <span class="math inline">\(\frac{1}{\|w\|}\)</span> 等价于最小化 <span class="math inline">\(\frac{1}{2}\|w\|^{2}\)</span>，于是得到 <span class="math inline">\(\mathbf{\color{red}{线性可分支持向量机的最优化问题，即SVM的标准问题}}\)</span>：<br>
<span class="math display">\[
   \begin{array}{ll}
   {\color{Red}{\underset{w,b}{\min}}} &amp; {\frac{1}{2} {\color{red}{ \|w\|^{2}}}} \\ 
   {\text { s.t. }} &amp; {y_{i}\left(w x_{i}+b\right)  {\color{red}{-1}} \geq 0, \quad i=1,2, \cdots, N}\end{array} \\
   {\color{green} {主问题 （primal \ problem）}}
   \]</span></p>
<blockquote>
<p>为什么令 <span class="math inline">\(\hat{\gamma} = 1\)</span>？——比例改变 <span class="math inline">\((ω,b)\)</span>，超平面不会改变，但函数间隔 <span class="math inline">\(\hat{\gamma}\)</span> 会成比例改变，因此可以通过等比例改变 <span class="math inline">\((ω,b)\)</span> 使函数间隔 <span class="math inline">\(\hat{\gamma} = 1\)</span></p>
</blockquote>
<p>这是一个<strong>凸二次优化</strong>问题，这种带约束的最优化问题，我们可以使用<strong>拉格朗日乘子法</strong>来求解。并且可以自然的引入<strong>核函数</strong>，进而推广到非线性的情况。</p>
<blockquote>
<p><strong>凸优化问题</strong>是指约束最优化问题：<br>
<span class="math display">\[
\begin{array}{ll}{\min _{w}} &amp; {f(w)} \\ {\text { s.t. }} &amp; {g_{i}(w) \leqslant 0, \quad i=1,2, \cdots, k} \\ &amp; {h_{i}(w)=0, \quad i=1,2, \cdots, l}\end{array}
\]</span><br>
其中，目标函数 <span class="math inline">\(f(w)\)</span> 约束函数 <span class="math inline">\(g_i(w)\)</span> 都是 <span class="math inline">\(\mathbf{R}^{n}\)</span> 上的连续可微的凸函数，约束函数 <span class="math inline">\(h_i(w)\)</span> 是 <span class="math inline">\(\mathbf{R}^{n}\)</span> 上的仿射函数。</p>
<p>当目标函数 <span class="math inline">\(f(w)\)</span> 是二次函数，且约束函数 <span class="math inline">\(g_i(w)\)</span> 是仿射函数时，上述凸优化问题成为<strong>凸二次规划问题</strong>。</p>
<p><span class="math inline">\(f(x)\)</span> 称为<strong>仿射函数</strong>，如果它满足 <span class="math inline">\(f(x) = a \cdot x + b, \quad a \in \mathbf{R}^{n}, \quad b \in \mathbf{R}, \quad x \in \mathbf{R}^{n}\)</span></p>
</blockquote>
<h4 id="svm-对偶算法的推导"><strong>SVM 对偶算法的推导</strong></h4>
<ol style="list-style-type: decimal">
<li><p>构建<strong>拉格朗日函数</strong><br>
<span class="math display">\[
   \begin{aligned} L(w,b,{\color{Red} \alpha})=&amp;\frac{1}{2}w^Tw-\sum_{i=1}^N{\color{Red} {\alpha_i}}[y_i(w^Tx_i+b)-1]\\ &amp;{\color{Red}{\alpha_i \geq 0}},\quad i=1,2,\cdots,N \end{aligned}
   \]</span></p></li>
<li><p>标准问题是求极小极大问题：</p></li>
</ol>
<p><span class="math display">\[
   \begin{aligned} {\color{Red}{\underset{w,b}{\min}}}\ {\color{Blue} {\underset{\alpha}{\max}}}\ L(w,b,\alpha) \end{aligned}
   \]</span><br>
其对偶问题为：</p>
<p><span class="math display">\[
   \begin{aligned} {\color{Blue} {\underset{\alpha}{\max}}}\ {\color{Red} {\underset{w,b}{\min}}}\ L(w,b,\alpha) \end{aligned}
   \]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>求 <span class="math inline">\(L\)</span> 对 <span class="math inline">\((w,b)\)</span> 的极小</li>
</ol>
<p><span class="math display">\[
   \begin{aligned} \mathrm{set}\quad \frac{\partial L}{\partial w}=0 \;\;&amp;\Rightarrow\; w-\sum_{i=1}^N {\color{Red}{\alpha_i y_i x_i}}=0\\ &amp;\Rightarrow\; w=\sum_{i=1}^N {\color{Red} {\alpha_i y_i x_i}} \end{aligned}
   \]</span></p>
<p><span class="math display">\[
   \begin{aligned} \mathrm{set}\quad \frac{\partial L}{\partial b}=0 \;\;&amp;\Rightarrow\; \sum_{i=1}^N {\color{Red} {\alpha_i y_i}}=0 \end{aligned}
   \]</span></p>
<p>结果代入 <span class="math inline">\(L\)</span>，有：<br>
<span class="math display">\[
   \begin{aligned} L(w,b,{\color{Red} \alpha}) &amp;=\frac{1}{2}w^Tw-\sum_{i=1}^N{\color{Red} {\alpha_i}}[y_i(w^Tx_i +b)-1]\\ &amp;=\frac{1}{2}w^Tw-w^T\sum_{i=1}^N \alpha_iy_ix_i-b\sum_{i=1}^N \alpha_iy_i +\sum_{i=1}^N \alpha_i\\ &amp;=\frac{1}{2}w^Tw-w^Tw +\sum_{i=1}^N \alpha_i\\ &amp;=-\frac{1}{2}w^Tw +\sum_{i=1}^N \alpha_i\\ &amp;=-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N \alpha_i\alpha_j\cdot y_iy_j\cdot {\color{Red} {x_i^Tx_j}} +\sum_{i=1}^N \alpha_i \end{aligned}
   \]</span><br>
即</p>
<p><span class="math display">\[
   \min _{w, b}  \quad L(w, b, \alpha)=-\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} \cdot y_{i} y_{j} \cdot {\color{Red}{x_{i}^{T} x_{j}}}+\sum_{i=1}^{N} \alpha_{i}
   \]</span></p>
<ol start="4" style="list-style-type: decimal">
<li>求 <span class="math inline">\(L\)</span> 对 <span class="math inline">\(α\)</span> 的极大，即<br>
<span class="math display">\[
   \begin{aligned} 
   &amp;\underset{\alpha}{\max} \quad -\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N \alpha_i\alpha_j\cdot y_iy_j\cdot x_i^Tx_j+\sum_{i=1}^N \alpha_i\\ 
   &amp;\ \mathrm{s.t.}\quad\; \sum_{i=1}^N \alpha_i y_i=0,\\
   &amp; \quad\; \quad\; \ {\color{Red} {\alpha_i \geq 0}},\quad i=1,2,\cdots,N \end{aligned}
   \]</span><br>
该问题的对偶问题为：<br>
<span class="math display">\[
   \begin{aligned} 
   &amp;{\color{Red} {\underset{\alpha}{\min}}} \quad\ \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N \alpha_i\alpha_j\cdot y_iy_j\cdot x_i^Tx_j-\sum_{i=1}^N \alpha_i\\ 
   &amp;\ \mathrm{s.t.}\quad\; \sum_{i=1}^N \alpha_i y_i=0, \\
   &amp; \  \quad\; \quad\; \  {\color{Red} {\alpha_i \geq 0}}, \quad i=1,2,\cdots,N \end{aligned}
   \]</span><br>
于是，标准问题最后等价于求解该<strong>对偶问题</strong></li>
</ol>
<blockquote>
<p>继续求解该优化问题，有 <a href="https://blog.csdn.net/ajianyingxiaoqinghan/article/details/73087304#t11" target="_blank" rel="noopener">SMO 方法</a></p>
<p><strong>SMO思想</strong>： 先固定 <span class="math inline">\(a_i\)</span> 之外的所有参数，然后求 <span class="math inline">\(a_i\)</span> 上的极值。由于存在约束 <span class="math inline">\(\sum^m_{i=1} a_i y_i = 0\)</span>，若固定 <span class="math inline">\(a_i\)</span> 之外的其他变量，则 <span class="math inline">\(a_i\)</span> 可由其他变量导出。于是，SMO不断执行如下两个步骤直至收敛：</p>
<ul>
<li>选取一对需要更新的变量 <span class="math inline">\(a_i\)</span> 和 <span class="math inline">\(a_j\)</span></li>
<li>固定 <span class="math inline">\(a_i\)</span> 和 <span class="math inline">\(a_j\)</span> 以外的参数，求解 上述对偶问题，获得更新后的 <span class="math inline">\(a_i\)</span> 和 <span class="math inline">\(a_j\)</span></li>
</ul>
</blockquote>
<ol start="5" style="list-style-type: decimal">
<li>设 <span class="math inline">\(α\)</span> 的解为 <span class="math inline">\(α^*\)</span>，则存在下标 <span class="math inline">\(j\)</span> 使 <span class="math inline">\(α_j &gt; 0\)</span>，可得标准问题的解为：<br>
<span class="math display">\[
   \begin{aligned} w^{*} &amp;=\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i} \\ b^{*} &amp;={\color{Red}{y_{j}}}-\sum_{i=1}^{N} \alpha_{i}^{*} y_{i}\left(x_{i}^{T} {\color{Red}{x_{j}}}\right) \end{aligned}
   \]</span><br>
可得分离超平面及分类决策函数为：<br>
<span class="math display">\[
   \begin{array}{c}{w^{*} \cdot x+b^{*}=0} \\ {f(x)=\operatorname{sign}\left(w^{*} \cdot x+b^{*}\right)} \end{array}
   \]</span></li>
</ol>
<h4 id="二线性支持向量机">二、线性支持向量机</h4>
<p>当训练数据<strong>接近线性可分</strong>时，训练数据中有一些异常点，将这些异常点去除后，剩下的大部分的样本点组成的集合是线性可分的。通过<strong>软间隔最大化</strong>，学习一个线性分类器，即线性支持向量机，又称<strong>软间隔支持向量机</strong>。</p>
<p>通过增加松弛因子 <span class="math inline">\(\xi_{i} \geq 0\)</span> 使函数间隔加上松弛变量大于等于1，则约束条件变成：<br>
<span class="math display">\[
y_{i}\left(w \cdot x_{i}+b\right) \geqslant 1-\xi_{i}
\]</span><br>
对每个松弛变量 <span class="math inline">\(\xi_{i}\)</span>，添加一个代价，目标函数为：<br>
<span class="math display">\[
\frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i}
\]</span><br>
<span class="math inline">\(C &gt; 0\)</span> 为惩罚参数，<span class="math inline">\(C\)</span> 值大时对误分类的惩罚增大，<span class="math inline">\(C\)</span> 值小时对误分类的惩罚减小。即最小化目标函数包含两层含义：使 <span class="math inline">\(\frac{1}{2}||w||^2\)</span> 尽量小即间隔尽量大，同事使误分类点的个数尽量小，<span class="math inline">\(C\)</span> 是调和二者的系数。</p>
<ul>
<li><span class="math inline">\(\mathbf{\color{red}{线性支持向量机的最优化问题}}\)</span></li>
</ul>
<p><span class="math display">\[
\begin{array}{ll}{\color{red}{\underset{w, b, \xi}{\min}}} &amp; {\frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i}} \\ {\text { s.t. }} &amp; {y_{i}\left(w \cdot x_{i}+b\right) \geqslant 1-\xi_{i}, \quad i=1,2, \cdots, N} \\ {} &amp; {\xi_{i} \geqslant 0, \quad i=1,2, \cdots, N}\end{array}
\]</span></p>
<ul>
<li>构造拉格朗日函数</li>
</ul>
<p><span class="math display">\[
L(w, b, \xi, \alpha, \mu) = \frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i}-\sum_{i=1}^{N} \alpha_{i}\left(y_{i}\left(w \cdot x_{i}+b\right)-1+\xi_{i}\right)-\sum_{i=1}^{N} \mu_{i} \xi_{i} \\
其中，\alpha_{i} \geqslant 0, \mu_{i} \geqslant 0
\]</span></p>
<ul>
<li>对 <span class="math inline">\(w, b, \xi\)</span> 求偏导</li>
</ul>
<p><span class="math display">\[
\begin{array}{l}{\frac{\partial L}{\partial w}=0 \Rightarrow w=\sum_{i=1}^{n} \alpha_{i} y_{i} x_{i}} \\ {\frac{\partial L}{\partial b}=0 \Rightarrow \sum_{i=1}^{n} \alpha_{i} y_{i}} = 0 \\ {\frac{\partial L}{\partial \xi}=0 \Rightarrow C-\alpha_{i}-u_{i}=0}\end{array}
\]</span></p>
<ul>
<li>将三式带入 <span class="math inline">\(L\)</span> 中，得</li>
</ul>
<p><span class="math display">\[
{\color{red}{\min _{w, b, \xi}}} \   L(w, b, \xi, \alpha, \mu)=-\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i}
\]</span></p>
<ul>
<li>再对 <span class="math inline">\({\color{red}{\underset{w, b, \xi}{\min}}} \  L(w, b, \xi, \alpha, \mu)\)</span> 求 <span class="math inline">\(a\)</span> 的极大，即得对偶问题：</li>
</ul>
<p><span class="math display">\[
\begin{aligned} &amp;
{\color{blue}{\max _{\alpha}}}  \quad\ -\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i} \\
&amp;\ \mathrm{s.t.}\quad\; \sum_{i=1}^N \alpha_i y_i=0,
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\begin{array}{l}{C-\alpha_{i}-u_{i}=0} \\ {\alpha_{i} \geq 0} \\ {u_{i} \geq 0, i=1,2, \cdots, n}\end{array} \} \Rightarrow 0 \leq \alpha_{i} \leq C
\]</span></p>
<ul>
<li>整理得到对偶问题的最优化问题：</li>
</ul>
<p><span class="math display">\[
\begin{aligned}
&amp;{\color{red}{\min _{\alpha}}} \quad\ \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} x_{j}\right)-\sum_{i=1}^{n} \alpha_{i} \\
&amp; s.t. \quad\ \sum^n_{i = 1} a_i y_i \\
&amp; \quad\  \quad\ \ 0 \leq \alpha_{i} \leq C, i=1,2, \cdots, N
\end{aligned}
\]</span></p>
<ul>
<li><p>求得最优解 <span class="math inline">\(a^*\)</span>，则存在下标 <span class="math inline">\(j\)</span> 使 <span class="math inline">\(0 &lt; α_j &lt; C\)</span>，可得标准问题的解为：<br>
<span class="math display">\[
  \begin{aligned} w^{*} &amp;=\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i} \\ b^{*} &amp;={\color{Red}{y_{j}}}-\sum_{i=1}^{N} \alpha_{i}^{*} y_{i}\left(x_{i}^{T} {\color{Red}{x_{j}}}\right) \end{aligned}
  \]</span></p></li>
<li><p>可得分离超平面及分类决策函数为：<br>
<span class="math display">\[
  \begin{array}{c}{w^{*} \cdot x+b^{*}=0} \\ {f(x)=\operatorname{sign}\left(w^{*} \cdot x+b^{*}\right)} \\ \end{array}
  \]</span></p></li>
</ul>
<h4 id="三核函数">三、核函数</h4>
<p><strong>引入核函数目的</strong>：把原坐标系里线性不可分的数据用核函数Kernel投影到另一个空间，尽量使得数据在新的空间里线性可分。</p>
<blockquote>
<p><strong>核函数</strong>表示将输入从输入空间映射到特征空间后得到的特征向量之间的内积</p>
</blockquote>
<p>核函数方法的广泛应用，与其特点是分不开的：</p>
<p>1）核函数的引入避免了“维数灾难”，大大减小了计算量。而输入空间的维数n对核函数矩阵无影响。因此，核函数方法可以有效处理高维输入。</p>
<p>2）无需知道非线性变换函数Φ的形式和参数。</p>
<p>3）核函数的形式和参数的变化会隐式地改变从输入空间到特征空间的映射，进而对特征空间的性质产生影响，最终改变各种核函数方法的性能。</p>
<p>4）核函数方法可以和不同的算法相结合，形成多种不同的基于核函数技术的方法，且这两部分的设计可以单独进行，并可以为不同的应用选择不同的核函数和算法。</p>
<ul>
<li><strong>常见的核函数</strong></li>
</ul>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/常见的核函数.png">

</div>
<h4 id="svm为什么用对偶问题来求解">SVM为什么用对偶问题来求解</h4>
<ol style="list-style-type: decimal">
<li>对偶问题将原始问题中的约束转为了对偶问题中的等式约束</li>
<li>方便核函数的引入</li>
<li>改变了问题的复杂度。由求特征向量 <span class="math inline">\(w\)</span> 转化为求比例系数 <span class="math inline">\(a\)</span>，在原始问题下，求解的复杂度与样本的维度有关，即 <span class="math inline">\(w\)</span> 的维度。在对偶问题下，只与样本数量有关。</li>
<li>求解更高效，因为只用求解比例系数 <span class="math inline">\(a\)</span>，而比例系数 <span class="math inline">\(a\)</span> 只有支持向量才为非0，其他全为0.</li>
</ol>
<h4 id="为何令间隔为1">为何令间隔为1</h4>
<h4 id="什么是拉格朗日对偶">什么是拉格朗日对偶</h4>
<h4 id="什么是kkt条件">什么是KKT条件</h4>
<p><a href="http://whatbeg.com/2017/04/13/svmlearning.html" class="uri" target="_blank" rel="noopener">http://whatbeg.com/2017/04/13/svmlearning.html</a></p>
<hr>
<h3 id="反向传播">* 反向传播</h3>
<hr>
<h3 id="knn">KNN</h3>
<p>给定一个训练数据集，对于新输入的实例，在训练数据集中找到与该实例最近的 <span class="math inline">\(k\)</span> 个实例，这 <span class="math inline">\(k\)</span> 个实例属于哪个类最多，就把该输入实例分为那个类。</p>
<hr>
<h3 id="决策树">决策树</h3>
<p>决策树是一种基本的分类与回归方法。学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型。测试时，对新的数据集，利用决策树模型进行分类。<br>
决策树是一种自上而下，对样本数据进行树形分类的过程，由结点和有向边组成。 结点分为内部结点和叶结点， 其中每个内部结点表示一个特征或属性， 叶结点表示类别。 从顶部根结点开始， 所有样本聚在一起。 经过根结点的划分， 样本被分到不同的子结点中。 再根据子结点的特征进一步划分， 直至所有样本都被归到某一个类别（ 即叶结点） 中。</p>
<h4 id="决策树的三要素">决策树的三要素</h4>
<p>决策树的训练通常由三部分组成：<strong>特征选择</strong>、<strong>树的生成</strong>、<strong>剪枝</strong></p>
<ul>
<li><strong>特征选择</strong>：从训练数据中众多的特征中选择一个特征作为当前节点的分裂标准，如何选择特征有着很多不同量化评估标准，从而衍生出不同的决策树算法。</li>
<li><p><strong>树的生成</strong>：根据选择的特征评估标准，从上至下递归地生成子节点，直到数据集不可分则决策树停止生长。</p></li>
<li><p><strong>剪枝</strong>：决策树容易过拟合，一般来需要剪枝，缩小树结构规模、缓解过拟合。剪枝技术有预剪枝和后剪枝两种。</p></li>
</ul>
<h4 id="剪枝处理的作用及策略">剪枝处理的作用及策略</h4>
<p><strong>作用</strong>：缩小树的规模、缓解过拟合问题。</p>
<p>在决策树算法中，为了尽可能正确分类训练样本， 节点划分过程不断重复， 有时候会造成决策树分支过多，以至于将训练样本集自身特点当作泛化特点， 而导致过拟合。 因此可以采用剪枝处理来去掉一些分支来降低过拟合的风险。</p>
<p>剪枝的基本策略有预剪枝(pre-pruning) 和 后剪枝(post-pruning)</p>
<ul>
<li><strong>预剪枝</strong>：在决策树生成过程中，在每个节点划分前先估计其划分后的泛化性能， 如果不能提升，则停止划分，将当前节点标记为叶结点。</li>
<li><strong>后剪枝</strong>：生成决策树以后，再自下而上对非叶结点进行考察， 若将此节点标记为叶结点可以带来泛化性能提升，则修改之。</li>
</ul>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/预剪枝.png">

</div>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/后剪枝.png">

</div>
<h4 id="决策树学习基本算法">决策树学习基本算法</h4>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/2-5.png">

</div>
<h4 id="分类树-id3-决策树与-c4.5-决策树">[分类树] ID3 决策树与 C4.5 决策树</h4>
<p>ID3 决策树和 C4.5 决策树的<strong>区别</strong>在于：前者使用 <strong>信息增益</strong> 来进行特征选择，而后者使用 <strong>信息增益比</strong>。</p>
<h4 id="回归树---cart-决策树">[回归树] - CART 决策树</h4>
<p>CART 使用 <strong>基尼指数</strong> 来选择属性划分</p>
<h4 id="熵">熵</h4>
<p><strong>熵</strong>：度量随机变量的不确定性。<br>
定义：假设随机变量 <span class="math inline">\(X\)</span> 的可能取值有 <span class="math inline">\(x_{1},x_{2},...,x_{n}\)</span>，对于每一个可能的取值 <span class="math inline">\(x_{i}\)</span>，其概率为<span class="math inline">\(P(X=x_{i})=p_{i}, \ \ i=1,2, ...n\)</span>。随机变量的熵为：<br>
<span class="math display">\[
H(X)=-\sum_{i=1}^{n}p_{i}log_{2}p_{i}
\]</span><br>
对于样本集合 <span class="math inline">\(D\)</span>，类别数为 <span class="math inline">\(K\)</span>，每个类别的概率为<span class="math inline">\(\frac{|C_{k}|}{|D|}\)</span>，其中 <span class="math inline">\({|C_{k}|}\)</span>为类别为 <span class="math inline">\(k\)</span> 的样本个数，<span class="math inline">\(|D|\)</span>为样本总数。样本集合 <span class="math inline">\(D\)</span> 的熵为：<br>
<span class="math display">\[
H(D)=-\sum_{k=1}^{K}\frac{|C_{k}|}{|D|}log_{2}\frac{|C_{k}|}{|D|}
\]</span></p>
<h4 id="条件熵">条件熵</h4>
<p>计算某个特征 <span class="math inline">\(A\)</span> 对于数据集 <span class="math inline">\(D\)</span> 的条件熵 <span class="math inline">\(H(D|A)\)</span> 为：<br>
<span class="math display">\[
H(D | A)=\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} H\left(D_{i}\right)=\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D |}\left(-\sum_{k=1}^{K} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|} \log _{2} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|}\right)
\]</span><br>
其中， <span class="math inline">\(D_i\)</span> 表示 <span class="math inline">\(D\)</span> 中特征 <span class="math inline">\(A\)</span> 取第 <span class="math inline">\(i\)</span> (年龄、长相、工资、写代码)个值的样本子集， <span class="math inline">\(D_{ik}\)</span> 表示 <span class="math inline">\(D_i\)</span> 中属于第 <span class="math inline">\(k\)</span> (见、不见)类的样本子集。</p>
<h4 id="信息增益">信息增益</h4>
<ul>
<li><strong>定义</strong>：<strong>以某特征划分数据集前后的熵的差值</strong>。</li>
</ul>
<p>以表示样本集合的不确定性，熵越大，样本的不确定性就越大。因此可以使用划分前后集合熵的差值来衡量使用当前特征对于样本集合 <span class="math inline">\(D\)</span> 划分效果的好坏。</p>
<p>假设划分前样本集合 <span class="math inline">\(D\)</span> 的熵为 <span class="math inline">\(H(D)\)</span> 。使用某个特征 <span class="math inline">\(A\)</span> 划分数据集 <span class="math inline">\(D\)</span>，计算划分后的数据子集的熵为 <span class="math inline">\(H(D|A)\)</span>。<br>
则信息增益为：<br>
<span class="math display">\[
g(D,A)=H(D)-H(D|A)
\]</span></p>
<blockquote>
<p><em>注：</em> 在决策树构建的过程中我们总是希望集合往最快到达纯度更高的子集合方向发展，因此我们总是选择使得信息增益最大的特征来划分当前数据集 <span class="math inline">\(D\)</span>。</p>
</blockquote>
<ul>
<li><strong>思想</strong>：计算所有特征划分数据集 <span class="math inline">\(D\)</span>，得到多个特征划分数据集 <span class="math inline">\(D\)</span> 的信息增益，从这些信息增益中选择最大的，因而当前结点的划分特征便是使信息增益最大的划分所使用的特征。<br>
</li>
<li><strong>缺点：信息增益对取值数目较多的属性有所偏好</strong></li>
<li><strong>举例说明</strong></li>
</ul>
<p>假设共有5个人追求场景中的女孩， 年龄有两个属性（老， 年轻） ， 长相有三个属性（帅， 一般， 丑） ， 工资有三个属性（高， 中等， 低） ， 会写代码有两个属性（会， 不会） ， 最终分类结果有两类（见， 不见） 。 我们根据女孩有监督的主观意愿可以得到下表：</p>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center">年龄</th>
<th align="center">长相</th>
<th align="center">工资</th>
<th align="center">写代码</th>
<th align="center">类别</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">小A</td>
<td align="center">老</td>
<td align="center">帅</td>
<td align="center">高</td>
<td align="center">不会</td>
<td align="center">不见</td>
</tr>
<tr class="even">
<td align="center">小B</td>
<td align="center">年轻</td>
<td align="center">一般</td>
<td align="center">中等</td>
<td align="center">会</td>
<td align="center">见</td>
</tr>
<tr class="odd">
<td align="center">小C</td>
<td align="center">年轻</td>
<td align="center">丑</td>
<td align="center">高</td>
<td align="center">不会</td>
<td align="center">不见</td>
</tr>
<tr class="even">
<td align="center">小D</td>
<td align="center">年轻</td>
<td align="center">一般</td>
<td align="center">高</td>
<td align="center">会</td>
<td align="center">见</td>
</tr>
<tr class="odd">
<td align="center">小L</td>
<td align="center">年轻</td>
<td align="center">一般</td>
<td align="center">低</td>
<td align="center">不会</td>
<td align="center">不见</td>
</tr>
</tbody>
</table>
<p>（1）首先计算数据集 <span class="math inline">\(D\)</span> 的信息熵<br>
<span class="math display">\[
H(D)=-\sum_{k=1}^{K}\frac{|C_{k}|}{|D|}log_{2}\frac{|C_{k}|}{|D|}   \ \ \ \ \  ，(k：见/不见)
\]</span></p>
<p><span class="math display">\[
H(D)=-\frac{3}{5} \log _{2} \frac{3}{5}-\frac{2}{5} \log _{2} \frac{2}{5}=0.971
\]</span></p>
<p>（2）然后计算每个特征对数据集的条件熵：<br>
<span class="math display">\[
H(D | A)=\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} H\left(D_{i}\right)=\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D |}\left(-\sum_{k=1}^{K} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|} \log _{2} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|}\right)
\]</span></p>
<p><span class="math display">\[
\begin{align*}  
H(D|年龄) &amp; = \frac{1}{5}H(老) + \frac{4}{5}H(年轻) \\
        &amp; =\frac{1}{5}(-0)+\frac{4}{5}\left(-\frac{2}{4} \log _{2} \frac{2}{4}-\frac{2}{4} \log _{2} \frac{2}{4}\right)=0.8
\\
H(D|长相) &amp; = \frac{1}{5}H(帅) + \frac{3}{5}H(一般) +  \frac{1}{5}H(丑)  \\
        &amp; =0+\frac{3}{5}\left(-\frac{2}{3} \log _{2} \frac{2}{3}-\frac{1}{3} \log _{2} \frac{1}{3}\right)+0=0.551
\\
H(D|工资) &amp; = \frac{1}{5}H(高) + \frac{3}{5}H(中等) +  \frac{1}{5}H(低)  \\
        &amp; =\frac{3}{5}\left(-\frac{2}{3} \log _{2} \frac{2}{3}-\frac{1}{3} \log _{2} \frac{1}{3}\right)+0+0=0.551
\\ 
H(D|写代码) &amp; = \frac{3}{5}H(不会) + \frac{2}{5}H(会) \\
          &amp; =\frac{3}{5}(0)+\frac{2}{5}(0)=0
\end{align*}
\]</span></p>
<p>（3）然后计算每个特征的信息增益：<br>
<span class="math display">\[
g(D,A)=H(D)-H(D|A)
\]</span></p>
<p><span class="math display">\[
g(D, 年龄) = H(D) - H(D|年龄) = 0.971 - 0.8 = 0.171 \\
g(D|长相) = 0.42 \\
g(D|工资) = 0.42 \\
g(D|写代码) = 0.971
\]</span></p>
<h4 id="信息增益率">信息增益率</h4>
<p><strong>（2）信息增益率</strong></p>
<p><strong>信息增益率本质</strong>：在信息增益的基础之上乘上一个惩罚参数。特征个数较多时，惩罚参数较小；特征个数较少时，惩罚参数较大。<br>
<strong>惩罚参数</strong>：数据集 <span class="math inline">\(D\)</span> 以特征 <span class="math inline">\(A\)</span> 作为随机变量的熵的倒数。<br>
<span class="math display">\[
{\color{red}{信息增益率}} = {\color{red}{惩罚参数}} \times {\color{red}{信息增益}}   \\
\begin{align*}  
信息增益率：&amp; g_{R}(D, A)=\frac{g(D, A)}{H_{A}(D)} \\ 
数据集D以A作为随机变量的熵(取值熵)：&amp;H_{A}(D)=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D |} \log _{2} \frac{\left|D_{i}\right|}{|D|}\\ (惩罚参数的倒数)
\end{align*}
\]</span></p>
<blockquote>
<p><strong>缺点：信息增益率对取值数目较少的属性有所偏好</strong><br>
所以 C4.5 算法并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。</p>
</blockquote>
<p>计算数据集 <span class="math inline">\(D\)</span> 关于每个特征的取值熵：<br>
<span class="math display">\[
H_{年龄}(D) = -\frac{1}{5}log_2\frac{1}{5} - \frac{4}{5}log_2\frac{4}{5} = 0.722 \\
H_{长相}(D) = -\frac{1}{5}log_2\frac{1}{5} - \frac{3}{5}log_2\frac{3}{5}  - \frac{1}{5}log_2\frac{1}{5}= 1.371 \\
H_{工资}(D) = -\frac{3}{5}log_2\frac{3}{5} - \frac{1}{5}log_2\frac{1}{5}  - \frac{1}{5}log_2\frac{1}{5}= 1.371 \\
H_{年龄}(D) = -\frac{3}{5}log_2\frac{3}{5} - \frac{2}{5}log_2\frac{2}{5} = 0.971
\]</span><br>
然后计算各个特征的信息增益率：<br>
<span class="math display">\[
g_R(D, 年龄) = 0.236， g_R(D, 长相) = 0.402 \\
g_R(D, 工资) =0.402， g_R(D, 写代码) = 1
\]</span><br>
信息增益比最大的仍是特征“写代码”， 但通过信息增益比， 特征“年龄”对应的指标上升了， 而特征“长相”和特征“工资”却有所下降。</p>
<h4 id="最大基尼指数gini">最大基尼指数（Gini）</h4>
<p>Gini 描述的是数据的纯度， 与信息熵含义类似。<br>
<span class="math display">\[
\operatorname{Gini}(D)=1-\sum_{k=1}^{K}\left(\frac{\left|C_{k}\right|}{|D|}\right)^{2}
\]</span><br>
CART 在每一次迭代中选择基尼指数最小的特征及其对应的切分点进行分类。但与ID3、 C4.5不同的是， CART是一颗二叉树， 采用二元切割法， 每一步将数据按特征A的取值切成两份， 分别进入左右子树。 <strong>特征 <span class="math inline">\(A\)</span> 的Gini指数</strong> 定义为：<br>
<span class="math display">\[
\operatorname{Gini}(D | A)=\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} \operatorname{Gini}\left(D_{i}\right)
\]</span><br>
计算各个特征的 Gini 指数：<br>
<span class="math display">\[
Gini(D|年龄=老) = 0.4，Gini(D|年龄=年轻) = 0.4 \\
Gini(D|长相=帅) = 0.4，Gini(D|长相=丑) = 0.4 \\
Gini(D|写代码=会) = 0，Gini(D|写代码=不会) = 0 \\
Gini(D|工资=高) = 0.47，Gini(D|工资=中等) = 0.3，Gini(D|工资=低) = 0.4
\]</span><br>
在“年龄”“长相”“工资”“写代码”四个特征中， 我们可以很快地发现特征“写代码”的Gini指数最小为0， 因此选择特征“写代码”作为最优特征， “写代码=会”为最优切分点。 按照这种切分， 从根结点会直接产生两个叶结点， 基尼指数降为0， 完成决策树生长。</p>
<h4 id="id3c4.5cart决策树区别与联系">ID3、C4.5、CART决策树区别与联系</h4>
<h4 id="决策树算法优缺点">决策树算法优缺点</h4>
<p><strong>优点：</strong></p>
<ul>
<li>易理解，解释起来简单</li>
<li>可以用于小数据集</li>
<li>复杂度较小，为用于训练决策树的数据点的对数</li>
<li>能够处理多输出的问题</li>
<li>对缺失值不敏感</li>
<li>可以处理不相关特征数据</li>
<li>效率高，决策树只需要一次构建，反复使用，每一次预测的最大计算次数不超过决策树的深度</li>
<li>相比于其他算法智能分析一种类型变量，决策树算法可处理数字和数据的类别</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>对连续性的字段比较难预测</li>
<li><p>容易出现过拟合</p></li>
<li>当类别太多时，错误可能就会增加的比较快</li>
<li>在处理特征关联性比较强的数据时表现得不是太好</li>
<li><p>对于各类别样本数量不一致的数据，在决策树当中，信息增益的结果偏向于那些具有更多数值的特征</p></li>
</ul>
<hr>
<h3 id="朴素贝叶斯">朴素贝叶斯</h3>
<h4 id="概率知识">概率知识</h4>
<p><strong>条件概率</strong></p>
<p>设 <span class="math inline">\(P(A) &gt; 0\)</span> ，条件概率为：<span class="math inline">\(P(B|A) = \frac{P(AB)}{P(A)}\)</span>，称为在 <span class="math inline">\(A\)</span> 发生条件下，<span class="math inline">\(B\)</span> 发生的概率</p>
<p><strong>乘法公式</strong></p>
<p>① <span class="math inline">\(P(AB) = P(A) \cdot P(B|A)\)</span></p>
<p>② <span class="math inline">\(P(ABC) = P(A) \cdot P(B|A) \cdot P(C|AB)\)</span></p>
<p><strong>独立性</strong></p>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/条件独立.jpg">

</div>
<p><strong>全概率公式</strong></p>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/全概率公式.png">

</div>
<p><strong>贝叶斯公式</strong></p>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/贝叶斯公式.png">

</div>
<h4 id="图解极大似然估计">图解极大似然估计</h4>
<p>极大似然估计的原理，用一张图片来说明，如下图所示：</p>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/2.19.1.1.png">

</div>
<p>例：有两个外形完全相同的箱子，1号箱有99只白球，1只黑球；2号箱有1只白球，99只黑球。在一次实验中，取出的是黑球，请问是从哪个箱子中取出的？</p>
<p>一般的根据经验想法，会猜测这只黑球最像是从2号箱取出，此时描述的“最像”就有“最大似然”的意思，这种想法常称为“最大似然原理”。</p>
<h4 id="极大似然估计原理">极大似然估计原理</h4>
<blockquote>
<p>总结起来，最大似然估计的目的就是：<strong>利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值。</strong></p>
</blockquote>
<p>极大似然估计是建立在极大似然原理的基础上的一个统计方法。极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“<strong>模型已定，参数未知</strong>”。通过若干次试验，观察其结果，利用试验结果得到某个参数值能够使样本出现的概率为最大，则称为极大似然估计。</p>
<p>由于样本集中的样本都是独立同分布，可以只考虑一类样本集<span class="math inline">\(D\)</span>，来估计参数向量 <span class="math inline">\(\vec\theta\)</span>。记已知的样本集为：<br>
<span class="math display">\[
D=\vec x_{1},\vec x_{2},...,\vec x_{n}
\]</span><br>
似然函数（likelihood function）：联合概率密度函数 <span class="math inline">\(p(D|\vec\theta )\)</span> 称为相对于 <span class="math inline">\(\vec x_{1},\vec x_{2},...,\vec x_{n}\)</span> 的 <span class="math inline">\(\vec\theta\)</span> 的似然函数。<br>
<span class="math display">\[
l(\vec\theta )=p(D|\vec\theta ) =p(\vec x_{1},\vec x_{2},...,\vec x_{n}|\vec\theta )=\prod_{i=1}^{n}p(\vec x_{i}|\vec \theta )
\]</span><br>
如果 <span class="math inline">\(\hat{\vec\theta}\)</span> 是参数空间中能使似然函数 <span class="math inline">\(l(\vec\theta)\)</span> 最大的 <span class="math inline">\(\vec\theta\)</span> 值，则 <span class="math inline">\(\hat{\vec\theta}\)</span> 应该是“最可能”的参数值，那么 <span class="math inline">\(\hat{\vec\theta}\)</span> 就是<span class="math inline">\(\theta\)</span>的极大似然估计量。它是样本集的函数，记作：<br>
<span class="math display">\[
\hat{\vec\theta}=d(D)= \mathop {\arg \max}_{\vec\theta} l(\vec\theta )
\]</span><br>
<span class="math inline">\(\hat{\vec\theta}(\vec x_{1},\vec x_{2},...,\vec x_{n})\)</span> 称为极大似然函数估计值。</p>
<h4 id="朴素贝叶斯-1">朴素贝叶斯</h4>
<p>朴素贝叶斯法是基于 <strong>贝叶斯定理</strong> 和 <strong>特征条件独立假设</strong> 的分类方法。对于给定的训练数据集，首先基于特征条件独立假设学习 输入/输出的联合概率分布；然后基于此模型，对给定的输入 <span class="math inline">\(x\)</span>，利用贝叶斯定理求出后验概率最大的输出 <span class="math inline">\(y\)</span> .</p>
<p>输入空间 <span class="math inline">\(\mathcal{X} \subseteq \mathbf{R}^{n}\)</span> 为 <span class="math inline">\(n\)</span> 维向量的集合，输出空间为类别标记集合 <span class="math inline">\(\mathcal{Y}=\left\{c_{1}\right. c_2, ..., c_k \}\)</span>. 输入特征向量<span class="math inline">\(x \in \mathcal{X}\)</span> ，输入为类别标记 <span class="math inline">\(y \in \mathcal{Y}\)</span>. <span class="math inline">\(X\)</span> 是定义在输入空间上的随机向量， <span class="math inline">\(Y\)</span> 是定义在输出空间 <span class="math inline">\(\mathcal{Y}\)</span> 上的随机变量. <span class="math inline">\(P(X, Y)\)</span> 是 <span class="math inline">\(X\)</span> 和 <span class="math inline">\(Y\)</span> 的联合概率分布.</p>
<p>训练数据集： <span class="math inline">\(T = \{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\}\)</span> ，由 <span class="math inline">\(P(X, Y)独立同分布产生\)</span>.</p>
<p>朴素贝叶斯算法流程：</p>
<ul>
<li>计算 <strong>先验概率</strong></li>
</ul>
<p><span class="math display">\[
P\left(Y=c_{k}\right), \quad k=1,2, \cdots, K
\]</span></p>
<ul>
<li>计算 <strong>条件概率</strong></li>
</ul>
<p><span class="math display">\[
\begin{aligned} P\left(X=x | Y=c_{k}\right) &amp;=P\left(X^{(1)}=x^{(1)}, \cdots, X^{(n)}=x^{(n)} | Y=c_{k}\right) \\ &amp;=\prod_{j=1}^{n} P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right) \end{aligned} \\
\color{red}{(朴素贝叶斯对条件概率分布作了条件独立性假设)}
\]</span></p>
<blockquote>
<p>从而学习到联合概率分布 <span class="math inline">\(P(X, Y)\)</span>。朴素贝叶斯法实际上学习到生成数据的机制，所以属于生成模型。条件独立假设等于说用于分类的特征在类确定的条件下都是条件独立的。</p>
</blockquote>
<ul>
<li>根据贝叶斯定理，将后验概率最大的类作为 <span class="math inline">\(x\)</span> 的类输出</li>
</ul>
<blockquote>
<p><span class="math display">\[
贝叶斯定理：  P(Y = c_k | X = x) = \frac{P(X = x|Y = c_k) P(Y = c_k)}{\sum_{k}P(X = x|Y = c_k)P(Y = c_k)}
\]</span></p>
</blockquote>
<p><span class="math display">\[
P\left(Y=c_{k} | X=x\right)=\frac{P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right)}{\sum_{k} P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right)}, \quad k=1,2, \cdots, K
\]</span></p>
<p>以上是朴素贝叶斯法分类的基本公式。于是，朴素贝叶斯分类器可以表示为：<br>
<span class="math display">\[
y=f(x)=\arg \max _{c_{k}} \frac{P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right)}{\sum_{k} P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right)}
\]</span><br>
注意到，公式中分母对所有的 <span class="math inline">\(c_k\)</span> 都是相同的，所以：<br>
<span class="math display">\[
y=\arg \max _{a} P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right)
\]</span></p>
<h4 id="举例说明">举例说明</h4>
<p>试由下表的训练数据学习一个朴素贝叶斯分类器，并确定 <span class="math inline">\(x = (2, S)^T\)</span> 的类的标记 <span class="math inline">\(y\)</span>. 表中 <span class="math inline">\(X^{1}, X{2}\)</span> 为特征，取值的集合分别为 <span class="math inline">\(A_1 = \{1, 2, 3\}, A_2 = \{S, M, L\}, Y\)</span> 为类标记，<span class="math inline">\(Y \in C=\{1,-1\}\)</span></p>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center">1</th>
<th align="center">2</th>
<th align="center">3</th>
<th align="center">4</th>
<th align="center">5</th>
<th align="center">6</th>
<th align="center">7</th>
<th align="center">8</th>
<th align="center">9</th>
<th align="center">10</th>
<th align="center">11</th>
<th align="center">12</th>
<th align="center">13</th>
<th align="center">14</th>
<th align="center">15</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(X^{(1)}\)</span></td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">2</td>
<td align="center">2</td>
<td align="center">2</td>
<td align="center">2</td>
<td align="center">2</td>
<td align="center">3</td>
<td align="center">3</td>
<td align="center">3</td>
<td align="center">3</td>
<td align="center">3</td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(X^{(2)}\)</span></td>
<td align="center"><span class="math inline">\(S\)</span></td>
<td align="center"><span class="math inline">\(M\)</span></td>
<td align="center"><span class="math inline">\(M\)</span></td>
<td align="center"><span class="math inline">\(S\)</span></td>
<td align="center"><span class="math inline">\(S\)</span></td>
<td align="center"><span class="math inline">\(S\)</span></td>
<td align="center"><span class="math inline">\(M\)</span></td>
<td align="center"><span class="math inline">\(M\)</span></td>
<td align="center"><span class="math inline">\(L\)</span></td>
<td align="center"><span class="math inline">\(L\)</span></td>
<td align="center"><span class="math inline">\(L\)</span></td>
<td align="center"><span class="math inline">\(M\)</span></td>
<td align="center"><span class="math inline">\(M\)</span></td>
<td align="center"><span class="math inline">\(L\)</span></td>
<td align="center"><span class="math inline">\(L\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(Y\)</span></td>
<td align="center">-1</td>
<td align="center">-1</td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">-1</td>
<td align="center">-1</td>
<td align="center">-1</td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">-1</td>
</tr>
</tbody>
</table>
<ol style="list-style-type: decimal">
<li>计算先验概率</li>
</ol>
<p><span class="math inline">\(P(Y=1)=\frac{9}{15}, \quad P(Y=-1)=\frac{6}{15}\)</span></p>
<ol start="2" style="list-style-type: decimal">
<li>计算条件概率</li>
</ol>
<p><span class="math inline">\(P\left(X^{(1)}=1 | Y=1\right)=\frac{2}{9}, \quad \quad P\left(X^{(1)}=2 | Y=1\right)=\frac{3}{9}, \quad \quad P\left(X^{(1)}=3 | Y=1\right)=\frac{4}{9}\)</span><br>
<span class="math inline">\(P\left(X^{(2)}=S | Y=1\right)=\frac{1}{9}, \quad \quad P\left(X^{(2)}=M | Y=1\right)=\frac{4}{9}, \ \quad P\left(X^{(2)}=L | Y=1\right)=\frac{4}{9}\)</span><br>
<span class="math inline">\(P\left(X^{(1)}=1 | Y=-1\right)=\frac{3}{6}, \ \quad P\left(X^{(1)}=2 | Y=-1\right)=\frac{2}{6}, \ \quad P\left(X^{(1)}=3 | Y=-1\right)=\frac{1}{6}\)</span><br>
<span class="math inline">\(P\left(X^{(2)}=S | Y=-1\right)=\frac{3}{6}, \ \quad P\left(X^{(2)}=M | Y=-1\right)=\frac{2}{6}, \quad P\left(X^{(2)}=L | Y=-1\right)=\frac{1}{6}\)</span></p>
<ol start="3" style="list-style-type: decimal">
<li>对于给定的 <span class="math inline">\(x = (2, S)^T\)</span>，计算：<br>
<span class="math display">\[
\begin{array}{l}{P(Y=1) P\left(X^{(i)}=2 | Y=1\right) P\left(X^{(2)}=S | Y=1\right)=\frac{9}{15} \cdot \frac{3}{9} \cdot \frac{1}{9}=\frac{1}{45}} \\ {P(Y=-1) P\left(X^{(1)}=2 | Y=-1\right) P\left(X^{(2)}=S | Y=-1\right)=\frac{6}{15} \cdot \frac{2}{6} \cdot \frac{3}{6}=\frac{1}{15}}\end{array}
\]</span><br>
因为 <span class="math inline">\(P(Y=-1) P\left(X^{(1)}=2 | Y=-1\right) P\left(X^{(2)}=S | Y=-1\right)\)</span> 最大，所以 <span class="math inline">\(y = -1\)</span> .</li>
</ol>
<hr>
<h3 id="k-means">K-means</h3>
<h4 id="算法流程"><strong>算法流程</strong></h4>
<p>K-means 是最普及的聚类算法，算法接受一个未标记的数据集，然后将数据聚类成不同的组。<br>
K-means 是一个迭代算法，假设我们想要将数据聚类成 <span class="math inline">\(n\)</span> 个组，其方法为:<br>
（1）首先选择 <span class="math inline">\(K\)</span> 个随机的点，称为聚类中心（ cluster centroids）<br>
（2）对于数据集中的每一个数据，按照距离 <span class="math inline">\(K\)</span> 个中心点的距离，将其与距离最近的中心点关联起来，与同一个中心点关联的所有点聚成一类。<br>
（3）计算每一个组的平均值，将该组所关联的中心点移动到平均值的位置。<br>
（4）重复步骤 2-4 直至中心点不再变化。</p>
<h4 id="伪代码实现">伪代码实现</h4>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/k-means.png">

</div>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 迭代方式计算样本点所属聚类中心索引</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">findClosestCentroids</span><span class="params">(x, cent)</span>:</span></span><br><span class="line">    k = np.size(cent, <span class="number">0</span>)</span><br><span class="line">    idx = np.zeros((np.size(x, <span class="number">0</span>),), dtype=int)</span><br><span class="line">    dist = np.zeros((k, <span class="number">1</span>))</span><br><span class="line">    m = np.size(x, <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(k):</span><br><span class="line">            dist[j] = np.sum((x[i, :] - cent[j, :])**<span class="number">2</span>)</span><br><span class="line">        idx[i] = np.argmin(dist)</span><br><span class="line">    <span class="keyword">return</span> idx</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新每组聚类中心</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">computeCentroids</span><span class="params">(x, idx, k)</span>:</span></span><br><span class="line">    m, n = np.shape(x)</span><br><span class="line">    cent = np.zeros((k, n))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">        x_idx = np.where(idx == i)</span><br><span class="line">        cent[i, :] = np.sum(x[x_idx], <span class="number">0</span>) / np.size(x[x_idx], <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> cent</span><br></pre></td></tr></table></figure>
<h4 id="k值的选择-手肘法">K值的选择 手肘法</h4>
<p>手肘法的核心指标是SSE(sum of the squared errors，误差平方和)<br>
<span class="math display">\[
S S E=\sum_{i=1}^{k} \sum_{p \in C_{i}}\left|p-m_{i}\right|^{2}
\]</span><br>
其中，<span class="math inline">\(C_i\)</span> 是第 <span class="math inline">\(i\)</span> 个簇，<span class="math inline">\(p\)</span> 是 <span class="math inline">\(C_i\)</span> 中的样本点，<span class="math inline">\(m_i\)</span> 是 <span class="math inline">\(C_i\)</span> 的质心（Ci中所有样本的均值），SSE是所有样本的聚类误差，代表了聚类效果的好坏。</p>
<p>手肘法的核心思想是：随着聚类数 <span class="math inline">\(k\)</span> 的增大，样本划分会更加精细，每个簇的聚合程度会逐渐提高，那么误差平方和SSE自然会逐渐变小。并且，当 <span class="math inline">\(k\)</span> 小于真实聚类数时，由于 <span class="math inline">\(k\)</span> 的增大会大幅增加每个簇的聚合程度，故SSE的下降幅度会很大，而当 <span class="math inline">\(k\)</span> 到达真实聚类数时，再增加 <span class="math inline">\(k\)</span> 所得到的聚合程度回报会迅速变小，所以SSE的下降幅度会骤减，然后随着 <span class="math inline">\(k\)</span> 值的继续增大而趋于平缓，也就是说SSE和 <span class="math inline">\(k\)</span> 的关系图是一个手肘的形状，而这个肘部对应的 <span class="math inline">\(k\)</span> 值就是数据的真实聚类数。当然，这也是该方法被称为手肘法的原因。</p>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/K-means.png">

</div>
<hr>
<h3 id="pca">PCA</h3>
<h4 id="思想总结">思想总结</h4>
<ol style="list-style-type: decimal">
<li>PCA就是将高维的数据通过线性变换投影到低维空间上去。</li>
<li>投影思想：找出最能够代表原始数据的投影方法，最大化样本的投影方差。被 PCA 降掉的那些维度只能是那些噪声或是冗余的数据。</li>
</ol>
<h4 id="算法总结"><strong>算法总结</strong></h4>
<p>输入：<span class="math inline">\(n\)</span> 维样本集 <span class="math inline">\(D = \left( x^{(1)},x^{(2)},...,x^{(m)} \right)\)</span> ，目标降低到 <span class="math inline">\(k\)</span> 维</p>
<p>输出：降维后的新样本集 <span class="math inline">\(D&#39; = \left( z^{(1)},z^{(2)},...,z^{(m)} \right)\)</span> 。</p>
<p>主要步骤如下：</p>
<ol style="list-style-type: decimal">
<li>对所有的样本进行归一化，<span class="math inline">\(x^{(i)} = x^{(i)} - \frac{1}{m} \sum^m_{j=1} x^{(j)}\)</span> ，如果特征是不同的数量级上，我们还要将其除以标准差</li>
<li>计算样本的协方差矩阵 <span class="math inline">\(XX^T\)</span></li>
<li>对协方差矩阵 <span class="math inline">\(XX^T\)</span> 进行特征值分解（或奇异值分解）</li>
<li>取出最大的 <span class="math inline">\(k\)</span> 个特征值对应的特征向量 <span class="math inline">\(\{ w_1,w_2,...,w_{n&#39;} \}\)</span></li>
<li>标准化特征向量，得到特征向量矩阵 <span class="math inline">\(W\)</span></li>
<li>转化样本集中的每个样本 <span class="math inline">\(z^{(i)} = W^T x^{(i)}\)</span></li>
<li>得到输出矩阵 <span class="math inline">\(D&#39; = \left( z^{(1)},z^{(2)},...,z^{(n)} \right)\)</span> 。</li>
</ol>
<blockquote>
<p>注：在降维时，有时不明确目标维数，而是指定降维到的主成分比重阈值 <span class="math inline">\(k(k \epsilon(0,1])\)</span> 。假设 <span class="math inline">\(n\)</span> 个特征值为 <span class="math inline">\(\lambda_1 \geqslant \lambda_2 \geqslant ... \geqslant \lambda_n\)</span> ，则 <span class="math inline">\(n&#39;\)</span> 可从 $^{n’}_{i=1} <em>i k ^n</em>{i=1} _i $ 得到。</p>
</blockquote>
<hr>
<h4 id="层次聚类">层次聚类</h4>
<p><strong>1 .算法</strong></p>
<p>假设有 n 个待聚类的样本，对于层次聚类算法，它的步骤是：</p>
<ul>
<li>步骤一：（初始化）将每个样本都视为一个聚类；</li>
<li>步骤二：计算各个聚类之间的相似度；</li>
<li>步骤三：寻找最近的两个聚类，将他们归为一类；</li>
<li>步骤四：重复步骤二，步骤三；直到所有样本归为一类。</li>
</ul>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/层次聚类.jpg">

</div>
<p>整个过程就是建立一棵树，在建立的过程中，可以在步骤四设置所需分类的类别个数，作为迭代的终止条件，毕竟都归为一类并不实际。</p>
<p><strong>2 . 聚类之间的相似度</strong></p>
<p>聚类和聚类之间的相似度有什么来衡量呢？既然是空间中的点，可以采用距离的方式来衡量，一般有下面三种：</p>
<p><strong>Single Linkage</strong></p>
<p>又叫做 <code>nearest-neighbor</code> ，就是取两个类中距离最近的两个样本的距离作为这两个集合的距离。这种计算方式容易造成一种叫做 <code>Chaining</code> 的效果，两个 cluster 明明从“大局”上离得比较远，但是由于其中个别的点距离比较近就被合并了，并且这样合并之后 Chaining 效应会进一步扩大，最后会得到比较松散的 cluster 。</p>
<p><strong>Complete Linkage</strong></p>
<p>这个则完全是 <code>Single Linkage</code> 的反面极端，取两个集合中距离最远的两个点的距离作为两个集合的距离。其效果也是刚好相反的，限制非常大。这两种相似度的定义方法的共同问题就是指考虑了某个有特点的数据，而没有考虑类内数据的整体特点。</p>
<p><strong>Average Linkage</strong> 这种方法就是把两个集合中的点两两的距离全部放在一起求均值，相对也能得到合适一点的结果。有时异常点的存在会影响均值，平常人和富豪平均一下收入会被拉高是吧，因此这种计算方法的一个变种就是取两两距离的中位数。</p>
<p><strong>3 . 层次聚类的优缺点</strong></p>
<p>优点：</p>
<ul>
<li>一次性得到聚类树，后期再分类无需重新计算；</li>
<li>相似度规则容易定义；</li>
<li>可以发现类别的层次关系。</li>
</ul>
<p>缺点：</p>
<ul>
<li>计算复杂度高，不适合数据量大的；</li>
<li>算法很可能形成链状。</li>
</ul>
<blockquote>
<p>Reference: <a href="https://zhuanlan.zhihu.com/p/32438294" target="_blank" rel="noopener">1</a> <a href="https://zhuanlan.zhihu.com/p/34168766" target="_blank" rel="noopener">2</a></p>
</blockquote>
<hr>
<h3 id="lda">LDA</h3>
<p><strong>线性判别分析（Linear Discriminant Analysis，LDA）</strong>是一种经典的降维方法。和主成分分析 PCA 不考虑样本类别输出的无监督降维技术不同，<u>LDA 是一种监督学习的降维技术</u>，数据集的每个样本有类别输出。</p>
<h4 id="lda-分类思想"><strong>LDA 分类思想</strong></h4>
<ol style="list-style-type: decimal">
<li>多维空间中，数据处理分类问题较为复杂，LDA 算法将多维空间中的数据投影到一条直线上，将 <span class="math inline">\(d\)</span> 维数据转化成 <span class="math inline">\(1\)</span> 维数据进行处理。<br>
</li>
<li>对于训练数据，设法将多维数据投影到一条直线上，同类数据的投影点尽可能接近，异类数据点尽可能远离。<br>
</li>
<li>对新数据进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定样本的类别。</li>
</ol>
<p>如果用一句话概括LDA思想，即 <strong>“投影后类内方差最小，类间方差最大”</strong></p>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/LDA.png">

</div>
<h4 id="lda-降维算法流程">LDA 降维算法流程</h4>
<p>输入：数据集 <span class="math inline">\(D=\{(\boldsymbol x_1,\boldsymbol y_1),(\boldsymbol x_2,\boldsymbol y_2),...,(\boldsymbol x_m,\boldsymbol y_m)\}\)</span>，其中样本 $x_i $ 是 <span class="math inline">\(n\)</span> 维向量，<span class="math inline">\(\boldsymbol y_i \epsilon \{0, 1\}\)</span>，降维后的目标维度 <span class="math inline">\(d\)</span> ，定义</p>
<ul>
<li><span class="math inline">\(N_j(j=0,1)\)</span> 为第 <span class="math inline">\(j\)</span> 类样本<strong>个数</strong></li>
<li><span class="math inline">\(X_j(j=0,1)​\)</span> 为第 <span class="math inline">\(j​\)</span> 类样本的<strong>集合</strong></li>
<li><span class="math inline">\(u_j(j=0,1)\)</span> 为第 <span class="math inline">\(j\)</span> 类样本的<strong>均值向量</strong></li>
<li><span class="math inline">\(\sum_j(j=0,1)\)</span> 为第 <span class="math inline">\(j\)</span> 类样本的<strong>协方差矩阵</strong></li>
</ul>
<p>其中<br>
<span class="math display">\[
u_j = \frac{1}{N_j} \sum_{\boldsymbol x\epsilon X_j}\boldsymbol x \ \ \ \ \ \ \ \ (j=0,1)， \\
\sum_j = \sum_{\boldsymbol x\epsilon X_j}(\boldsymbol x-u_j)(\boldsymbol x-u_j)^T \ \ \ \ \ (j=0,1)
\]</span><br>
假设投影直线是向量 <span class="math inline">\(\boldsymbol w\)</span>，对任意样本 <span class="math inline">\(\boldsymbol x_i\)</span>，它在直线 <span class="math inline">\(\boldsymbol w\)</span>上的投影为 <span class="math inline">\(\boldsymbol {w^Tx_i}\)</span>，两个类别的中心点 <span class="math inline">\(u_0\)</span>, <span class="math inline">\(u_1\)</span>在直线 <span class="math inline">\(\boldsymbol w\)</span> 的投影分别为 <span class="math inline">\(\boldsymbol w^Tu_0\)</span> 、<span class="math inline">\(\boldsymbol w^Tu_1\)</span>。</p>
<p>LDA 的目标是让同类样例投影点尽可能近，既让同类样例投影点的协方差<span class="math inline">\(\boldsymbol w^T \sum_0 \boldsymbol w\)</span>、<span class="math inline">\(\boldsymbol w^T \sum_1 \boldsymbol w\)</span> 尽量小，即最小化 <span class="math inline">\(\boldsymbol w^T \sum_0 \boldsymbol w +\boldsymbol w^T \sum_1 \boldsymbol w\)</span> 。而使异类样例的投影点尽可能远，即可让两类别的数据中心间的距离 <span class="math inline">\(\| \boldsymbol w^Tu_0 - \boldsymbol w^Tu_1 \|^2_2\)</span> 尽量大。</p>
<ul>
<li><strong>类内散度矩阵 <span class="math inline">\(S_w\)</span></strong></li>
</ul>
<p><span class="math display">\[
S_w = \sum_0 + \sum_1 = 
    \sum_{\boldsymbol x\epsilon X_0}(\boldsymbol x-u_0)(\boldsymbol x-u_0)^T + 
    \sum_{\boldsymbol x\epsilon X_1}(\boldsymbol x-u_1)(\boldsymbol x-u_1)^T
\]</span><br>
- <strong>类间散度矩阵 <span class="math inline">\(S_b\)</span></strong></p>
<p><span class="math display">\[
S_b = (u_0 - u_1)(u_0 - u_1)^T
\]</span></p>
<p>据上分析，优化目标为<br>
<span class="math display">\[
\mathop{\arg\max}_\boldsymbol w J(\boldsymbol w) = \frac{\| \boldsymbol w^Tu_0 - \boldsymbol w^Tu_1 \|^2_2}{\boldsymbol w^T \sum_0\boldsymbol w + \boldsymbol w^T \sum_1\boldsymbol w} = 
\frac{\boldsymbol w^T(u_0-u_1)(u_0-u_1)^T\boldsymbol w}{\boldsymbol w^T(\sum_0 + \sum_1)\boldsymbol w} =
\frac{\boldsymbol w^TS_b\boldsymbol w}{\boldsymbol w^TS_w\boldsymbol w}
\]</span><br>
根据广义瑞利商的性质，矩阵 <span class="math inline">\(\frac{S_b}{S_w}\)</span> 的最大特征值为 <span class="math inline">\(J(\boldsymbol w)\)</span> 的最大值，矩阵 <span class="math inline">\(\frac{S_b}{S_w}\)</span> 的最大特征值对应的特征向量即为 <span class="math inline">\(\boldsymbol w\)</span></p>
<h4 id="lda-降维算法流程总结">LDA 降维算法流程总结</h4>
<p><strong>输入</strong>：数据集 <span class="math inline">\(D = \{ (x_1,y_1),(x_2,y_2), ... ,(x_m,y_m) \}\)</span>，其中样本 $x_i $ 是 <span class="math inline">\(n\)</span> 维向量，<span class="math inline">\(y_i \epsilon \{C_1, C_2, ..., C_k\}\)</span>，降维后的目标维度 <span class="math inline">\(d\)</span></p>
<p><strong>输出</strong>：降维后的数据集 $ $</p>
<p><strong>步骤：</strong></p>
<ol style="list-style-type: decimal">
<li>计算类内散度矩阵 <span class="math inline">\(S_w\)</span></li>
<li>计算类间散度矩阵 <span class="math inline">\(S_b\)</span></li>
<li>计算矩阵 <span class="math inline">\(\frac{S_b}{S_w}\)</span></li>
<li>计算矩阵 <span class="math inline">\(\frac{S_b}{S_w}\)</span> 的最大的 <span class="math inline">\(d\)</span> 个特征值</li>
<li>计算 <span class="math inline">\(d\)</span> 个特征值对应的 <span class="math inline">\(d\)</span> 个特征向量，记投影矩阵为 <span class="math inline">\(W\)</span></li>
<li>转化样本集的每个样本，得到新样本 <span class="math inline">\(P_i = W^Tx_i\)</span></li>
<li>输出新样本集 <span class="math inline">\(\overline{D} = \{ (p_1,y_1),(p_2,y_2),...,(p_m,y_m) \}\)</span></li>
</ol>
<h4 id="lda-和-pca区别">LDA 和 PCA区别</h4>
<table>
<thead>
<tr class="header">
<th>异同点</th>
<th>LDA</th>
<th>PCA</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>相同点</strong></td>
<td>1. 两者均可以对数据进行降维<br>2. 两者在降维时均使用了矩阵特征分解的思想；<br>3. 两者都假设数据符合高斯分布</td>
<td></td>
</tr>
<tr class="even">
<td><strong>不同点</strong></td>
<td>有监督的降维方法</td>
<td>无监督的降维方法</td>
</tr>
<tr class="odd">
<td></td>
<td>降维最多降到k-1维</td>
<td>降维多少没有限制</td>
</tr>
<tr class="even">
<td></td>
<td>可以用于降维，还可以用于分类</td>
<td>只用于降维</td>
</tr>
<tr class="odd">
<td></td>
<td>选择分类性能最好的投影方向</td>
<td>选择样本点投影具有最大方差的方向</td>
</tr>
<tr class="even">
<td></td>
<td>更明确，更能反映样本间差异</td>
<td>目的较为模糊</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="em">EM</h3>
<h4 id="em算法基本思想">EM算法基本思想</h4>
<p>最大期望算法（Expectation-Maximization algorithm, EM），是一类通过迭代进行极大似然估计的优化算法，通常作为牛顿迭代法的替代，用于对包含隐变量或缺失数据的概率模型进行参数估计。</p>
<p>最大期望算法基本思想是经过两个步骤交替进行计算：</p>
<ul>
<li>第一步是计算期望（E），利用对隐藏变量的现有估计值，计算其最大似然估计值<strong>；</strong></li>
<li>第二步是最大化（M），最大化在E步上求得的最大似然值来计算参数的值。</li>
<li>M步上找到的参数估计值被用于下一个E步计算中，这个过程不断交替进行。</li>
</ul>
<h4 id="em算法推导">EM算法推导</h4>
<p>对于<span class="math inline">\(m\)</span>个样本观察数据<span class="math inline">\(x=(x^{1},x^{2},...,x^{m})\)</span>，现在想找出样本的模型参数<span class="math inline">\(\theta\)</span>，其极大化模型分布的对数似然函数为：<br>
<span class="math display">\[
\theta = \mathop{\arg\max}_\theta\sum\limits_{i=1}^m logP(x^{(i)};\theta)
\]</span><br>
如果得到的观察数据有未观察到的隐含数据<span class="math inline">\(z=(z^{(1)},z^{(2)},...z^{(m)})\)</span>，极大化模型分布的对数似然函数则为：<br>
<span class="math display">\[
\theta =\mathop{\arg\max}_\theta\sum\limits_{i=1}^m logP(x^{(i)};\theta) = \mathop{\arg\max}_\theta\sum\limits_{i=1}^m log\sum\limits_{z^{(i)}}P(x^{(i)}, z^{(i)};\theta)  \tag{a}
\]</span><br>
由于上式不能直接求出 <span class="math inline">\(\theta\)</span>，采用缩放技巧：<br>
<span class="math display">\[
\begin{align} \sum\limits_{i=1}^m log\sum\limits_{z^{(i)}}P(x^{(i)}, z^{(i)};\theta)   &amp; = \sum\limits_{i=1}^m log\sum\limits_{z^{(i)}}Q_i(z^{(i)})\frac{P(x^{(i)}, z^{(i)};\theta)}{Q_i(z^{(i)})} \\ &amp; \geqslant  \sum\limits_{i=1}^m \sum\limits_{z^{(i)}}Q_i(z^{(i)})log\frac{P(x^{(i)}, z^{(i)};\theta)}{Q_i(z^{(i)})} \end{align}   \tag{1}
\]</span><br>
上式用到了 Jensen 不等式：<br>
<span class="math display">\[
log\sum\limits_j\lambda_jy_j \geqslant \sum\limits_j\lambda_jlogy_j\;\;,  \lambda_j \geqslant 0, \sum\limits_j\lambda_j =1
\]</span><br>
并且引入了一个未知的新分布<span class="math inline">\(Q_i(z^{(i)})\)</span>。</p>
<p>此时，如果需要满足 Jensen 不等式中的等号，所以有：<br>
<span class="math display">\[
\frac{P(x^{(i)}, z^{(i)};\theta)}{Q_i(z^{(i)})} =c  \ \ \ \ \ \ \  (c为常数)
\]</span><br>
由于<span class="math inline">\(Q_i(z^{(i)})\)</span>是一个分布，所以满足<br>
<span class="math display">\[
\sum\limits_{z}Q_i(z^{(i)}) =1
\]</span><br>
综上，可得：<br>
<span class="math display">\[
Q_i(z^{(i)})  = \frac{P(x^{(i)}， z^{(i)};\theta)}{\sum\limits_{z}P(x^{(i)}, z^{(i)};\theta)} =  \frac{P(x^{(i)}, z^{(i)};\theta)}{P(x^{(i)};\theta)} = P( z^{(i)}|x^{(i)};\theta)
\]</span><br>
如果<span class="math inline">\(Q_i(z^{(i)}) = P( z^{(i)}|x^{(i)};\theta)\)</span> ，则第(1)式是我们的包含隐藏数据的对数似然的一个下界。如果我们能极大化这个下界，则也在尝试极大化我们的对数似然。即我们需要最大化下式：<br>
<span class="math display">\[
\mathop{\arg\max}_\theta \sum\limits_{i=1}^m \sum\limits_{z^{(i)}}Q_i(z^{(i)})log\frac{P(x^{(i)}， z^{(i)};\theta)}{Q_i(z^{(i)})}
\]</span><br>
简化得：<br>
<span class="math display">\[
\mathop{\arg\max}_\theta \sum\limits_{i=1}^m \sum\limits_{z^{(i)}}Q_i(z^{(i)})log{P(x^{(i)}, z^{(i)};\theta)}
\]</span><br>
以上即为EM算法的M步，<span class="math inline">\(\sum\limits_{z^{(i)}}Q_i(z^{(i)})log{P(x^{(i)}, z^{(i)};\theta)}\)</span>可理解为$logP(x^{(i)}, z^{(i)};) <span class="math inline">\(基于条件概率分布\)</span>Q_i(z^{(i)}) $的期望。以上即为EM算法中E步和M步的具体数学含义。</p>
<h4 id="图解em算法">图解EM算法</h4>
<p>考虑上一节中的（a）式，表达式中存在隐变量，直接找到参数估计比较困难，通过EM算法迭代求解下界的最大值到收敛为止。</p>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/2.20.1.jpg">

</div>
<p>图片中的紫色部分是我们的目标模型 <span class="math inline">\(p(x|\theta)​\)</span>，该模型复杂，难以求解析解，为了消除隐变量 <span class="math inline">\(z^{(i)}​\)</span> 的影响，我们可以选择一个不包含 <span class="math inline">\(z^{(i)}​\)</span> 的模型 <span class="math inline">\(r(x|\theta)​\)</span>，使其满足条件 <span class="math inline">\(r(x|\theta) \leqslant p(x|\theta) ​\)</span>。</p>
<p>求解步骤如下：</p>
<p>（1）选取 <span class="math inline">\(\theta_1\)</span>，使得 <span class="math inline">\(r(x|\theta_1) = p(x|\theta_1)\)</span>，然后对此时的 <span class="math inline">\(r\)</span> 求取最大值，得到极值点 <span class="math inline">\(\theta_2\)</span>，实现参数的更新。</p>
<p>（2）重复以上过程到收敛为止，在更新过程中始终满足 $r p $.</p>
<h4 id="em算法流程">EM算法流程</h4>
<p>输入：观察数据 <span class="math inline">\(x=(x^{(1)},x^{(2)},...x^{(m)})\)</span>，联合分布 <span class="math inline">\(p(x,z ;\theta)\)</span>，条件分布 <span class="math inline">\(p(z|x; \theta)\)</span>，最大迭代次数 <span class="math inline">\(J\)</span></p>
<p>1）随机初始化模型参数 <span class="math inline">\(\theta\)</span> 的初值 <span class="math inline">\(\theta^0\)</span>。</p>
<p>2）<span class="math inline">\(for \ j \ from \ 1 \ to \ j\)</span>：</p>
<p>​ a） E步。计算联合分布的条件概率期望：<br>
<span class="math display">\[
Q_i(z^{(i)}) = P( z^{(i)}|x^{(i)}, \theta^{j})
\]</span></p>
<p><span class="math display">\[
L(\theta, \theta^{j}) = \sum\limits_{i=1}^m\sum\limits_{z^{(i)}}P( z^{(i)}|x^{(i)}, \theta^{j})log{P(x^{(i)}, z^{(i)};\theta)}
\]</span></p>
<p>​ b） M步。极大化 <span class="math inline">\(L(\theta, \theta^{j})\)</span>，得到 <span class="math inline">\(\theta^{j+1}\)</span>:<br>
<span class="math display">\[
\theta^{j+1} = \mathop{\arg\max}_\theta L(\theta, \theta^{j})
\]</span><br>
​ c） 如果 <span class="math inline">\(\theta^{j+1}\)</span>收敛，则算法结束。否则继续回到步骤a）进行E步迭代。</p>
<p>输出：模型参数 <span class="math inline">\(\theta\)</span>。</p>
<hr>
<h3 id="集成学习">集成学习</h3>
<h4 id="集成学习的基本思想">集成学习的基本思想</h4>
<ul>
<li>结合多个学习器组合成一个性能更好的学习器</li>
</ul>
<h4 id="集成学习为什么有效">集成学习为什么有效</h4>
<ul>
<li>不同的模型通常会在<strong>测试集</strong>上产生不同的误差；如果成员的误差是独立的，集成模型将显著地比其成员表现更好</li>
</ul>
<h4 id="boosting-方法">Boosting 方法</h4>
<ul>
<li>基于<strong>串行策略</strong>：基学习器之间存在依赖关系，新的学习器需要根据上一个学习器生成。</li>
<li><strong>基本思路</strong>：</li>
<li>先从<strong>初始训练集</strong>训练一个基学习器；初始训练集中各样本的权重是相同的；</li>
<li>根据上一个基学习器的表现，<strong>调整样本权重</strong>，使分类错误的样本得到更多的关注；</li>
<li>基于调整后的样本分布，训练下一个基学习器；</li>
<li>测试时，对各基学习器加权得到最终结果</li>
<li><strong>特点</strong>：</li>
<li>每次学习都会使用全部训练样本</li>
<li><strong>代表算法</strong>：</li>
<li><a href="#adaboost-算法">AdaBoost 算法</a></li>
<li><a href="#gbdt-算法">GBDT 算法</a></li>
<li><a href>XGBoost</a></li>
</ul>
<h4 id="bagging-方法">Bagging 方法</h4>
<ul>
<li><p>基于<strong>并行策略</strong>：基学习器之间不存在依赖关系，可同时生成。</p></li>
<li><p><strong>基本思路</strong>：</p></li>
<li><p>利用<strong>自助采样法</strong>对训练集随机采样，重复进行 <code>T</code> 次;</p></li>
<li><p>基于每个采样集训练一个基学习器，并得到 <code>T</code> 个基学习器；</p></li>
<li><p>预测时，集体<strong>投票决策</strong>。</p>
<blockquote>
<p><strong>自助采样法</strong>：对 m 个样本的训练集，有放回的采样 m 次；此时，样本在 m 次采样中始终没被采样的概率约为 <code>0.368</code>，即每次自助采样只能采样到全部样本的 <code>63%</code> 左右。<br>
<span class="math display">\[
\lim _{m \rightarrow \infty}\left(1-\frac{1}{m}\right)^{m} \rightarrow \frac{1}{e} \approx 0.368
\]</span></p>
</blockquote></li>
<li><p><strong>特点</strong>：</p></li>
<li><p>训练每个基学习器时只使用一部分样本；</p></li>
<li><p>偏好<strong>不稳定</strong>的学习器作为基学习器；</p>
<blockquote>
<p>所谓不稳定的学习器，指的是对<strong>样本分布</strong>较为敏感的学习器。</p>
</blockquote></li>
<li><p><strong>代表方法</strong></p></li>
<li><p><a href>RF</a></p></li>
</ul>
<h4 id="为什么使用决策树作为基学习器">为什么使用决策树作为基学习器</h4>
<ul>
<li><strong>类似问题</strong></li>
<li>基学习器有什么特点？</li>
<li>基学习器有什么要求？</li>
<li>使用决策树作为基学习器的原因：</li>
<li>决策树的表达能力和泛化能力，可以通过剪枝快速调整</li>
<li>决策树可以方便地将<strong>样本的权重</strong>整合到训练过程中 （适合 Boosting 策略）</li>
<li>决策树是一种<strong>不稳定</strong>的学习器。所谓不稳定，指的是数据样本的扰动会对决策树的结果产生较大的影响。 （适合 Bagging 策略）</li>
</ul>
<h4 id="为什么不稳定的学习器更适合作为基学习器">为什么不稳定的学习器更适合作为基学习器</h4>
<ul>
<li>不稳定的学习器容易受到<strong>样本分布</strong>的影响（方差大），很好的引入了<strong>随机性</strong>；这有助于在集成学习（特别是采用 <strong>Bagging</strong> 策略）中提升模型的<strong>泛化能力</strong></li>
<li>为了更好的引入随机性，有时会随机选择一个<strong>属性子集</strong>中的最优分裂属性，而不是全局最优（<strong>随机森林</strong>）</li>
</ul>
<h4 id="还有哪些模型也适合作为基学习器">还有哪些模型也适合作为基学习器</h4>
<ul>
<li><strong>神经网络</strong></li>
<li>神经网络也属于<strong>不稳定</strong>的学习器；</li>
<li>此外，通过调整神经元的数量、网络层数，连接方式初始权重也能很好的引入随机性和改变模型的表达能力和泛化能力。</li>
</ul>
<h4 id="boosting-方法中能使用线性分类器作为基学习器吗-bagging-呢">Boosting 方法中能使用线性分类器作为基学习器吗？ Bagging 呢？</h4>
<ul>
<li>Boosting 方法中<strong>可以使用</strong></li>
<li>Boosting 方法主要通过降低<strong>偏差</strong>的方式来提升模型的性能，而线性分类器本身具有方差小的特点，所以两者有一定相性</li>
<li>XGBoost 中就支持以线性分类器作为基学习器。</li>
<li>Bagging 方法中<strong>不推荐</strong></li>
<li>线性分类器都属于稳定的学习器（方差小），对数据不敏感；</li>
<li>甚至可能因为 Bagging 的采样，导致在训练中难以收敛，增大集成分类器的<strong>偏差</strong></li>
</ul>
<h4 id="boostingbagging-与-偏差方差-的关系"><a href="#%5B偏差方差与Boosting和Bagging联系%5D(#集成学习)">Boosting/Bagging 与 偏差/方差 的关系</a></h4>
<h4 id="bagging与dropout区别与联系"><a href="#Dropout">Bagging与Dropout区别与联系</a></h4>
<ul>
<li>在 Bagging 的情况下，所有模型都是独立的。而在 Dropout 的情况下，所有模型<strong>共享参数</strong>，其中每个模型继承父神经网络参数的不同子集。</li>
<li>在 Bagging 的情况下，每一个模型都会在其相应训练集上训练到收敛。而在 Dropout 的情况下，通常大部分模型都没有显式地被训练；取而代之的是，在单个步骤中我们训练一小部分的子网络，参数共享会使得剩余的子网络也能有好的参数设定。</li>
</ul>
<h3 id="adaboost">AdaBoost</h3>
<h3 id="gbdt">GBDT</h3>
<ul>
<li>GBDT 是以<strong>决策树</strong>为基学习器、采用 Boosting 策略的一种集成学习模型</li>
<li><strong>与提升树的区别</strong>：残差的计算不同，提升树使用的是真正的残差，梯度提升树用当前模型的负梯度来拟合残差。</li>
</ul>
<h3 id="rf-随机森林">RF 随机森林</h3>
<h3 id="xgboost">XGBoost</h3>
<hr>
<h3 id="神经网络">神经网络</h3>
<h4 id="超参数">超参数</h4>
<p>在机器学习的上下文中，超参数是在开始学习过程之前设置值的参数，而不是通过训练得到的参数数据。通常情况下，需要对超参数进行优化，给学习机选择一组最优超参数，以提高学习的性能和效果。</p>
<p>超参数具体来讲比如算法中的</p>
<ul>
<li>学习率（learning rate）</li>
<li>梯度下降法迭代的数量（iterations）</li>
<li>隐藏层数目（hidden layers）</li>
<li>隐藏层单元数目</li>
<li>激活函数（ activation function）</li>
</ul>
<p>都需要根据实际情况来设置，这些数字实际上控制了最后的参数和的值，所以它们被称作超参数。</p>
<h4 id="如何寻找超参数的最优值">如何寻找超参数的最优值</h4>
<p>在使用机器学习算法时，总有一些难调的超参数。例如权重衰减大小，高斯核宽度等等。这些参数需要人为设置，设置的值对结果产生较大影响。常见设置超参数的方法有：</p>
<ul>
<li><strong>猜测和检查</strong>：根据经验或直觉，选择参数，一直迭代。</li>
<li><strong>网格搜索</strong>：让计算机尝试在一定范围内均匀分布的一组值。</li>
<li><strong>随机搜索</strong>：让计算机随机挑选一组值。</li>
<li><strong>贝叶斯优化</strong>：使用贝叶斯优化超参数，会遇到贝叶斯优化算法本身就需要很多的参数的困难。</li>
<li><strong>MITIE方法</strong>，好初始猜测的前提下进行局部优化。它使用BOBYQA算法，并有一个精心选择的起始点。由于BOBYQA只寻找最近的局部最优解，所以这个方法是否成功很大程度上取决于是否有一个好的起点。在MITIE的情况下，我们知道一个好的起点，但这不是一个普遍的解决方案，因为通常你不会知道好的起点在哪里。从好的方面来说，这种方法非常适合寻找局部最优解。稍后我会再讨论这一点。</li>
<li>最新提出的 <strong><code>LIPO</code> 的全局优化方法</strong>。这个方法没有参数，而且经验证比随机搜索方法好。</li>
</ul>
<h4 id="超参数搜索一般过程">超参数搜索一般过程</h4>
<ol style="list-style-type: decimal">
<li>将数据集划分成训练集、验证集及测试集。</li>
<li>在训练集上根据模型的性能指标对模型参数进行优化。</li>
<li>在验证集上根据模型的性能指标对模型的超参数进行搜索。</li>
<li>步骤 2 和步骤 3 交替迭代，最终确定模型的参数和超参数，在测试集中验证评价模型的优劣。</li>
</ol>
<p>其中，搜索过程需要搜索算法，一般有：<strong>网格搜索、随机搜过、启发式智能搜索、贝叶斯搜索</strong>。</p>
<h3 id="梯度下降">梯度下降</h3>
<h4 id="机器学习中为什么需要梯度下降">机器学习中为什么需要梯度下降</h4>
<ul>
<li>梯度下降是迭代法的一种，可以用于求解最小二乘问题。</li>
<li>在求解机器学习算法的模型参数，即无约束优化问题时，主要有梯度下降法（Gradient Descent）和最小二乘法。</li>
<li>在求解损失函数的最小值时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数和模型参数值。</li>
<li>如果我们需要求解损失函数的最大值，可通过梯度上升法来迭代。梯度下降法和梯度上升法可相互转换。</li>
<li>在机器学习中，梯度下降法主要有随机梯度下降法和批量梯度下降法。</li>
</ul>
<h4 id="梯度下降法缺点">梯度下降法缺点</h4>
<ul>
<li>靠近极小值时收敛速度减慢。</li>
<li>直线搜索时可能会产生一些问题。</li>
<li>可能会“之字形”地下降。</li>
</ul>
<p>梯度概念需注意：</p>
<ul>
<li>梯度是一个向量，即有方向有大小。</li>
<li>梯度的方向是最大方向导数的方向。</li>
<li>梯度的值是最大方向导数的值。</li>
</ul>
<h4 id="梯度下降法直观理解">梯度下降法直观理解</h4>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/梯度下降.png">

</div>
<p>​ 形象化举例，由上图，假如最开始，我们在一座大山上的某处位置，因为到处都是陌生的，不知道下山的路，所以只能摸索着根据直觉，走一步算一步，在此过程中，每走到一个位置的时候，都会求解当前位置的梯度，沿着梯度的负方向，也就是当前最陡峭的位置向下走一步，然后继续求解当前位置梯度，向这一步所在位置沿着最陡峭最易下山的位置走一步。不断循环求梯度，就这样一步步地走下去，一直走到我们觉得已经到了山脚。当然这样走下去，有可能我们不能走到山脚，而是到了某一个局部的山势低处。<br>
​ 由此，从上面的解释可以看出，梯度下降不一定能够找到全局的最优解，有可能是一个局部的最优解。当然，如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解。</p>
<ol style="list-style-type: decimal">
<li>初始化参数，随机选取取值范围内的任意数；</li>
<li>迭代操作：<br>
a）计算当前梯度；<br>
b）修改新的变量；<br>
c）计算朝最陡的下坡方向走一步；<br>
d）判断是否需要终止，如否，返回a）；</li>
<li>得到全局最优解或者接近全局最优解。</li>
</ol>
<h4 id="梯度下降法算法描述">梯度下降法算法描述</h4>
<p><strong>1 . 确定优化模型的假设函数</strong><br>
举例，对于线性回归，假设函数为：<br>
<span class="math display">\[
h_\theta(x_1,x_2,...,x_n)=\theta_0+\theta_1x_1+...+\theta_nx_n
\]</span><br>
其中，<span class="math inline">\(\theta_j,x_j(j=0,1,2,...,n)\)</span>分别为模型参数、每个样本的特征值。<br>
<strong>2 . 损失函数</strong><br>
<span class="math display">\[
J(\theta_0,\theta_1,...,\theta_n)=\frac{1}{2m}\sum^{m}_{i=0}(h_\theta (x^{(i)})-y^{(i)})^2
\]</span><br>
<strong>3 . 相关参数初始化</strong><br>
主要初始化 <span class="math inline">\({\theta}_j\)</span>、算法迭代步长 ${} $、终止距离 ${} $。初始化时可以根据经验初始化，即 <span class="math inline">\({\theta}\)</span> 初始化为0，步长 <span class="math inline">\({\alpha}\)</span> 初始化为1。当前步长记为 <span class="math inline">\({\varphi}_j\)</span>。当然，也可随机初始化。</p>
<p><strong>4 . 迭代计算</strong></p>
<p>1）计算当前位置时损失函数的梯度，对 ${}_j $，其梯度表示为：<br>
<span class="math display">\[
\frac{\partial}{\partial \theta_j}J({\theta}_0,{\theta}_1,...,{\theta}_n)=\frac{1}{2m}\sum^{m}_{i=0}(h_\theta (x^{(i)})-y^{(i)})^2
\]</span><br>
2）计算当前位置下降的距离。<br>
<span class="math display">\[
{\varphi}_j={\alpha} \frac{\partial}{\partial \theta_j}J({\theta}_0,{\theta}_1,...,{\theta}_n)
\]</span><br>
3）判断是否终止</p>
<p>确定是否所有 <span class="math inline">\({\theta}_j\)</span> 梯度下降的距离 <span class="math inline">\({\varphi}_j\)</span> 都小于终止距离 <span class="math inline">\({\zeta}\)</span>，如果都小于 <span class="math inline">\({\zeta}\)</span>，则算法终止，当然的值即为最终结果，否则进入下一步。</p>
<p>4）更新所有的 <span class="math inline">\({\theta}_j\)</span>，更新后的表达式为<br>
<span class="math display">\[
\begin{aligned}
{\theta}_j
&amp; ={\theta}_j-\alpha \frac{\partial}{\partial \theta_j}J({\theta}_0,{\theta}_1,...,{\theta}_n) \\
&amp; =\theta_j - \alpha \frac{1}{m} \sum^{m}_{i=0}(h_\theta (x^{(i)}-y^{(i)})x^{(i)}_j)
\end{aligned}
\]</span><br>
5）令上式 <span class="math inline">\(x^{(i)}_0=1\)</span>，更新完毕后转入 1)<br>
​ 由此，可看出，当前位置的梯度方向由所有样本决定，上式中 <span class="math inline">\(\frac{1}{m}\)</span>、<span class="math inline">\(\alpha \frac{1}{m}\)</span> 的目的是为了便于理解。</p>
<h4 id="如何对梯度下降法进行调优">如何对梯度下降法进行调优</h4>
<p>实际使用梯度下降法时，各项参数指标不能一步就达到理想状态，对梯度下降法调优主要体现在以下几个方面：</p>
<ol style="list-style-type: decimal">
<li><strong>算法迭代步长 <span class="math inline">\(\alpha\)</span> 选择。</strong><br>
在算法参数初始化时，有时根据经验将步长初始化为1。实际取值取决于数据样本。可以从大到小，多取一些值，分别运行算法看迭代效果，如果损失函数在变小，则取值有效。如果取值无效，说明要增大步长。但步长太大，有时会导致迭代速度过快，错过最优解。步长太小，迭代速度慢，算法运行时间长。</li>
<li><strong>参数的初始值选择。</strong><br>
初始值不同，获得的最小值也有可能不同，梯度下降有可能得到的是局部最小值。如果损失函数是凸函数，则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。</li>
<li><strong>标准化处理。</strong><br>
由于样本不同，特征取值范围也不同，导致迭代速度慢。为了减少特征取值的影响，可对特征数据标准化，使新期望为0，新方差为1，可节省算法运行时间。</li>
</ol>
<h4 id="随机梯度和批量梯度区别">随机梯度和批量梯度区别</h4>
<ol style="list-style-type: decimal">
<li><strong>随机梯度下降（SGD）</strong></li>
</ol>
<p>其每次迭代，只用一个训练数据来更新 <span class="math inline">\(\theta\)</span> ，即代价函数对参数的偏导数为：<br>
<span class="math display">\[
\begin{aligned}
{\alpha} \frac{\partial}{\partial \theta_j}J({\theta}_0,{\theta}_1,...,{\theta}_n) &amp; = \frac{\partial}{\partial \theta_j} [\frac{1}{2}(h_{\theta}(x) - y)^2] \\
&amp; = 2 \cdot \frac{1}{2}(h_{\theta}(x) - y) \cdot \frac{\partial}{\partial \theta_j} (h_{\theta}(x) - y) \\
&amp; = (h_{\theta}(x) - y)\cdot x_j
\end{aligned}
\]</span><br>
即此时的参数更新为：<br>
<span class="math display">\[
{\theta}_j =\theta_j - \alpha \frac{1}{m} (h_\theta (x^{(i)}-y^{(i)})x^{(i)}_j)
\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li><strong>批量梯度下降（BGD）</strong></li>
</ol>
<p>每次迭代，使用所有的数据来更新 <span class="math inline">\(\theta\)</span> ，此时代价函数对参数的偏导数为：<br>
<span class="math display">\[
\begin{aligned}
{\alpha} \frac{\partial}{\partial \theta_j}J({\theta}_0,{\theta}_1,...,{\theta}_n) &amp; = \frac{\partial}{\partial \theta_j} [\frac{1}{2m}\sum_{i = 1}^{m}(h_{\theta}(x^{(i)}) -y^{(i)})^2] \\
&amp; = \frac{1}{m} \sum_{i = 1}^{m}(h_{\theta}(x^{(i)}) -y^{(i)})\cdot x_j^{(i)}
\end{aligned} \\
\]</span><br>
即此时的参数更新为：<br>
<span class="math display">\[
{\theta}_j =\theta_j - \alpha \frac{1}{m} \sum_{i = 1}^{m} (h_\theta (x^{(i)}-y^{(i)})x^{(i)}_j)
\]</span><br>
<strong>小结：</strong>随机梯度下降法、批量梯度下降法相对来说都比较极端，简单对比如下：</p>
<table style="width:67%;">
<colgroup>
<col width="9%">
<col width="56%">
</colgroup>
<thead>
<tr class="header">
<th>方法</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>批量梯度下降</td>
<td>a）采用所有数据来梯度下降。<br>b）批量梯度下降法在样本量很大的时候，训练速度慢。</td>
</tr>
<tr class="even">
<td>随机梯度下降</td>
<td>a）随机梯度下降用一个样本来梯度下降。<br>b）训练速度很快。<br>c）随机梯度下降法仅仅用一个样本决定梯度方向，导致解有可能不是全局最优。<br>d）收敛速度来说，随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解。</td>
</tr>
</tbody>
</table>
<p>下面介绍能结合两种方法优点的小批量梯度下降法。</p>
<ol start="3" style="list-style-type: decimal">
<li><strong>小批量梯度下降（Mini-batch GD）</strong></li>
</ol>
<p>对于总数为 <span class="math inline">\(m\)</span> 个样本的数据，根据样本的数据，选取其中的<span class="math inline">\(n(1&lt; n&lt; m)\)</span>个子样本来迭代。其参数 <span class="math inline">\(\theta\)</span> 按梯度方向更新 <span class="math inline">\(\theta_j\)</span> 公式如下：<br>
<span class="math display">\[
{\theta}_j =\theta_j - \alpha \frac{1}{m} \sum_{i = t}^{t+n-1} (h_\theta (x^{(i)}-y^{(i)})x^{(i)}_j)
\]</span></p>
<h4 id="各种梯度下降法性能比较">各种梯度下降法性能比较</h4>
<p>下表简单对比 <code>随机梯度下降（SGD）</code>、<code>批量梯度下降（BGD）</code>、<code>小批量梯度下降（Mini-batch GD）</code>、和 <code>Online GD</code> 的区别：</p>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="center">BGD</th>
<th align="center">SGD</th>
<th align="center">Mini-batch GD</th>
<th align="center">Online GD</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">训练集</td>
<td align="center">固定</td>
<td align="center">固定</td>
<td align="center">固定</td>
<td align="center">实时更新</td>
</tr>
<tr class="even">
<td align="left">单次迭代样本数</td>
<td align="center">整个训练集</td>
<td align="center">单个样本</td>
<td align="center">训练集的子集</td>
<td align="center">根据具体算法定</td>
</tr>
<tr class="odd">
<td align="left">算法复杂度</td>
<td align="center">高</td>
<td align="center">低</td>
<td align="center">一般</td>
<td align="center">低</td>
</tr>
<tr class="even">
<td align="left">时效性</td>
<td align="center">低</td>
<td align="center">一般</td>
<td align="center">一般</td>
<td align="center">高</td>
</tr>
<tr class="odd">
<td align="left">收敛性</td>
<td align="center">稳定</td>
<td align="center">不稳定</td>
<td align="center">较稳定</td>
<td align="center">不稳定</td>
</tr>
</tbody>
</table>
<p>BGD、SGD、Mini-batch GD，前面均已讨论过，这里介绍一下Online GD</p>
<p><strong>Online GD</strong> 与Mini-batch GD/SGD的区别在于，所有训练数据只用一次，然后丢弃。这样做的优点在于可预测最终模型的变化趋势。</p>
<p>Online GD在互联网领域用的较多，比如搜索广告的点击率（CTR）预估模型，网民的点击行为会随着时间改变。用普通的BGD算法（每天更新一次）一方面耗时较长（需要对所有历史数据重新训练）；另一方面，无法及时反馈用户的点击行为迁移。而Online GD算法可以实时的依据网民的点击行为进行迁移。</p>
<h4 id="梯度爆炸与梯度消失">梯度爆炸与梯度消失</h4>
<ul>
<li><strong>梯度消失 / 梯度爆炸</strong></li>
</ul>
<p>在深层网络中，由于网络过深，如果初始得到的梯度过小，或者传播途中在某一层上过小，则在之后的层上得到的梯度会越来越小，即产生了梯度消失。梯度爆炸也是同样的。一般地，不合理的权值初始化（过大或过小）以及激活函数，如 <span class="math inline">\(Sigmoid\)</span> 等，都会导致梯度过大或者过小，从而引起消失/爆炸。</p>
<p>（1）网络深度</p>
<p>若在网络很深时，若权重初始化较小，各层上的相乘得到的数值都会0-1之间的小数，而激活函数梯度也是0-1之间的数。那么连乘后，结果数值就会变得非常小，导致<strong>梯度消失</strong>。若权重初始化较大，大到乘以激活函数的导数都大于1，那么连乘后，可能会导致求导的结果很大，形成<strong>梯度爆炸</strong>。</p>
<p>（2）激活函数<br>
如果激活函数选择不合适，比如使用 <span class="math inline">\(Sigmoid\)</span>，就很容易导致梯度消失。<span class="math inline">\(Sigmoid\)</span> 函数，在输入取绝对值非常大的正值或负值时会出现 <strong>饱和</strong> 现象，梯度趋于0，并且 <span class="math inline">\(Sigmoid\)</span> 函数的梯度在 <span class="math inline">\((0, \frac{1}{4}]\)</span> ，这样经过链式求导之后，很容易发生梯度消失。</p>
<h4 id="梯度消失爆炸的解决方案">梯度消失、爆炸的解决方案</h4>
<ul>
<li><strong>梯度剪切、正则</strong></li>
</ul>
<p>梯度剪切这个方案主要是针对梯度爆炸提出的，其思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。这可以防止梯度爆炸。<br>
另外一种解决梯度爆炸的手段是采用权重正则化（weithts regularization）比较常见的是L1和L2正则。</p>
<ul>
<li><strong>使用 <span class="math inline">\(ReLu、Leaky \ ReLu\)</span> 等激活函数</strong></li>
</ul>
<p><span class="math inline">\(ReLU\)</span> 的导数始终是一个常数，负半区为 0，正半区为 1，所以不会发生梯度消失现象。<span class="math inline">\(Leaky \ ReLU\)</span> 则是为了解决 <span class="math inline">\(ReLU\)</span> 神经元 Dead 现象，在 <span class="math inline">\(x &lt; 0\)</span> 时，取 <span class="math inline">\(0.1x\)</span></p>
<ul>
<li><strong>使用 BN</strong></li>
</ul>
<p>BN 方法会针对<strong>每一批数据</strong>，在<strong>网络的每一层输入</strong>之前增加<strong>归一化</strong>处理，使输入的均值为 <code>0</code>，标准差为 <code>1</code>。<strong>目的</strong>是将数据限制在统一的分布下，保证网络的稳定性。然后再引入一个重构变换，还原最优的原始输入分布，做为神经元的激活值，该方法可以很好的缓解梯度消失问题。</p>
<ul>
<li><strong>残差结构</strong></li>
</ul>
<p>残差的方式，能使得深层的网络梯度通过跳级连接路径直接返回到浅层部分，使得网络无论多深都能将梯度进行有效的回传。</p>
<ul>
<li><strong>LSTM</strong></li>
</ul>
<p>LSTM全称是长短期记忆网络（long-short term memory networks），是不那么容易发生梯度消失的，主要原因在于LSTM内部复杂的“门”(gates)。在计算时，将过程中的梯度进行了抵消。</p>
<h4 id="深度学习为什么要使用梯度更新规则">深度学习为什么要使用梯度更新规则</h4>
<p>深度学习中的反向传播，即根据损失函数计算的误差，计算得到梯度，通过梯度反向传播的方式，指导深度网络权值的更新优化。这样做的原因在于，深层网络由许多非线性层堆叠而来，每一层非线性层都可以视为是一个非线性函数，因此整个深度网络可以视为是一个复合的非线性多元函数：<br>
<span class="math display">\[
F(x)=f_n(\cdots f_3(f_2(f_1(x)*\theta_1+b)*\theta_2+b)\cdots)
\]</span><br>
我们最终的目的是希望这个多元函数可以很好的完成输入到输出之间的映射，假设不同的输入，输出的最优解是g(x) ，那么，优化深度网络就是为了寻找到合适的权值，满足 <span class="math inline">\(Loss=L(g(x),F(x))\)</span> 取得极小值点，比如最简单的损失函数：<br>
<span class="math display">\[
Loss = \lVert g(x)-f(x) \rVert^2_2.
\]</span><br>
假设损失函数的数据空间是下图这样的，我们最优的权值就是为了寻找下图中的最小值点， 对于这种数学寻找最小值问题，采用梯度下降的方法再适合不过了。</p>
<h4 id="如何防止梯度下降陷入局部最优解"><a href="#优化算法">如何防止梯度下降陷入局部最优解</a></h4>
<ul>
<li><strong>Mini-Batch SGD</strong></li>
<li><strong>SGD-M</strong></li>
<li><strong>自适应学习率</strong></li>
<li>Adagrad</li>
<li>RMSprop</li>
<li>Adam</li>
</ul>
<h4 id="梯度下降与正规方程的比较">梯度下降与正规方程的比较</h4>
<table style="width:89%;">
<colgroup>
<col width="31%">
<col width="56%">
</colgroup>
<thead>
<tr class="header">
<th>梯度下降(Gradient Descent)</th>
<th>正规方程(Normal Equation)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>需要选择学习率</td>
<td>不需要</td>
</tr>
<tr class="even">
<td>需要多次迭代</td>
<td>一次运算得出</td>
</tr>
<tr class="odd">
<td>当特征数量 n 大时，也能较好适用</td>
<td>需要计算 <br>如果特征数量 n 较大则运算代价大，因为矩阵逆的计算时间复杂度为 ，通常来说当n小于10000时还是可以接受的</td>
</tr>
<tr class="even">
<td>适用于各种类型的模型</td>
<td>只适用于线性模型，不适合逻辑回归等其他模型</td>
</tr>
</tbody>
</table>
<h4 id="最小二乘法和梯度下降法区别">最小二乘法和梯度下降法区别</h4>
<p><strong>最小二乘法的目标：求误差的最小平方和</strong>，对应有两种：线性和非线性。线性最小二乘的解是closed-form即<span class="math inline">\(x=\left(A^{T} A\right)^{-1} A^{T} b\)</span>，而非线性最小二乘没有closed-form，通常用迭代法求解。</p>
<p>迭代法，即在每一步update未知量逐渐逼近解，可以用于各种各样的问题（包括最小二乘），比如求的不是误差的最小平方和而是最小立方和。</p>
<p>梯度下降是迭代法的一种，可以用于求解最小二乘问题（线性和非线性都可以）。高斯-牛顿法是另一种经常用于求解非线性最小二乘的迭代法（一定程度上可视为标准非线性最小二乘求解方法）。</p>
<p>所以如果把最小二乘看做是优化问题的话，那么梯度下降是求解方法的一种，<span class="math inline">\(x=\left(A^{T} A\right)^{-1} A^{T} b\)</span> 是求解线性最小二乘的一种，高斯-牛顿法和Levenberg-Marquardt则能用于求解非线性最小二乘。</p>
<h4 id="基于二阶梯度的优化算法">基于二阶梯度的优化算法</h4>
<h4 id="牛顿法">牛顿法</h4>
<ul>
<li><p>梯度下降使用的梯度信息实际上是<strong>一阶导数</strong></p></li>
<li><p>牛顿法除了一阶导数外，还会使用<strong>二阶导数</strong>的信息</p></li>
<li><p>根据导数的定义，一阶导描述的是函数值的变化率，即<strong>斜率</strong>；二阶导描述的则是斜率的变化率，即曲线的弯曲程度——<strong>曲率</strong></p></li>
</ul>
<blockquote>
<p>数学/<a href="../C-数学/B-微积分的本质#泰勒级数">泰勒级数</a></p>
</blockquote>
<p><strong>牛顿法更新过程</strong> TODO</p>
<blockquote>
<p>《统计学习方法》 附录 B</p>
</blockquote>
<h5 id="为什么牛顿法比梯度下降收敛更快">为什么牛顿法比梯度下降收敛更快？</h5>
<blockquote>
<p><a href="https://www.cnblogs.com/shixiangwan/p/7532830.html" target="_blank" rel="noopener">常见的几种最优化方法（梯度下降法、牛顿法、拟牛顿法、共轭梯度法等）</a> - 蓝鲸王子 - 博客园</p>
</blockquote>
<p><strong>几何理解</strong></p>
<ul>
<li>牛顿法就是用一个<strong>二次曲面</strong>去拟合你当前所处位置的局部曲面；而梯度下降法是用一个平面去拟合当前的局部曲面。</li>
<li>通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的<strong>下降路径</strong>会更符合真实的最优下降路径。</li>
</ul>
<p><strong>通俗理解</strong></p>
<ul>
<li>比如你想找一条最短的路径走到一个盆地的最底部，</li>
<li>梯度下降法每次只从你当前所处位置选一个坡度最大的方向走一步；</li>
<li>牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。</li>
<li>所以，牛顿法比梯度下降法看得更远，能<strong>更快</strong>地走到最底部。</li>
</ul>
<h5 id="牛顿法的优缺点">牛顿法的优缺点</h5>
<ul>
<li><p><strong>优点</strong></p></li>
<li><p>收敛速度快，能用更少的迭代次数找到最优解</p></li>
<li><p><strong>缺点</strong></p></li>
<li><p>每一步都需要求解目标函数的 <strong>Hessian 矩阵</strong>的逆矩阵，计算复杂</p>
<blockquote>
<p>Hessian 矩阵即由二阶偏导数构成的方阵</p>
</blockquote></li>
</ul>
<h4 id="拟牛顿法-todo">拟牛顿法 TODO</h4>
<ul>
<li>用其他近似方法代替求解 <strong>Hessian 矩阵</strong>的逆矩阵</li>
</ul>
<h4 id="梯度检验">梯度检验</h4>
<p>思想：是通过估计梯度值来检验我们计算的导数值是否真的是我们要求的。</p>
<p>对梯度的估计采用的方法是在代价函数上沿着切线的方向选择离两个非常近的点然后计算两个点的平均值用以估计梯度。即对于某个特定的 <span class="math inline">\(θ\)</span>，我们计算出在 <span class="math inline">\(θ-ε\)</span> 处和 <span class="math inline">\(θ+ε\)</span> 的代价值（ ε 是一个非常小的值，通常选取 0.001），然后求两个代价的平均，用以估计在 <span class="math inline">\(θ\)</span> 处的代价值。</p>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/梯度检验.png">

</div>
<p>当 <span class="math inline">\(θ\)</span> 是一个向量时，我们则需要对偏导数进行检验。因为代价函数的偏导数检验只针对一个参数的改变进行检验，下面是一个只针对 <span class="math inline">\(θ_1\)</span> 进行检验的示例：<br>
<span class="math display">\[
\frac{\partial}{\partial \theta_{I}}=\frac{J\left(\theta_{I}+\varepsilon_{l}, \theta_{2}, \theta_{3} \dots \theta_{n}\right)-J\left(\theta_{I}-\varepsilon_{l}, \theta_{2}, \theta_{3} \dots \theta_{n}\right)}{2 \varepsilon}
\]</span><br>
最后我们还需要对通过反向传播方法计算出的偏导数进行检验。</p>
<p>根据上面的算法，计算出的偏导数存储在矩阵 <span class="math inline">\(D^{(l)}_{ij}\)</span> 中。检验时，我们要将该矩阵展开成为向量，同时我们也将 <span class="math inline">\(θ\)</span> 矩阵展开为向量，我们针对每一个 <span class="math inline">\(θ\)</span> 都计算一个近似的梯度值，将这些值存储于一个近似梯度矩阵中，最终将得出的这个矩阵同 <span class="math inline">\(D^{(l)}_{ij}\)</span> 进行比较。</p>
<hr>
<h3 id="batch_size">batch_size</h3>
<h4 id="为什么需要-batch_size">为什么需要 batch_size</h4>
<p><code>batch</code> 的选择，首先决定的是下降的方向。</p>
<p>如果数据集比较小，可采用全数据集的形式，好处是：由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。但由于不同权重的梯度值差别巨大，因此选取一个全局的学习率很困难。 随着数据集的海量增长和内存限制，一次性载入所有的数据进来变得越来越不可行。</p>
<p>而 batch_size = 1 时为随机梯度下降，每次只用一个训练数据来更新参数，导致迭代方向变化很大，不能很快的收敛到局部最优解。</p>
<p>所以可以采用 batch_size 批量的进行训练。</p>
<h4 id="在合理范围内增大batch_size有何好处">在合理范围内，增大Batch_Size有何好处</h4>
<ul>
<li>内存利用率提高了，大矩阵乘法的并行化效率提高。</li>
<li>跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快。</li>
<li>在一定范围内，一般来说 batch_size 越大，其确定的下降方向越准，引起训练震荡越小。</li>
</ul>
<h4 id="盲目增大-batch_size-有何坏处">盲目增大 Batch_Size 有何坏处</h4>
<ol style="list-style-type: decimal">
<li>batch_size 太小，模型表现效果极其糟糕(error飙升)。</li>
<li>随着 batch_size 增大，处理相同数据量的速度越快。</li>
<li>随着 batch_size 增大，达到相同精度所需要的 epoch 数量越来越多。</li>
<li>由于上述两种因素的矛盾， batch_size 增大到某个时候，达到时间上的最优。</li>
<li>由于最终收敛精度会陷入不同的局部极值，因此 batch_size 增大到某些时候，达到最终收敛精度上的最优。</li>
</ol>
<hr>
<h3 id="学习率-learning-rate">学习率 learning rate</h3>
<hr>
<h3 id="数据处理">数据处理</h3>
<h4 id="类别不平衡问题">类别不平衡问题</h4>
<ul>
<li>对多的那个类别进行<strong>欠采样(under-sampling)</strong>，舍弃一部分数据，使其与较少类别的数据相当（代表方法：<strong>Easy Ensemble</strong>）</li>
</ul>
<blockquote>
<p><strong>Easy Ensemble</strong>： 结合集成学习来有效的使用数据，假设正例数为少数 n，而反例数据多为 m 个。我们可以通过欠采样，随机无重复的生成（k=m/n）个反例子集，并将每个子集都与相同正例数据合并生成 k 个新的训练样本。我们在 k 个训练样本上分别训练一个分类器，预测时，将 k 个分类器进行集体投票表决</p>
</blockquote>
<ul>
<li>对较少的类别进行<strong>过采样(over-sampling)</strong>，重复使用一部分数据，使其与较多类别的数据相当（代表方法：<strong>SMOTE</strong>）</li>
</ul>
<blockquote>
<p><strong>SMOTE</strong>：通过对训练集里的较少的类别进行插值来产生而外的的数据</p>
</blockquote>
<ul>
<li><strong>阈值调整（threshold moving）</strong>，将原本默认为0.5的阈值调整到 <code>较少类别 / (较少类别+较多类别)</code> 即可</li>
</ul>
<h4 id="处理数据中的缺失值">处理数据中的缺失值</h4>
<p>可以分为以下 2 种情况：</p>
<ol style="list-style-type: decimal">
<li><strong>缺失值较多</strong></li>
</ol>
<p>直接舍弃该列特征，否则可能会带来较大的噪声，从而对结果造成不良影响。</p>
<ol start="2" style="list-style-type: decimal">
<li><strong>缺失值较少</strong></li>
</ol>
<p>当缺失值较少（&lt;10%）时，可以考虑对缺失值进行填充，以下是几种常用的填充策略：</p>
<ul>
<li><p>用一个<strong>异常值</strong>填充（比如 0），将缺失值作为一个特征处理<code>data.fillna(0)</code></p></li>
<li><p>用<strong>均值</strong>|<strong>条件均值</strong>填充 <code>data.fillna(data.mean())</code></p></li>
</ul>
<blockquote>
<p>如果数据是不平衡的，那么应该使用条件均值填充。所谓<strong>条件均值</strong>，指的是与缺失值所属标签相同的所有数据的均值</p>
</blockquote>
<ul>
<li><strong>用相邻数据填充</strong></li>
</ul>
<figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 用前一个数据填充</span><br><span class="line">data.fillna(<span class="function"><span class="keyword">method</span>='<span class="title">pad</span>')</span></span><br><span class="line"><span class="function"># 用后一个数据填充</span></span><br><span class="line"><span class="function"><span class="title">data</span>.<span class="title">fillna</span><span class="params">(<span class="keyword">method</span>=<span class="string">'bfill'</span>)</span></span></span><br></pre></td></tr></table></figure>
<ul>
<li><p><strong>插值</strong> <code>data.interpolate()</code></p></li>
<li><p><strong>拟合</strong></p></li>
</ul>
<blockquote>
<p>简单来说，就是将缺失值也作为一个预测问题来处理：将数据分为正常数据和缺失数据，对有值的数据采用随机森林等方法拟合，然后对有缺失值的数据进行预测，用预测的值来填充。</p>
</blockquote>
<h4 id="训练样本少的问题">训练样本少的问题</h4>
<p>（1）利用预训练模型进行迁移微调（fine-tuning）</p>
<p>（2）数据集增强</p>
<p>（3）正则化</p>
<p>（4）单样本或者少样本学习（one-shot，few-shot learning）</p>
<p>（5）半监督学习</p>
<h4 id="数据增强方法">数据增强方法</h4>
<ul>
<li><code>Color Jittering</code>：对颜色的数据增强：图像亮度、饱和度、对比度变化（此处对色彩抖动的理解不知是否得当）</li>
<li><code>PCA  Jittering</code>：首先按照RGB三个颜色通道计算均值和标准差，再在整个训练集上计算协方差矩阵，进行特征分解，得到特征向量和特征值，用来做PCA Jittering；</li>
<li><code>Random Scale</code>：尺度变换</li>
<li><code>Random Crop</code>：采用随机图像差值方式，对图像进行裁剪、缩放；包括 Scale Jittering 方法（VGG及ResNet模型使用）或者尺度和长宽比增强变换</li>
<li><code>Horizontal/Vertical Flip</code>：水平/垂直翻转；</li>
<li><code>Shift</code>：平移变换</li>
<li><code>Rotation/Reflection</code>：旋转/仿射变换；</li>
<li><code>Noise</code>：高斯噪声、模糊处理</li>
<li><code>Label Shuffle</code>：类别不平衡数据的增广</li>
</ul>
<h4 id="权值初始化方法">权值初始化方法</h4>
<p>在深度学习的模型中，从零开始训练时，权重的初始化有时候会对模型训练产生较大的影响。良好的初始化能让模型快速、有效的收敛，而糟糕的初始化会使得模型无法训练。</p>
<p>目前，大部分深度学习框架都提供了各类初始化方式，其中一般常用的会有如下几种：</p>
<ol style="list-style-type: decimal">
<li><strong>常数初始化(constant)</strong></li>
</ol>
<p>把权值或者偏置初始化为一个常数。例如设置为0，偏置初始化为0较为常见，权重很少会初始化为0。TensorFlow中也有zeros_initializer、ones_initializer等特殊常数初始化函数。</p>
<ol start="2" style="list-style-type: decimal">
<li><strong>高斯初始化(gaussian)</strong></li>
</ol>
<p>给定一组均值和标准差，随机初始化的参数会满足给定均值和标准差的高斯分布。高斯初始化是很常用的初始化方式。特殊地，在TensorFlow中还有一种截断高斯分布初始化（truncated_normal_initializer），其主要为了将超过两个标准差的随机数重新随机，使得随机数更稳定。</p>
<ol start="3" style="list-style-type: decimal">
<li><strong>均匀分布初始化(uniform)</strong></li>
</ol>
<p>给定最大最小的上下限，参数会在该范围内以均匀分布方式进行初始化，常用上下限为（0，1）。</p>
<ol start="4" style="list-style-type: decimal">
<li><strong>xavier 初始化(uniform)</strong></li>
</ol>
<p>在batchnorm还未出现之前，要训练较深的网络，防止梯度弥散，需要依赖非常好的初始化方式。xavier 就是一种比较优秀的初始化方式，也是目前最常用的初始化方式之一。其目的是为了使得模型各层的激活值和梯度在传播过程中的方差保持一致。本质上xavier 还是属于均匀分布初始化，但与上述的均匀分布初始化有所不同，xavier 的上下限将在如下范围内进行均匀分布采样：<br>
<span class="math display">\[
[-\sqrt{\frac{6}{n+m}},\sqrt{\frac{6}{n+m}}]
\]</span><br>
​ 其中，n为所在层的输入维度，m为所在层的输出维度。</p>
<ol start="6" style="list-style-type: decimal">
<li><strong>kaiming初始化（msra 初始化）</strong></li>
</ol>
<p>kaiming初始化，在caffe中也叫msra 初始化。kaiming初始化和xavier 一样都是为了防止梯度弥散而使用的初始化方式。kaiming初始化的出现是因为xavier存在一个不成立的假设。xavier在推导中假设激活函数都是线性的，而在深度学习中常用的ReLu等都是非线性的激活函数。而kaiming初始化本质上是高斯分布初始化，与上述高斯分布初始化有所不同，其是个满足均值为0，方差为2/n的高斯分布：<br>
<span class="math display">\[
[0,\sqrt{\frac{2}{n}}] \\
（其中，n为所在层的输入维度）
\]</span><br>
除上述常见的初始化方式以外，不同深度学习框架下也会有不同的初始化方式，读者可自行查阅官方文档。</p>
<h4 id="深度学习是否能胜任所有数据集">深度学习是否能胜任所有数据集</h4>
<p>深度学习并不能胜任目前所有的数据环境，以下列举两种情况：</p>
<p>（1）深度学习能取得目前的成果，很大一部分原因依赖于海量的数据集以及高性能密集计算硬件。因此，当数据集过小时，需要考虑与传统机器学习相比，是否在性能和硬件资源效率更具有优势。</p>
<p>（2）深度学习目前在视觉，自然语言处理等领域都有取得不错的成果。这些领域最大的特点就是具有局部相关性。例如图像中，人的耳朵位于两侧，鼻子位于两眼之间，文本中单词组成句子。这些都是具有局部相关性的，一旦被打乱则会破坏语义或者有不同的语义。所以当数据不具备这种相关性的时候，深度学习就很难取得效果。</p>
<h4 id="共线性如何判断和解决共线性问题">共线性，如何判断和解决共线性问题</h4>
<p>对于回归算法，无论是一般回归还是逻辑回归，在使用多个变量进行预测分析时，都可能存在多变量相关的情况，这就是多重共线性。共线性的存在，使得特征之间存在冗余，导致过拟合。<br>
常用判断是否存在共线性的方法有：</p>
<p>（1）<strong>相关性分析</strong>。当相关性系数高于0.8，表明存在多重共线性；但相关系数低，并不能表示不存在多重共线性；</p>
<p>（2）<strong>方差膨胀因子VIF</strong>。当VIF大于5或10时，代表模型存在严重的共线性问题；</p>
<p>（3）<strong>条件系数检验</strong>。 当条件数大于100、1000时，代表模型存在严重的共线性问题。</p>
<p>通常可通过PCA降维、逐步回归法和LASSO回归等方法消除共线性。</p>
<h4 id="ones-hot-编码">Ones-Hot 编码</h4>
<p>很多机器学习任务中，特征并不总是连续值，有可能是分类值。<br>
考虑以下三个特征：</p>
<blockquote>
<p>[“male”, “female”][“from Europe”, “from US”, “from Asia”]<br>
[“uses Firefox”, “uses Chrome”, “uses Safari”, “uses Internet Explorer”]</p>
</blockquote>
<p>如果将上述特征用数字表示，效率会高很多。例如：</p>
<blockquote>
<p>[“male”, “from US”, “uses Internet Explorer”] 表示为[0, 1, 3][“female”, “from Asia”, “uses Chrome”]表示为[1, 2, 1]</p>
</blockquote>
<p>但是，转化为数字表示后，上述数据不能直接用在我们的分类器中。因为，分类器往往默认数据数据是连续的，并且是有序的。但按上述表示的数字并不有序的，而是随机分配的。</p>
<p><strong>One-Hot Encoding 独热编码</strong>，又称一位有效编码，其方法是使用 N 位状态寄存器来对 N 个状态进行编码，每个状态都有它独立的寄存器位，并且在任意时候，其中只有一位有效。</p>
<blockquote>
<p>自然状态码为：000,001,010,011,100,101<br>
独热编码为：000001,000010,000100,001000,010000,100000</p>
</blockquote>
<p>可以这样理解，<strong>对于每一个特征，如果它有m个可能值，那么经过独热编码后，就变成了m个二元特征。并且，这些特征互斥，每次只有一个激活。因此，数据会变成稀疏的。</strong></p>
<p>这样做的好处主要有：</p>
<ol style="list-style-type: decimal">
<li>解决了分类器不好处理属性数据的问题</li>
<li>在一定程度上也起到了扩充特征的作用</li>
</ol>
<p><strong>例子</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">encoder = preprocessing.OneHotEncoder()</span><br><span class="line">encoder.fit([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">12</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">3</span>],</span><br><span class="line">    [<span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">12</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">3</span>]</span><br><span class="line">])</span><br><span class="line">encoded_vector = encoder.transform([[<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">3</span>]]).toarray()</span><br><span class="line">print(<span class="string">"\n Encoded vector ="</span>, encoded_vector)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line">Encoded vector = [[ <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">0.</span>]]</span><br></pre></td></tr></table></figure>
<p>分析</p>
<figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">4</span>个特征：</span><br><span class="line">第一个特征（即为第一列）为<span class="string">[0,1,2,1]</span> ，其中三类特征值<span class="string">[0,1,2]</span>，因此One-Hot Code可将<span class="string">[0,1,2]</span>表示为:<span class="string">[100,010,001]</span></span><br><span class="line">同理第二个特征列可将两类特征值<span class="string">[2,3]</span>表示为<span class="string">[10,01]</span></span><br><span class="line">第三个特征将<span class="number">4</span>类特征值<span class="string">[1,2,4,5]</span>表示为<span class="string">[1000,0100,0010,0001]</span></span><br><span class="line">第四个特征将<span class="number">2</span>类特征值<span class="string">[3,12]</span>表示为<span class="string">[10,01]</span></span><br><span class="line"></span><br><span class="line">因此最后可将<span class="string">[2,3,5,3]</span>表示为<span class="string">[0,0,1,0,1,0,0,0,1,1,0]</span></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="特征选择feature-selection">特征选择(feature selection)</h3>
<h4 id="特征类型有哪些">特征类型有哪些</h4>
<p>对象本身会有许多属性。所谓特征，即能在某方面最能表征对象的一个或者一组属性。一般地，我们可以把特征分为如下三个类型：</p>
<p>（1）<strong>相关特征</strong>：对于特定的任务和场景具有一定帮助的属性，这些属性通常能有效提升算法性能；</p>
<p>（2）<strong>无关特征</strong>：在特定的任务和场景下完全无用的属性，这些属性对对象在本目标环境下完全无用；</p>
<p>（3）<strong>冗余特征</strong>：同样是在特定的任务和场景下具有一定帮助的属性，但这类属性已过多的存在，不具有产生任何新的信息的能力。</p>
<h4 id="如何考虑特征选择">如何考虑特征选择</h4>
<p>当完成数据预处理之后，对特定的场景和目标而言很多维度上的特征都是不具有任何判别或者表征能力的，所以需要对数据在维度上进行筛选。一般地，可以从以下两个方面考虑来选择特征:</p>
<p>（1）<strong>特征是否具有发散性</strong>：某个特征若在所有样本上的都是一样的或者接近一致，即方差非常小。 也就是说所有样本的都具有一致的表现，那这些就不具有任何信息。</p>
<p>（2）<strong>特征与目标的相关性</strong>：与目标相关性高的特征，应当优先选择。</p>
<h4 id="特征选择方法分类">特征选择方法分类</h4>
<p>根据特征选择的形式又可以将特征选择方法分为 3 种:<br>
（1）<strong>过滤法</strong>：按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。</p>
<p>（2）<strong>包装法</strong>：根据目标函数(通常是预测效果评分)，每次选择若干特征，或者排除若干特征。</p>
<p>（3）<strong>嵌入法</strong>：先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。</p>
<h4 id="特征选择目的">特征选择目的</h4>
<p>（1）减少特征维度，使模型泛化能力更强，减少过拟合;</p>
<p>（2）降低任务目标的学习难度；</p>
<p>（3）一组优秀的特征通常能有效的降低模型复杂度，提升模型效率</p>
<h4 id="归一化-与-标准化">归一化 与 标准化</h4>
<blockquote>
<p><strong>归一化和标准化本质上都是一种线性变换</strong></p>
</blockquote>
<p><strong>（1）归一化</strong></p>
<p>１）把数据把数据映射到 0～1 范围之内处理，更加便捷，可以加快梯度下降求解速度，提升模收敛速度。消除了量纲，便于不同单位或量级的指标能够进行比较和加权。<br>
<span class="math display">\[
\frac{x - min}{max-min}
\]</span><br>
<strong>（2）标准化</strong></p>
<p>对数据减去均值，除以标准差，转换为标准正太分布（均值为0， 标准差为1），和整体样本分布相关，每个样本都能对标准化产生影响。<br>
<span class="math display">\[
\frac{x - \mu}{\sigma}
\]</span><br>
其中 <span class="math inline">\(\mu\)</span> 和 <span class="math inline">\(\sigma\)</span> 代表样本的均值和标准差。</p>
<p><strong>（3）两者的异同</strong></p>
<ul>
<li>归一化：缩放仅仅跟最大、最小值的差别有关。</li>
<li>标准化：缩放和每个点都有关系，通过方差（variance）体现出来。与归一化对比，标准化中所有数据点都有贡献（通过均值和标准差造成影响）。</li>
</ul>
<p><strong>（4）如何选择</strong></p>
<ul>
<li>如果对<strong>输出结果范围有要求</strong>，用归一化</li>
<li>如果<strong>数据较为稳定，不存在极端的最大最小值</strong>，用归一化</li>
<li>如果数据<strong>存在异常值和较多噪音</strong>，用标准化，可以间接通过中心化避免异常值和极端值的影响</li>
</ul>
<h3 id="batch-normalization批标准化">Batch Normalization(批标准化)</h3>
<h4 id="bn层作用以及如何使用bn层">BN层作用，以及如何使用BN层</h4>
<p><strong><a href="https://www.cnblogs.com/guoyaohua/p/8724433.html" target="_blank" rel="noopener">BN</a></strong> (Batch Normalization批标准化) 是一种<strong>正则化</strong>方法（减少泛化误差），主要作用有：</p>
<ul>
<li>加速网络的训练</li>
<li>缓解梯度消失</li>
<li>防止过拟合</li>
<li>增强模型的泛化能力</li>
<li>支持更大的学习率</li>
<li>降低了参数初始化的要求</li>
</ul>
<h4 id="动机">动机</h4>
<ul>
<li><strong>训练的本质是学习数据分布</strong>。如果训练数据与测试数据的分布不同会<strong>降低</strong>模型的<strong>泛化能力</strong>。因此，应该在开始训练前对所有输入数据做归一化处理。</li>
<li>而在神经网络中，因为<strong>每个隐层</strong>的参数不同，会使下一层的输入发生变化，从而导致每一批数据的分布也发生改变；<strong>致使</strong>网络在每次迭代中都需要拟合不同的数据分布，增大了网络的训练难度与<strong>过拟合</strong>的风险。</li>
</ul>
<h4 id="基本原理">基本原理</h4>
<p><strong>（1）训练阶段</strong></p>
<ul>
<li>BN 方法会针对<strong>每一批数据</strong>，在<strong>网络的每一层输入</strong>之前增加<strong>归一化</strong>处理，使输入的均值为 <code>0</code>，标准差为 <code>1</code>。<strong>目的</strong>是将数据限制在统一的分布下。</li>
<li>具体来说，针对每层的第 <code>k</code> 个神经元，计算<strong>这一批数据</strong>在第 <code>k</code> 个神经元的均值与标准差，然后将归一化后的值作为该神经元的激活值。</li>
</ul>
<p><span class="math display">\[
\boldsymbol{\hat{x}_{k}} = \frac{\boldsymbol{x_{k}}-\mathrm{E}\left[\boldsymbol{x_{k}}\right]}{\sqrt{\operatorname{Var}\left[\boldsymbol{x_{k}}\right]}}
\]</span></p>
<ul>
<li>BN 可以看作在各层之间加入了一个新的计算层，<strong>对数据分布进行额外的约束</strong>，从而增强模型的泛化能力；</li>
<li>但同时 BN 也降低了模型的拟合能力，破坏了之前学到的<strong>特征分布</strong>；</li>
<li>为了<strong>恢复数据的原始分布</strong>，BN 引入了一个<strong>重构变换</strong>来还原最优的输入数据分布，其中 <code>γ</code> 和 <code>β</code> 是我们要训练学习的参数。</li>
</ul>
<p><span class="math display">\[
\boldsymbol{y_{k}} \leftarrow \gamma \boldsymbol{\hat{x}_{k}}+\beta
\]</span></p>
<p><strong>小结：</strong></p>
<ul>
<li>以上过程可归纳为一个 <strong><code>BN(x)</code> 函数</strong>：</li>
</ul>
<p><span class="math display">\[
\large\begin{aligned}
\large\boldsymbol{y_i}=
\mathrm{BN}(\boldsymbol{x_i})
&amp;=\gamma\boldsymbol{\hat{x}_i} + \beta \\
&amp;=\gamma\frac{\boldsymbol{x_i}-\boldsymbol{\mathrm{E}[x_i]}}{\sqrt{\boldsymbol{\mathrm{Var}[x_i]}+\epsilon}}+\beta\end{aligned}
\]</span></p>
<ul>
<li>完整算法：</li>
</ul>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/BN.png">

</div>
<blockquote>
<p><strong>输入</strong>：上一层输出结果 $  = x_1, x_2, … , x_m$ ，学习参数 <span class="math inline">\(\gamma, \beta\)</span><br>
<strong>算法流程</strong>：</p>
<ol style="list-style-type: decimal">
<li>计算上一层输出数据的均值</li>
</ol>
<p><span class="math display">\[
\mu_{\mathcal{B}} = \frac{1}{m} \sum_{i=1}^m(x_i)
\]</span></p>
<p>​ （其中，m是此次训练样本 batch 的大小）</p>
<ol style="list-style-type: decimal">
<li>计算上一层输出数据的标准差</li>
</ol>
<p><span class="math display">\[
\sigma_{\mathcal{B}}^2 = \frac{1}{m} \sum_{i=1}^m (x_i - \mu_{\mathcal{B}})^2
\]</span></p>
<ol style="list-style-type: decimal">
<li>归一化处理，目的是为了将数据限制在统一的分布之下</li>
</ol>
<p><span class="math display">\[
\hat x_i = \frac{x_i - \mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{B}}^2 + \epsilon}}
\]</span></p>
<p>（其中 $ $ 是为了避免分母为 0 而加进去的接近于 0 的很小值）</p>
<ol style="list-style-type: decimal">
<li>重构，对经过上面归一化处理得到的数据进行重构，还原最优的输入数据分布，作为该层神经元的激活值<br>
<span class="math display">\[
   y_i = \gamma \hat x_i + \beta
   \]</span><br>
（其中，$ , $ 为可学习参数）</li>
</ol>
</blockquote>
<p><strong>（2）测试阶段</strong></p>
<p>测试的时候，每次可能只会传入<strong>单个数据</strong>，此时的均值和标准差，模型会使用<strong>全局统计量</strong>代替<strong>批统计量</strong>；即使用训练时所有batch得到的一组组的均值和方差，计算其数学期望做为全局统计量。<br>
<span class="math display">\[
\begin{array}{c}{\mathrm{E}[x] \leftarrow \mathrm{E}\left[\mu_{i}\right]} \\ {\operatorname{Var}[x] \leftarrow \frac{m}{m-1} \mathrm{E}\left[\sigma_{i}^{2}\right]}\end{array}
\]</span></p>
<blockquote>
<p>其中 <span class="math inline">\(μ_i\)</span> 和 <span class="math inline">\(σ_i\)</span> 分别表示第 <span class="math inline">\(i\)</span> 轮 batch 保存的均值和标准差；<span class="math inline">\(m\)</span> 为 batch_size，系数 <span class="math inline">\(\frac{m}{m-1}\)</span> 用于计算<strong>无偏方差估计</strong> （原文称该方法为<strong>移动平均</strong>（moving averages））</p>
</blockquote>
<p>然后再将按照训练的流程，将输入数据，减去全局统计量均值除以标准差，再进行重构变换得到新的数据作为神经元的激活值。</p>
<ul>
<li>此时，<code>BN(x)</code> 调整为：</li>
</ul>
<p><span class="math display">\[
\large\begin{aligned}\mathrm{BN}(\boldsymbol{x_i})&amp;=\gamma\frac{\boldsymbol{x_i}-\boldsymbol{\mathrm{E}[x_i]}}{\sqrt{\boldsymbol{\mathrm{Var}[x_i]} + \epsilon}} + \beta\\&amp;=\frac{\gamma}{\sqrt{\boldsymbol{\mathrm{Var}[x_i]} + \epsilon}}\boldsymbol{x_i} + \left(\beta-\frac{\gamma\boldsymbol{\mathrm{E}[x_i]}}{\sqrt{\boldsymbol{\mathrm{Var}[x_i]} + \epsilon}}\right)\end{aligned}
\]</span></p>
<p>这样写的目的是为了减少计算量，推理阶段公式中的两个分式是固定值，可以预先计算好，这样推理阶段就可以直接使用。</p>
<ul>
<li><strong>完整算法：</strong></li>
</ul>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/BN_02.png">

</div>
<ul>
<li><strong>Reference:</strong> <a href="https://www.cnblogs.com/guoyaohua/p/8724433.html" target="_blank" rel="noopener">理解</a> <a href="https://blog.csdn.net/qq_25737169/article/details/79048516" target="_blank" rel="noopener">源码解读</a> <a href="https://zhuanlan.zhihu.com/p/34879333" target="_blank" rel="noopener">实战</a> <a href="https://www.zhihu.com/question/38102762" target="_blank" rel="noopener">知乎</a></li>
</ul>
<h4 id="为什么训练时不采用移动平均">为什么训练时不采用移动平均？</h4>
<ul>
<li>用 BN 的目的就是为了保证每批数据的分布稳定，使用训练时使用全局统计量反而违背了这个初衷</li>
<li>BN 的作者认为在训练时采用移动平均可能会与梯度优化存在冲突</li>
</ul>
<h4 id="bnlningn的异同">BN、LN、IN、GN的异同</h4>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/GN.png">

</div>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/BN_croped.png">

</div>
<p>深度网络中的数据维度一般为：[N, C, H, W] 或者[N, H, W, C]格式，分别对应上图两种排列方式。</p>
<p>其中：</p>
<ul>
<li>N ：batch_size</li>
<li>W ：feature map 的宽</li>
<li>H ：feature map 的高</li>
<li>C ：feature map 的通道数</li>
</ul>
<p><strong><code>BN</code>：</strong> 在batch的维度上进行norm，归一化维度为 <strong>[N, H, W]</strong>，BN对batch size有依赖，当batch size较大时，有不错的效果。而 LN、IN、GN 能够摆脱这种依赖，其中GN效果最好。</p>
<p><strong><code>LN</code>：</strong> 避开了batch维度，归一化的维度为 <strong>[C，H，W]</strong></p>
<p><strong><code>IN</code>：</strong> 归一化维度为 <strong>[H, W]</strong></p>
<p><strong><code>GN</code>：</strong> GN 介于 LN 和 IN 之间，其首先将channel分为许多组（group），对每一组做归一化，即先将feature的维度由[N, C, H, W] reshape为 [N*G，C//G , H, W]，归一化的维度为 <strong>[C//G , H, W]</strong> 。GN相当于特征的group归一化，其对batch_size更鲁棒</p>
<p>事实上，GN 的极端情况就是 LN 和 IN，分别对应G等于1和G等于C</p>
<hr>
<h3 id="dropout">Dropout</h3>
<blockquote>
<p><a href="https://blog.csdn.net/stdcoutzyx/article/details/49022443" target="_blank" rel="noopener">Reference</a></p>
</blockquote>
<p>Dropout是指在网络的训练过程中， 每份小批量训练数据集，以一定的概率随机地 “临时丢弃”一部分神经元节点， 相当于每次迭代都在训练不同结构的神经网络，相当于Bagging的近似集成。 对于任意神经元， 每次训练中都与一组随机挑选的不同的神经元集合共同进行优化， 这个过程会减弱全体神经元之间的联合适应性， 减少过拟合的风险， 增强泛化能力。</p>
<p>但与Bagging不同的是：</p>
<ul>
<li><p>Bagging涉及多个模型的同时训练与测试评估， 当网络与参数规模庞大时， 这种集成方式需要消耗大量的运算时间与空间。 Dropout在小批量级别上的操作， 提供了一种轻量级的Bagging集成近似， 能够实现指数级数量神经网络的训练与评测。</p></li>
<li><p>Bagging训练所有的模型都是独立的，而在 Dropout 的情况下，所有模型<strong>共享参数</strong>，其中每个模型继承父神经网络参数的不同子集。</p></li>
</ul>
<blockquote>
<ul>
<li>利用<strong>自助采样法</strong>对训练集随机采样，重复进行 <code>T</code> 次;</li>
<li>基于每个采样集训练一个基学习器，并得到 <code>T</code> 个基学习器；</li>
<li>预测时，集体<strong>投票决策</strong>。</li>
</ul>
</blockquote>
<ul>
<li>在 Bagging 的情况下，每一个模型都会在其相应训练集上训练到收敛。而在 Dropout 的情况下，通常大部分模型都没有显式地被训练；取而代之的是，在单个步骤中我们训练一小部分的子网络，参数共享会使得剩余的子网络也能有较好的参数设定。</li>
</ul>
<h4 id="dropout-具体实现">Dropout 具体实现</h4>
<p><strong>(1) 训练时</strong>， 每个神经元节点需要增加一个概率系数：</p>
<ul>
<li>没有Dropout的网络计算公式：</li>
</ul>
<p><span class="math display">\[
\begin{aligned} z_{i}^{(l+1)} &amp;=\mathrm{w}_{i}^{(l+1)} \mathrm{y}^{l}+b_{i}^{(l+1)} \\ y_{i}^{(l+1)} &amp;=f\left(z_{i}^{(l+1)}\right) \end{aligned}
\]</span></p>
<ul>
<li>采用Dropout的网络计算公式：</li>
</ul>
<p><span class="math display">\[
\begin{aligned} r_{j}^{(l)} &amp; \sim \text { Bernoulli }(p) \\ \widetilde{\mathbf{y}}^{(l)} &amp;=\mathbf{r}^{(l)} * \mathbf{y}^{(l)} \\ z_{i}^{(l+1)} &amp;=\mathbf{w}_{i}^{(l+1)} \widetilde{\mathbf{y}}^{l}+b_{i}^{(l+1)} \\ y_{i}^{(l+1)} &amp;=f\left(z_{i}^{(l+1)}\right) \end{aligned}
\]</span></p>
<p>Bernoulli函数的作用是以概率系数 <span class="math inline">\(p\)</span> 随机生成一个取值为0或1的向量， 代表每个神经元是否需要被丢弃。 如果取值为 0， 则该神经元将不会计算梯度或参与后面的误差传播。</p>
<p><strong>(2) 测试时</strong>，每一个神经单元的权重参数要乘以概率 <span class="math inline">\(p\)</span>，以恢复在训练中该神经元只有 <span class="math inline">\(p\)</span> 的概率被用于整个神经网络的前向传播计算。<br>
<span class="math display">\[
w_{t e s t}^{(l)}=p W^{(l)}
\]</span></p>
<h4 id="dropput-为何能解决过拟合">Dropput 为何能解决过拟合？</h4>
<ul>
<li>对于任意神经元， 每次训练中都与一组随机挑选的不同的神经元集合共同进行优化， 这个过程会减弱全体神经元之间的联合适应性， 减少过拟合的风险， 增强泛化能力。</li>
</ul>
<h4 id="dropout率的选择">Dropout率的选择</h4>
<ul>
<li>经过交叉验证，隐含节点 Dropout 率等于 0.5 的时候效果最好，原因是 0.5 的时候 dropout 随机生成的网络结构最多。</li>
<li>Dropout 也可以被用作一种添加噪声的方法，直接对 input 进行操作。输入层设为更接近 1 的数。使得输入变化不会太大（0.8）</li>
<li>对参数 <span class="math inline">\(w\)</span> 的训练进行球形限制 (max-normalization)，对 dropout 的训练非常有用。</li>
<li>球形半径 <span class="math inline">\(c\)</span> 是一个需要调整的参数，可以使用验证集进行参数调优。</li>
<li>dropout 自己虽然也很牛，但是 dropout、max-normalization、large decaying learning rates and high momentum 组合起来效果更好，比如 max-norm regularization 就可以防止大的learning rate 导致的参数 blow up。</li>
<li>使用 pretraining 方法也可以帮助 dropout 训练参数，在使用 dropout 时，要将所有参数都乘以 $ 1/p $。</li>
</ul>
<h4 id="dropout-缺点">Dropout 缺点</h4>
<ul>
<li>增加训练时间：因为引入dropout之后相当于每次只是训练的原先网络的一个子网络，为了达到同样的精度需要的训练次数会增多。</li>
<li>代价函数 <span class="math inline">\(J\)</span> 不再被明确定义，每次迭代，都会随机移除一些节点，如果再三检查梯度下降的性能，实际上是很难进行复查的。定义明确的代价函数 <span class="math inline">\(J\)</span> 每次迭代后都会下降，因为我们所优化的代价函数 <span class="math inline">\(J\)</span> 实际上并没有明确定义，或者说在某种程度上很难计算，所以我们失去了调试工具来绘制这样的图片。我通常会关闭dropout函数，将keep-prob的值设为1，运行代码，确保 <span class="math inline">\(J\)</span> 函数单调递减。然后打开dropout函数，希望在dropout过程中，代码并未引入bug。我觉得你也可以尝试其它方法，虽然我们并没有关于这些方法性能的数据统计，但你可以把它们与dropout方法一起使用。</li>
</ul>
<hr>
<h3 id="优化算法">优化算法</h3>
<p>梯度下降是目前神经网络中使用最为广泛的优化算法之一。为了弥补朴素梯度下降的种种缺陷，研究者们发明了一系列变种算法，优化算法经历了 <code>SGD -&gt; SGDM -&gt; NAG -&gt; AdaGrad -&gt; AdaDelta -&gt; RMSprop -&gt; Adam -&gt; NAdam</code> 这样的发展历程。</p>
<h4 id="gradient-descent">Gradient Descent</h4>
<p>梯度下降是指，在给定待优化的模型参数 <span class="math inline">\(\theta \in \mathbb{R}^d\)</span> 和目标函数 <span class="math inline">\(J(\theta)\)</span> 后，算法通过沿梯度 <span class="math inline">\(\nabla_\theta J(\theta)\)</span>的相反方向更新 <span class="math inline">\(\theta\)</span> 来最小化 <span class="math inline">\(J(\theta)\)</span> 。学习率 <span class="math inline">\(\eta\)</span> 决定了每一时刻的更新步长。对于每个epoch <span class="math inline">\(t\)</span> ，我们可以用下述步骤描述梯度下降的流程：</p>
<p>(1) 计算目标函数关于参数的梯度<br>
<span class="math display">\[
g_t = \nabla_\theta J(\theta)
\]</span><br>
(2) 根据历史梯度计算一阶和二阶动量<br>
<span class="math display">\[
m_t = \phi(g_1, g_2, \cdots, g_t)
\]</span></p>
<p><span class="math display">\[
v_t = \psi(g_1, g_2, \cdots, g_t)
\]</span></p>
<p>(3) 计算当前时刻的下降梯度(步长) <span class="math inline">\(\frac{m_t}{\sqrt{v_t + \epsilon}}\)</span> ，并根据下降梯度更新模型参数<br>
<span class="math display">\[
\theta_{t+1} = \theta_t - \frac{m_t}{\sqrt{v_t + \epsilon}}
\]</span><br>
其中， <span class="math inline">\(\epsilon\)</span> 为平滑项，防止分母为零，通常取 <span class="math inline">\(1e-8\)</span> 。</p>
<blockquote>
<p>注意：本文提到的步长，就是下降梯度。</p>
</blockquote>
<h4 id="gradient-descent-和其算法变种">Gradient Descent 和其算法变种</h4>
<p>根据以上框架，我们来分析和比较梯度下降的各变种算法。</p>
<h4 id="vanilla-sgd">Vanilla SGD</h4>
<p>朴素 SGD (Stochastic Gradient Descent) 最为简单，没有动量的概念，即<br>
<span class="math display">\[
m_t = \eta g_t
\]</span></p>
<p><span class="math display">\[
v_t = I^2
\]</span></p>
<p><span class="math display">\[
\epsilon = 0
\]</span></p>
<p>这时，更新步骤就是最简单的<br>
<span class="math display">\[
\theta_{t+1}= \theta_t - \eta g_t
\]</span><br>
<strong>缺点：</strong></p>
<ul>
<li>收敛速度慢，而且可能会在沟壑的两边持续震荡</li>
<li>容易停留在一个局部最优点</li>
<li>如何合理的选择学习率也是 SGD 的一大难点</li>
</ul>
<h4 id="sgd-with-momentum">SGD with Momentum</h4>
<p><strong>为了抑制SGD的震荡，SGD-M认为梯度下降过程可以加入惯性(动量) Momentum[3]，加速 SGD 在正确方向的下降并抑制震荡</strong>。就好像下坡的时候，如果发现是陡坡，那就利用惯性跑的快一些。其在SGD的基础上，引入了一阶动量，然后进行更新：<br>
<span class="math display">\[
m_t = \gamma m_{t-1} + \eta g_t \\
\theta_{t+1}= \theta_t - m_t
\]</span><br>
即在原步长(SGD中是<span class="math inline">\(\eta g_t\)</span>) 之上，增加了与上一时刻动量相关的一项 <span class="math inline">\(\gamma m_{t-1}\)</span>，目的是结合上一时刻步长(下降梯度)， 其中 <span class="math inline">\(m_{t-1}\)</span> 是上一时刻的动量，<span class="math inline">\(\gamma\)</span> 是动量因子。也就是说，<span class="math inline">\(t\)</span> 时刻的下降方向，不仅由当前点的梯度方向决定，而且由此前累积的下降方向决定，<span class="math inline">\(\gamma\)</span> 通常取 0.9 左右。这就意味着下降方向主要是此前累积的下降方向，并略微偏向当前时刻的下降方向。<strong>这使得参数中那些梯度方向变化不大的维度可以加速更新，并减少梯度方向变化较大的维度上的更新幅度</strong>。由此产生了加速收敛和减小震荡的效果。</p>
<table>
<thead>
<tr class="header">
<th align="center"><img src="/2018/04/07/Try-your-best!You-will-find-the-job/2.jpg"></th>
<th align="center"><img src="/2018/04/07/Try-your-best!You-will-find-the-job/3.jpg"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">图 1(a): SGD</td>
<td align="center">图 1(b): SGD with momentum</td>
</tr>
</tbody>
</table>
<p>从图 1 中可以看出，引入动量有效的加速了梯度下降收敛过程。</p>
<h4 id="nesterov-accelerated-gradient">Nesterov Accelerated Gradient</h4>
<p><strong>SGD 还有一个问题是困在局部最优的沟壑里面震荡</strong>。想象一下你走到一个盆地，四周都是略高的小山，你觉得没有下坡的方向，那就只能待在这里了。可是如果你爬上高地，就会发现外面的世界还很广阔。因此，我们不能停留在当前位置去观察未来的方向，而要向前一步、多看一步、看远一些。</p>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/4.jpg">

</div>
​
<center>
图 2: Nesterov update
</center>
<p>NAG全称Nesterov Accelerated Gradient，则是在SGD、SGD-M的基础上的进一步改进，算法能够在目标函数有增高趋势之前，减缓更新速率。我们知道在时刻 <span class="math inline">\(t\)</span> 的主要下降方向是由累积动量决定的，自己的梯度方向说了也不算，那与其看当前梯度方向，不如先看看如果跟着累积动量走了一步后，那个时候再怎么走。因此，NAG在步骤1，<strong>不计算当前位置的梯度方向，而是计算如果按照累积动量走了一步，那个时候的下降方向</strong>：<br>
<span class="math display">\[
g_t = \nabla_\theta J(\theta - \gamma m_{t-1})  \tag{1}
\]</span><br>
参考图2理解：最初，SGD-M先计算当前时刻的梯度（短蓝向量）和累积动量 （长蓝向量）进行参数更新。改进的方法NAG，先利用累积动量计算出下一时刻的 <span class="math inline">\(\theta\)</span> 的近似位置 <span class="math inline">\(\theta - \gamma m_{t-1}\)</span>（棕向量），并根据该未来位置计算梯度（红向量）公式(1)，然后使用和 SGD-M 中相同的方式计算当前时刻的动量项(下降梯度)，进而得到完整的参数更新（绿向量），公式(2)即将该未来位置的梯度与累积动量计算当前时刻的动量项(下降梯度)：<br>
<span class="math display">\[
\begin{equation}
\begin{split}
m_t   &amp;= \gamma m_{t-1} + \eta g_t \\
    &amp;= \gamma m_{t-1} + \eta  \nabla_\theta J(\theta - \gamma m_{t-1})  
\end{split}
\end{equation} \tag{2}
\]</span></p>
<p>更新参数：<br>
<span class="math display">\[
\theta_{t+1} = \theta_t -  m_t  \tag{3}
\]</span></p>
<p>这种计算梯度的方式可以使算法更好的「预测未来」，提前调整更新速率。</p>
<blockquote>
<p>注意：<br>
累积动量指的是上一时刻的动量乘上动量因子: <span class="math inline">\(\gamma m_{t-1}\)</span><br>
当前时刻的动量项指的是: <span class="math inline">\({m_t}\)</span><br>
(上面的图只是为了助于理解，其中累积动量</p>
</blockquote>
<h4 id="adagrad">Adagrad</h4>
<p>SGD、SGD-M 和 NAG 均是以相同的学习率去更新 <span class="math inline">\(\theta\)</span> 的各个分量 <span class="math inline">\(\theta_i\)</span>。而深度学习模型中往往涉及大量的参数，不同参数的更新频率往往有所区别。<strong>对于更新不频繁的参数</strong>（典型例子：更新 word embedding 中的低频词），<strong>我们希望单次步长更大，多学习一些知识；对于更新频繁的参数，我们则希望步长较小，使得学习到的参数更稳定，不至于被单个样本影响太多。</strong></p>
<p>Adagrad在 <span class="math inline">\(t\)</span> 时刻对每一个参数 <span class="math inline">\(\theta_i\)</span> 使用了不同的学习率，我们首先介绍 Adagrad 对每一个参数的更新，然后我们对其向量化。为了简洁，令 <span class="math inline">\(g_{t,i}\)</span> 为在 <span class="math inline">\(t\)</span> 时刻目标函数关于参数 <span class="math inline">\(θ_i\)</span> 的梯度：<br>
<span class="math display">\[
g_{t, i} = \nabla_\theta J(\theta_{i})
\]</span><br>
在 <span class="math inline">\(t\)</span> 时刻，对每个参数 <span class="math inline">\(θ_i\)</span> 的更新过程变为：<br>
<span class="math display">\[
\theta_{t+1, i} = \theta_{t, i} - \eta g_{t, i}
\]</span><br>
对于上述的更新规则，在 <span class="math inline">\(t\)</span> 时刻，我们要计算 <span class="math inline">\(θ_i\)</span> 从初始时刻到 <span class="math inline">\(t\)</span> 时刻的历史梯度平方和，来修正每一个参数 <span class="math inline">\(θ_i\)</span> 的学习率：<br>
<span class="math display">\[
\theta_{t+1, i}=\theta_{t, i}-\frac{\eta}{\sqrt{v_{t, i i}+\epsilon}} \cdot g_{t, i}
\]</span><br>
其中， <span class="math inline">\(v_t \in \mathbb{R}^{d\times d}\)</span> 是对角矩阵，其元素 <span class="math inline">\(v_{t, ii}\)</span> 为参数 第 <span class="math inline">\(i\)</span> 维<strong>从初始时刻到 <span class="math inline">\(t\)</span> 时刻的梯度平方和</strong>。即通过<strong>引入二阶动量</strong>，<strong>来调整每一个参数的学习率</strong>，等效为 <span class="math inline">\(\eta / \sqrt{v_t + \epsilon}\)</span><br>
<span class="math display">\[
v_t = \text{diag}(\sum_{i=1}^t g_{i,1}^2, \sum_{i=1}^t g_{i,2}^2, \cdots, \sum_{i=1}^t g_{i,d}^2)
\]</span><br>
由于 <span class="math inline">\(v_t\)</span> 的对角线上包含了关于所有参数 <span class="math inline">\(θ\)</span> 的历史梯度的平方和，现在，我们可以通过 <span class="math inline">\(v_t\)</span> 和 <span class="math inline">\(g_t\)</span> 之间的元素向量乘法⊙向量化上述的操作：<br>
<span class="math display">\[
\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{v_{t}+\epsilon}} \odot g_{t}
\]</span><br>
Adagrad算法的一个主要优点是无需手动调整学习率。在大多数的应用场景中，通常采用常数0.01。通过引入二阶动量，对于此前频繁更新过的参数，其二阶动量的对应分量较大，学习率就较小。这一方法在稀疏数据的场景下表现很好。但也<u>存在一些问题</u>：<strong>因为 <span class="math inline">\(\sqrt{v_t}\)</span> 是单调递增的，会使得学习率单调递减至0，可能会使得训练过程提前结束，即便后续还有数据也无法学到必要的知识。</strong></p>
<h4 id="adadelta">AdaDelta</h4>
<p><code>Adadelta</code> 是 <code>Adagrad</code> 的一种扩展算法，以<strong>处理Adagrad学习速率单调递减的问题</strong>。考虑<strong>在计算二阶动量时，不是计算所有的历史梯度平方和，而只关注最近某一时间窗口内的下降梯度，Adadelta将计算历史梯度的窗口大小限制为一个固定值 <span class="math inline">\(w\)</span></strong></p>
<p>在 <code>Adadelta</code> 中，无需存储先前的 <span class="math inline">\(w\)</span> 个平方梯度，而是将梯度的平方递归地表示成所有历史梯度平方的均值。在 <span class="math inline">\(t\)</span> 时刻的均值 <span class="math inline">\(E[g^2]_t\)</span> 只取决于先前的均值和当前的梯度（分量 <span class="math inline">\(γ\)</span> 类似于动量项）：<br>
<span class="math display">\[
E\left[g^{2}\right]_{t}=\gamma E\left[g^{2}\right]_{t-1}+(1-\gamma) g_{t}^{2}
\]</span><br>
我们将 <span class="math inline">\(γ\)</span> 设置成与动量项相似的值，即0.9左右。为了简单起见，我们利用参数更新向量 <span class="math inline">\(Δθ_t\)</span> 重新表示SGD的更新过程：</p>
<p><span class="math display">\[
\begin{array}{c}{\Delta \theta_{t}=-\eta \cdot g_{t, i}} \\ {\theta_{t+1}=\theta_{t}+\Delta \theta_{t}}\end{array}
\]</span><br>
我们先前得到的 <code>Adagrad</code> 参数更新向量变为：<br>
<span class="math display">\[
\Delta \theta_{t}=-\frac{\eta}{\sqrt{v_{t}+\epsilon}} \odot g_{t}
\]</span><br>
现在，我们简单将对角矩阵 <span class="math inline">\(v_t\)</span> 替换成历史梯度的均值 <span class="math inline">\(E[g^2]_t\)</span>：<br>
<span class="math display">\[
\Delta \theta_{t}=-\frac{\eta}{\sqrt{E\left[g^{2}\right]_{t}+\epsilon}} g_{t}
\]</span><br>
由于分母仅仅是梯度的均方根（root mean squared，RMS）误差，我们可以简写为：<br>
<span class="math display">\[
\Delta \theta_{t}=-\frac{\eta}{R M S[g]_{t}} g_{t}
\]</span><br>
作者指出上述更新公式中的每个部分（与SGD，SDG-M 或者Adagrad）并不一致，即更新规则中必须与参数具有相同的假设单位。为了实现这个要求，作者首次定义了另一个指数衰减均值，这次不是梯度平方，而是参数的平方的更新：<br>
<span class="math display">\[
E\left[\Delta \theta^{2}\right]_{t}=\gamma E\left[\Delta \theta^{2}\right]_{t-1}+(1-\gamma) \Delta \theta_{t}^{2}
\]</span><br>
因此，参数更新的均方根误差为：<br>
<span class="math display">\[
R M S[\Delta \theta]_{t}=\sqrt{E\left[\Delta \theta^{2}\right]_{t}+\epsilon}
\]</span><br>
由于 <span class="math inline">\(RMS[Δθ]_t\)</span> 是未知的，我们利用参数的均方根误差来近似更新。利用 <span class="math inline">\(RMS[Δθ]_{t−1}\)</span> 替换先前的更新规则中的学习率 <span class="math inline">\(η\)</span>，最终得到 <code>Adadelta</code> 的更新规则：<br>
<span class="math display">\[
\Delta \theta_{t}=-\frac{R M S[\Delta \theta]_{t-1}}{R M S[g]_{t}} g_{t}
\]</span></p>
<p><span class="math display">\[
\theta_{t+1}=\theta_{t}+\Delta \theta_{t}
\]</span></p>
<p>使用 <code>Adadelta</code> 算法，我们甚至都无需设置默认的学习率，因为更新规则中已经移除了学习率。</p>
<h4 id="rmsprop">RMSprop</h4>
<p><code>RMSprop</code> 是一个未被发表的<strong>自适应学习率的算法</strong>，该算法由Geoff Hinton在其<a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf" target="_blank" rel="noopener">Coursera课堂的课程6e</a>中提出。</p>
<p>RMSprop和Adadelta在相同的时间里被独立的提出，都起源于对Adagrad的极速递减的学习率问题的求解。实际上，<code>RMSprop</code> 是先前我们得到的 <code>Adadelta</code> 的第一个更新向量的特例：<br>
<span class="math display">\[
\begin{array}{l}{E\left[g^{2}\right]_{t}= γ E\left[g^{2}\right]_{t-1}+(1-γ) g_{t}^{2}} \\ {\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{E\left[g^{2}\right]_{t}+\epsilon}} g_{t}}\end{array}
\]</span><br>
同样，<code>RMSprop</code> 将学习率分解成一个平方梯度的指数衰减的平均。Hinton建议将 <span class="math inline">\(γ\)</span> 设置为0.9，对于学习率 <span class="math inline">\(η\)</span>，一个好的固定值为0.001。</p>
<h4 id="adam">Adam</h4>
<p>Adam 结合了Momentum 和 RMSprop 自适应梯度。Adam 中对一阶动量、二阶动量都使用指数衰减平均计算，即融合当前梯度和近一段时间内梯度的平均值，时间久远的梯度对当前平均值的贡献呈指数衰减。(β1， β2为衰减系数)<br>
<span class="math display">\[
m_t = \eta[ \beta_1 m_{t-1} + (1 - \beta_1)g_t ]   \\ 
v_t = \beta_2 v_{t-1} + (1-\beta_2) \cdot \text{diag}(g_t^2)
\]</span><br>
其中，初值</p>
<p><span class="math display">\[
m_0 = 0 \\
v_0 = 0
\]</span><br>
注意到，在迭代初始阶段，<span class="math inline">\(m_t\)</span> 和 <span class="math inline">\(v_t\)</span> 有一个向初值的偏移（过多的偏向了 0）。因此，可以对一阶和二阶动量做偏置校正 (bias correction)， (<a href="https://www.cnblogs.com/34fj/p/8780254.html" target="_blank" rel="noopener">做偏移矫正的原理</a>)<br>
<span class="math display">\[
\hat{m}_t = \frac{m_t}{1-\beta_1^t}
\]</span></p>
<p><span class="math display">\[
\hat{v}_t = \frac{v_t}{1-\beta_2^t}
\]</span><br>
再进行更新，</p>
<p><span class="math display">\[
\theta_{t+1} = \theta_t - \frac{1}{\sqrt{\hat{v}_t} + \epsilon } \hat{m}_t
\]</span><br>
可以保证迭代较为平稳。</p>
<blockquote>
<p>论文中建议参数为：<span class="math inline">\(\beta_1 = 0.9，\beta_2 = 0.999， \epsilon = 10^{-8}\)</span></p>
</blockquote>
<hr>
<h4 id="深度学习为什么不用二阶优化">深度学习为什么不用二阶优化</h4>
<p>目前深度学习中，反向传播主要是依靠一阶梯度。二阶梯度在理论和实际上都是可以应用都网络中的，但相比于一阶梯度，二阶优化会存在以下一些主要问题：</p>
<ul>
<li>计算量大，训练非常慢。<br>
</li>
<li>二阶方法能够更快地求得更高精度的解，这在浅层模型是有益的。而在<strong>神经网络这类深层模型中对参数的精度要求不高</strong>，甚至不高的精度对模型还有益处，能够提高模型的泛化能力。</li>
<li>稳定性。二阶方法能更快求高精度的解，同样对数据本身要的精度也会相应的变高，这就会导致稳定性上的问题。</li>
</ul>
<hr>
<h3 id="预训练与微调fine-tuning">预训练与微调(fine tuning)</h3>
<h4 id="什么是模型微调-fine-tuning">什么是模型微调 fine tuning</h4>
<p>用别人的参数、修改后的网络和自己的数据进行训练，使得参数适应自己的数据，这样一个过程，通常称之为微调（fine tuning).</p>
<h4 id="微调时候网络参数是否更新">微调时候网络参数是否更新</h4>
<p>答案：会更新。finetune 的过程相当于继续训练，跟直接训练的<strong>区别是初始化的时候</strong>。</p>
<ul>
<li>直接训练是按照网络定义指定的方式初始化。</li>
<li>finetune是用你已经有的参数文件来初始化。</li>
</ul>
<h4 id="fine-tuning-模型的三种状态">fine-tuning 模型的三种状态</h4>
<ol style="list-style-type: decimal">
<li>状态一：<strong>只预测，不训练</strong><br>
特点：相对快、简单，针对那些已经训练好，现在要实际对未知数据进行标注的项目，非常高效；</li>
<li>状态二：<strong>训练，但只训练最后分类层</strong><br>
特点：fine-tuning的模型最终的分类以及符合要求，现在只是在他们的基础上进行类别降维。</li>
<li>状态三：<strong>完全训练，分类层+之前卷积层都训练</strong><br>
特点：跟状态二的差异很小，当然状态三比较耗时和需要训练GPU资源，不过非常适合fine-tuning到自己想要的模型里面，预测精度相比状态二也提高不少。</li>
</ol>
<hr>
<h3 id="resnet">ResNet</h3>
<p><strong>目的</strong></p>
<p>解决网络<strong>“退化”</strong>问题，即当模型的层次加深时，错误率却提高的问题，如下图：</p>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/ResNet1.png">

</div>
<p>误差升高的原因是网络越深，梯度消失的现象就越明显，在反向传播的时候，无法有效的把梯度更新到前面的网络层，靠前的网络层参数无法更新，导致训练和测试效果变差。</p>
<p>针对这个问题，作者提出了一个Residual的结构来解决网络的退化问题。</p>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/ResNet2.png">

</div>
<p>残差网络增加了一个identity mapping（恒等映射），把当前输出直接传输给下两层网络（全部是1:1传输，不增加额外的参数），相当于走了一个捷径，跳过了本层运算，这个直接连接命名为“skip connection”，同时在后向传播过程中，也是将下一层网络的梯度直接传递给上层网络，这样就解决了深层网络的梯度消失问题。此时需要学的函数 <span class="math inline">\(H(x)\)</span> 转换成 <span class="math inline">\(F(x)+x\)</span>，即让 ResNet 学习的是残差函数 <span class="math inline">\(F(x) = H(x) - x\)</span>。</p>
<p>输入 <span class="math inline">\(x\)</span> 经过两层的变换得到 <span class="math inline">\(H(x)\)</span> ，然后增加一个 <span class="math inline">\(H(x)=x\)</span> 的层，identity mapping（恒等映射），可以把当前输出直接传递给之后的层（没有增加额外的参数），相当于走了一个捷径，将原始所需要学的函数 <span class="math inline">\(H(x)\)</span> 转换成 <span class="math inline">\(F(x)+x\)</span>，即让 ResNet 学习的是残差函数 <span class="math inline">\(F(x) = H(x) - x\)</span>。</p>
<p>首先考虑两层神经网络的简单叠加（图a），输入 <span class="math inline">\(x\)</span> 经过两个网络层的变换得到 <span class="math inline">\(H(x)\)</span>，激活函数采用ReLu，反向传播时， 梯度将涉及两层参数的交叉相乘， 可能会在离输入近的网络层中产生梯度消失的现象。ResNet残差网络增加了一个 identity mapping(恒等映射)， 既然离输入近的神经网络层较难训练， 那么我们可以将它短接到更靠近输出的层（图b）， 输入 <span class="math inline">\(x\)</span> 经过两个神经网络的变换得到 <span class="math inline">\(H(x)\)</span>， 同时也短接到两层之后， 最后这个包含两层的神经网络模块输出<span class="math inline">\(H(x)=F(x)+x\)</span>。 这样一来， <span class="math inline">\(F(x)\)</span> 被设计为只需要拟合输入 <span class="math inline">\(x\)</span> 与目标输出 <span class="math inline">\(H(x)\)</span> 的残差 <span class="math inline">\(H(x)-x\)</span> ， 残差网络的名称也因此而来。 如果某一层的输出已经较好的拟合了期望结果， 那么多加入一层不会使得模型变得更差， 因为该层的输出将直接被短接到两层之后， 相当于直接学习了一个恒等映射， 而跳过的两层只需要拟合上层输出和目标之间的残差即可。</p>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/ResNet_other.png">

</div>
<p>即增加一个 <span class="math inline">\(H(x)=x\)</span> 的层，identity mapping（恒等映射），将原始所需要学的函数 <span class="math inline">\(H(x)\)</span> 转换成 <span class="math inline">\(F(x)+x\)</span>，即让 ResNet 学习的是残差函数 <span class="math inline">\(F(x) = H(x) - x\)</span>。这里如果 <span class="math inline">\(F(x) = 0\)</span>, 那么就是上面提到的恒等映射。事实上，ResNet是“shortcut connections”的在connections是在恒等映射下的特殊情况，它没有引入额外的参数和计算复杂度。 假如优化目标函数是逼近一个恒等映射，而不是0映射， 那么<strong>学习找到对恒等映射的扰动会比重新学习一个映射函数要容易</strong>。从下图可以看出，残差函数一般会有较小的响应波动，表明恒等映射是一个合理的预处理。</p>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/ResNet3.png">

</div>
<p><strong>分析残差块结构：</strong></p>
<p>它有二层，如下表达式，其中 <span class="math inline">\(σ\)</span> 代表非线性函数 ReLU<br>
<span class="math display">\[
\mathcal{F}=W_{2} \sigma\left(W_{1} \mathbf{x}\right)
\]</span><br>
然后通过一个shortcut，和第2个ReLU，获得输出 <span class="math inline">\(y\)</span><br>
<span class="math display">\[
\mathbf{y}=\mathcal{F}\left(\mathbf{x},\left\{W_{i}\right\}\right)+\mathbf{x}
\]</span><br>
当需要对输入和输出维数进行变化时（如改变通道数目），可以在shortcut时对 <span class="math inline">\(x\)</span> 做一个线性变换<span class="math inline">\(W_s\)</span>，如下式，然而实验证明 <span class="math inline">\(x\)</span> 已经足够了，不需要再搞个维度变换，除非需求是某个特定维度的输出，如文章开头的 ResNet 网络结构图中的虚线，是将通道数翻倍。<br>
<span class="math display">\[
\mathbf{y}=\mathcal{F}\left(\mathbf{x},\left\{W_{i}\right\}\right)+W_{s} \mathbf{x}
\]</span><br>
实验证明，这个残差块往往需要两层以上，单单一层的残差块 <span class="math inline">\((y=W_1x+x)\)</span> 并不能起到提升作用。<br>
残差网络的确解决了退化的问题，在训练集和校验集上，都证明了的更深的网络错误率越小，如下图</p>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/ResNet4.png">

</div>
<p>实际中，考虑计算的成本，对残差块做了计算优化，即将两个 3x3的卷积层替换为1x1 + 3x3 + 1x1, 如下图。新结构中的中间 3x3 的卷积层首先在一个降维1x1卷积层下减少了计算，然后在另一个1x1的卷积层下做了还原，既保持了精度又减少了计算量。</p>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/ResNet5.png">

</div>
<p><strong>Reference</strong></p>
<ul>
<li><a href="https://blog.csdn.net/wspba/article/details/56019373" class="uri" target="_blank" rel="noopener">https://blog.csdn.net/wspba/article/details/56019373</a></li>
<li><a href="https://blog.csdn.net/mao_feng/article/details/52734438" class="uri" target="_blank" rel="noopener">https://blog.csdn.net/mao_feng/article/details/52734438</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/31852747" class="uri" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/31852747</a></li>
<li><a href="https://blog.csdn.net/lanran2/article/details/79057994" class="uri" target="_blank" rel="noopener">https://blog.csdn.net/lanran2/article/details/79057994</a></li>
</ul>
<hr>
<h3 id="caffe">Caffe</h3>
<h4 id="caffe卷积层的实现">Caffe卷积层的实现</h4>
<p>Caffe的卷积层实现，使用 <code>im2col</code> 操作，将数据以及卷积核分别转换成新的矩阵，然后将两对矩阵进行内积运算（inner product)。这样做，比原始的卷积操作速度更快。</p>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/caffe_conv_02.jpg">

</div>
<p>其中 <code>im2col</code> : 将一个大矩阵，重叠地划分为多个子矩阵，对每个子矩阵序列化成向量，最后得到另外一个矩阵。</p>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/caffe_conv_01.png">

</div>
<hr>
<h4 id="caffe-结构">Caffe 结构</h4>
<p>Caffe’s Abstract Framework:</p>
<ul>
<li><strong>Blob：</strong>是Caffe中 <strong>数据传输的媒介</strong>，相当于一个 N 维数组。网络中的输入数据、权重参数等，都是转化为Blob数据结构来存储。而实际上，它们只是一维的指针而已，其4维结构通过shape属性得以计算出来。</li>
</ul>
<p>Blob中重要的函数和成员有：</p>
<ul>
<li><p><strong>data_</strong>：数据</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取 cpu 数据的方法</span></span><br><span class="line"><span class="keyword">const</span> Dtype* bottom_data = bottom[<span class="number">0</span>]-&gt;cpu_data(); <span class="comment">// 获得输入的blob指针 （只读，不能改变数据内容）</span></span><br><span class="line">Dtype* top_data = top[<span class="number">0</span>]-&gt;mutable_cpu_data();	<span class="comment">// 获得输出blob的data指针 （读写，可以改变数据内容）</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取 gpu 数据的方法</span></span><br><span class="line"><span class="keyword">const</span> Dtype* bottom_data = bottom[<span class="number">0</span>]-&gt;gpu_data();	<span class="comment">// 只读</span></span><br><span class="line">Dtype* top_data = top[<span class="number">0</span>]-&gt;mutable_gpu_data();		<span class="comment">// 读写</span></span><br></pre></td></tr></table></figure></li>
<li><p><strong>diff_</strong> ：梯度</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> Dtype* top_diff = top[<span class="number">0</span>]-&gt;cpu_diff();			<span class="comment">// 只读</span></span><br><span class="line">Dtype* bottom_diff = bottom[<span class="number">0</span>]-&gt;mutable_cpu_diff();	<span class="comment">// 读写</span></span><br></pre></td></tr></table></figure></li>
<li><p><strong>Reshape() </strong>：重新修改Blob的形状(4维)，并根据形状来申请动态内存存储数据和梯度。</p></li>
<li><p><strong>count()</strong>：计算Blob所需要的基本数据单元的数量。</p></li>
<li><p><strong>Layer</strong>：作为<strong>网络的基础单元</strong>，层与层之间的数据节点、前向传递，后向传递都在该数据结构中被实现。层的种类有：卷积层、池化层、激活层、全连接层，Data层等，还可以自己添加所需要的层。</p></li>
</ul>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/Layer.png">

</div>
<figure class="highlight coq"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">从<span class="built_in">bottom</span>进行数据的输入 ，计算后，通过<span class="built_in">top</span>进行输出。图中的黄色多边形表示输入输出的数据，蓝色矩形表示层。</span><br><span class="line">每一种类型的层都定义了三种关键的计算：setup,forward and backword</span><br><span class="line">（<span class="number">1</span>）setup: 层的建立和初始化，以及在整个模型中的连接初始化。</span><br><span class="line">（<span class="number">2</span>）forward: 从<span class="built_in">bottom</span>得到输入数据，进行计算，并将计算结果送到<span class="built_in">top</span>，进行输出。</span><br><span class="line">（<span class="number">3</span>）backward: 从层的输出端<span class="built_in">top</span>得到数据的梯度，计算当前层的梯度，并将计算结果送到<span class="built_in">bottom</span>,向前传递。</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>Net：</strong>则是由多个layer组合而成。作为<strong>网络的整体骨架，</strong>决定了网络中的层次数目以及各个层的类别等信息。</li>
<li><strong>Solver：</strong>作为<strong>网络的求解策略，</strong>主要包括使用何种方法进行优化，比如随机梯度下降还是，自适应梯度下降等，以及学习率等一些配置等。</li>
</ul>
<p>=============================================================================</p>
<p><strong><a href="https://blog.csdn.net/tangwei2014/article/details/46815231" target="_blank" rel="noopener">添加新的一层</a></strong></p>
<p><strong>1）</strong>在<code>./src/caffe/proto/caffe.proto</code> 中增加对应 layer 的 <code>paramter message</code>；</p>
<p><strong>2）</strong>在 <code>./include/caffe/***layers.hpp</code>中增加该 layer 的类的声明，***表示有<code>common_layers.hpp</code> ， <code>data_layers.hpp</code>， <code>neuron_layers.hpp</code>， <code>vision_layers.hpp</code> 和 <code>loss_layers.hpp</code> 等；</p>
<p><strong>3）</strong>在 <code>./src/caffe/layers/</code> 目录下新建 .cpp和.cu文件，进行类实现。</p>
<p><strong>4）</strong>在 <code>./src/caffe/gtest/</code> 中增加 layer 的测试代码，对所写的 layer 前传和反传进行测试，测试还包括速度。</p>
<blockquote>
<p><strong><code>relu_layer.cpp</code></strong></p>
</blockquote>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"caffe/layers/relu_layer.hpp"</span></span></span><br><span class="line"><span class="keyword">namespace</span> caffe &#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 前向传递</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">void</span> ReLULayer&lt;Dtype&gt;::Forward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span><br><span class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123;</span><br><span class="line">  <span class="comment">// (只读)获得输入blob的data指针</span></span><br><span class="line">  <span class="keyword">const</span> Dtype* bottom_data = bottom[<span class="number">0</span>]-&gt;cpu_data();	</span><br><span class="line">  <span class="comment">// （读写）获得输出 blob 的 data 指针</span></span><br><span class="line">  Dtype* top_data = top[<span class="number">0</span>]-&gt;mutable_cpu_data(); </span><br><span class="line">  <span class="comment">// 获得输入 blob 元素个数</span></span><br><span class="line">  <span class="keyword">const</span> <span class="keyword">int</span> count = bottom[<span class="number">0</span>]-&gt;count();</span><br><span class="line">  <span class="comment">// Leaky ReLU参数，从layer_param_ 中获得，默认为0，即普通ReLU</span></span><br><span class="line">  Dtype negative_slope = <span class="keyword">this</span>-&gt;layer_param_.relu_param().negative_slope();</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; count; ++i) &#123;</span><br><span class="line">    <span class="comment">// 执行 ReLU 操作</span></span><br><span class="line">    top_data[i] = <span class="built_in">std</span>::max(bottom_data[i], Dtype(<span class="number">0</span>))</span><br><span class="line">        + negative_slope * <span class="built_in">std</span>::min(bottom_data[i], Dtype(<span class="number">0</span>));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 反向传播</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">void</span> ReLULayer&lt;Dtype&gt;::Backward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</span><br><span class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down,</span><br><span class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) &#123;</span><br><span class="line">  <span class="comment">// 如果需要做反向传播计算</span></span><br><span class="line">  <span class="keyword">if</span> (propagate_down[<span class="number">0</span>]) &#123;</span><br><span class="line">    <span class="comment">// （只读）获得前一层的data指针</span></span><br><span class="line">    <span class="keyword">const</span> Dtype* bottom_data = bottom[<span class="number">0</span>]-&gt;cpu_data();</span><br><span class="line">    <span class="comment">// （只读）获得后一层的diff指针</span></span><br><span class="line">    <span class="keyword">const</span> Dtype* top_diff = top[<span class="number">0</span>]-&gt;cpu_diff();</span><br><span class="line">    <span class="comment">// （读写）获得前一层的diff指针</span></span><br><span class="line">    Dtype* bottom_diff = bottom[<span class="number">0</span>]-&gt;mutable_cpu_diff();</span><br><span class="line">    <span class="comment">// 获得需要参与计算的元素总数</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> count = bottom[<span class="number">0</span>]-&gt;count();</span><br><span class="line">    <span class="comment">// Leaky ReLU参数，姑且认为是0</span></span><br><span class="line">    Dtype negative_slope = <span class="keyword">this</span>-&gt;layer_param_.relu_param().negative_slope();</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; count; ++i) &#123;</span><br><span class="line">      <span class="comment">// ReLU的导函数就是 （bottom_data[i] &gt; 0），根据求导链式法则，后一层的误差乘以导函数的误差</span></span><br><span class="line">      bottom_diff[i] = top_diff[i] * ((bottom_data[i] &gt; <span class="number">0</span>)</span><br><span class="line">          + negative_slope * (bottom_data[i] &lt;= <span class="number">0</span>));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> CPU_ONLY</span></span><br><span class="line">STUB_GPU(ReLULayer);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"></span><br><span class="line">INSTANTIATE_CLASS(ReLULayer);</span><br><span class="line">&#125;  <span class="comment">// namespace caffe</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>《深度学习21天实战Caffe》P94 中有讲一点源码</p>
</blockquote>
<p>=============================================================================</p>
<p><strong>1. Blob：</strong></p>
<p><strong>1.1. Blob的类型描述</strong></p>
<p>Caffe内部采用的数据类型主要是对protocol buffer所定义的数据结构的继承，因此可以在尽可能小的内存占用下获得很高的效率，虽然追求性能的同时Caffe也会牺牲了一些代码可读性。<br>
直观来说，可以把Blob看成一个有4维的结构体（包含数据和梯度），而实际上，它们只是一维的指针而已，其4维结构通过shape属性得以计算出来。</p>
<p><strong>1.2. Blob的重要成员函数和变量</strong></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">shared_ptr</span>&lt;SyncedMemory&gt; data_ <span class="comment">//数据</span></span><br><span class="line"><span class="built_in">shared_ptr</span>&lt;SyncedMemory&gt; diff_ <span class="comment">//梯度</span></span><br></pre></td></tr></table></figure>
<p>重新修改Blob的形状(4维)，并根据形状来申请动态内存存储数据和梯度。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::Reshape(<span class="keyword">const</span> <span class="keyword">int</span> num, <span class="keyword">const</span> <span class="keyword">int</span> channels, <span class="keyword">const</span> <span class="keyword">int</span> height,</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> width)</span><br></pre></td></tr></table></figure>
<p>计算Blob所需要的基本数据单元的数量。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">count</span><span class="params">(<span class="keyword">int</span> start_axis, <span class="keyword">int</span> end_axis)</span> <span class="keyword">const</span></span></span><br></pre></td></tr></table></figure>
<p><strong>2. Layer：</strong></p>
<p><strong>2.1. Layer的类型描述</strong></p>
<p>Layer是网络模型和计算的核心，在数据存储上，主要分成bottom_vecs、top_vecs、weights&amp;bias三个部分；在数据传递上，也主要分为LayerSetUp、Reshape、Forward、Backward四个过程，符合直观上对层与层之间连接的理解，贴切自然。</p>
<p><strong>2.2. Layer的重要成员函数和变量</strong></p>
<p>通过bottom Blob对象的形状以及LayerParameter(从prototxt读入)来确定Layer的学习参数（以Blob类型存储）的形状。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;Dtype&gt; loss_ <span class="comment">//每一层都会有一个loss值，但只有LossLayer才会产生非0的loss</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">shared_ptr</span>&lt;Blob&lt;Dtype&gt; &gt; &gt; blobs_ <span class="comment">//Layer所学习的参数，包括权值和偏差</span></span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">LayerSetUp</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span></span></span><br><span class="line"><span class="function"><span class="params">      <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)</span></span></span><br></pre></td></tr></table></figure>
<p>通过bottom Blob对象的形状以及Layer的学习参数的形状来确定top Blob对象的形状。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Reshape</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span></span></span><br><span class="line"><span class="function"><span class="params">      <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)</span></span></span><br></pre></td></tr></table></figure>
<p>Layer内部数据正向传播，从bottom到top方向。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Forward</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt; &amp;bottom,                     </span></span></span><br><span class="line"><span class="function"><span class="params">                     <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt; *top)</span> </span>= <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>Layer内部梯度反向传播，从top到bottom方向。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Backward</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt; &amp;top,</span></span></span><br><span class="line"><span class="function"><span class="params">                      <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt; &amp;propagate_down, </span></span></span><br><span class="line"><span class="function"><span class="params">                      <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt; *bottom)</span> </span>= <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p><strong>3. Net：</strong></p>
<p><strong>3.1. Net的类型描述</strong></p>
<p>Net用容器的形式将多个Layer有序地放在一起，其自身实现的功能主要是对逐层Layer进行初始化，以及提供Update( )的接口（更新网络参数），本身不能对参数进行有效地学习过程。</p>
<p><strong>3.2. Net的重要成员函数和变量</strong></p>
<p>根据NetParameter进行net初始化,简单的来说就是先把网络中所有层的bottom Blobs&amp;top Blobs（无重复）实例化，并从输入层开始，逐层地进行Setup的工作，从而完成了整个网络的搭建，为后面的数据前后传输打下基础。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">shared_ptr</span>&lt;Layer&lt;Dtype&gt; &gt; &gt; layers_ <span class="comment">//构成该net的layers</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt; &gt; bottom_vecs_ <span class="comment">//每一层layer中的bottom Blobs</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt; &gt; top_vecs_ <span class="comment">//每一层layer中的top Blobs</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">shared_ptr</span>&lt;Blob&lt;Dtype&gt; &gt; &gt; params_ <span class="comment">//整个net中的learnable parameter</span></span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Init</span><span class="params">(<span class="keyword">const</span> NetParameter&amp; param)</span></span></span><br></pre></td></tr></table></figure>
<p>是对整个网络的前向和方向传导，各调用一次就可以计算出网络的loss了。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; Forward(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;* &gt; &amp; bottom,</span><br><span class="line">                              Dtype* loss = <span class="literal">NULL</span>)</span><br><span class="line"><span class="keyword">void</span> Net&lt;Dtype&gt;::Backward()</span><br></pre></td></tr></table></figure>
<p><strong>4. Solver</strong></p>
<p><strong>4.1. Solver的类型描述</strong></p>
<p>Solver类中包含一个Net的指针，主要是实现了训练模型参数所采用的优化算法，根据优化算法的不同会派生不同的类，而基于这些子类就可以对网络进行正常的训练过程。</p>
<p><strong>4.2. Solver的重要成员函数和变量</strong></p>
<p>对已初始化后的网络进行固定次数的训练迭代过程。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">shared_ptr</span>&lt;Net&lt;Dtype&gt; &gt; net_ <span class="comment">//net对象</span></span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Step</span><span class="params">(<span class="keyword">int</span> iters)</span></span></span><br></pre></td></tr></table></figure>
<p>对已初始化后的网络进行固定次数的训练迭代过程。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ComputeUpdateValue();</span><br><span class="line">net_-&gt;Update();</span><br></pre></td></tr></table></figure>
<h4 id="cuda编程">CUDA编程 <a href="https://zhuanlan.zhihu.com/p/34587739" target="_blank" rel="noopener">*</a></h4>
<p><strong>1. 概念</strong></p>
<p>CUDA编程可以利用GPUs的并行计算引擎来更加高效地解决比较复杂的计算难题。在CUDA中，有两个比较重要的概念，host 和 device。</p>
<ul>
<li>host：指代CPU及其内存</li>
<li>device：指代GPU及其内存</li>
</ul>
<p>CUDA程序中既包含host程序，又包含device程序，它们分别在CPU和GPU上运行。同时，host与device之间可以进行通信，这样它们之间可以进行数据拷贝。典型的CUDA程序的执行流程如下：</p>
<ol style="list-style-type: decimal">
<li>分配host内存，并进行数据初始化；</li>
<li>分配device内存，并从host将数据拷贝到device上；</li>
<li>调用CUDA的核函数在device上完成指定的运算；</li>
<li>将device上的运算结果拷贝到host上；</li>
<li>释放device和host上分配的内存。</li>
</ol>
<p>上面流程中最重要的一个过程是调用CUDA的核函数来执行并行计算，<strong>kernel</strong> 是在device上线程中并行执行的函数，核函数用<code>__global__</code>符号声明，在调用时需要用<code>&lt;&lt;&lt;grid, block&gt;&gt;&gt;</code>来指定kernel要执行的线程数量，在CUDA中，每一个线程都要执行核函数，并且每个线程会分配一个唯一的线程号thread ID，这个ID值可以通过核函数的内置变量<code>threadIdx</code>来获得。</p>
<p>GPU上很多并行化的轻量级线程。kernel在device上执行时实际上是启动很多线程，一个kernel所启动的所有线程称为一个<strong>网格</strong>（grid），同一个网格上的线程共享相同的全局内存空间，grid是线程结构的第一层次，而网格又可以分为很多<strong>线程块</strong>（block），一个线程块里面包含很多线程，这是第二个层次。线程两层组织结构如下图所示，这是一个gird和block均为2-dim的线程组织。grid和block都是定义为<code>dim3</code>类型的变量，<code>dim3</code>可以看成是包含三个无符号整数（x，y，z）成员的结构体变量，在定义时，缺省值初始化为1。因此grid和block可以灵活地定义为1-dim，2-dim以及3-dim结构，对于图中结构（主要水平方向为x轴），定义的grid和block如下所示，kernel在调用时也必须通过<a href="https://link.zhihu.com/?target=http%3A//docs.nvidia.com/cuda/cuda-c-programming-guide/index.html%23execution-configuration" target="_blank" rel="noopener">执行配置</a><code>&lt;&lt;&lt;grid, block&gt;&gt;&gt;</code>来指定kernel所使用的线程数及结构。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">dim3 <span class="title">grid</span><span class="params">(<span class="number">3</span>, <span class="number">2</span>)</span></span>;</span><br><span class="line"><span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">5</span>, <span class="number">3</span>)</span></span>;</span><br><span class="line">kernel_fun&lt;&lt;&lt; grid, block &gt;&gt;&gt;(prams...);</span><br></pre></td></tr></table></figure>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/CUDA架构_01.jpg">

</div>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/CUDA架构_02.jpg">

</div>
<p><strong>2. 实战</strong></p>
<p><strong>（1）向量加法</strong></p>
<p>实现一个向量加法的实例，这里grid和block都设计为1-dim，首先定义kernel如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 两个向量加法kernel，grid和block均为一维</span></span><br><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">add</span><span class="params">(<span class="keyword">float</span>* x, <span class="keyword">float</span> * y, <span class="keyword">float</span>* z, <span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// 获取全局索引</span></span><br><span class="line">    <span class="keyword">int</span> index = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">    <span class="comment">// 步长</span></span><br><span class="line">    <span class="keyword">int</span> stride = blockDim.x * gridDim.x;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = index; i &lt; n; i += stride)</span><br><span class="line">    &#123;</span><br><span class="line">        z[i] = x[i] + y[i];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其中stride是整个grid的线程数，有时候向量的元素数很多，这时候可以将在每个线程实现多个元素（元素总数/线程总数）的加法，相当于使用了多个grid来处理，这是一种<a href="https://link.zhihu.com/?target=https%3A//devblogs.nvidia.com/cuda-pro-tip-write-flexible-kernels-grid-stride-loops/" target="_blank" rel="noopener">grid-stride loop</a>方式，不过下面的例子一个线程只处理一个元素，所以kernel里面的循环是不执行的。下面我们具体实现向量加法：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> N = <span class="number">1</span> &lt;&lt; <span class="number">20</span>;</span><br><span class="line">    <span class="keyword">int</span> nBytes = N * <span class="keyword">sizeof</span>(<span class="keyword">float</span>);</span><br><span class="line">    <span class="comment">// 申请host内存</span></span><br><span class="line">    <span class="keyword">float</span> *x, *y, *z;</span><br><span class="line">    x = (<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">    y = (<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">    z = (<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化数据</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        x[i] = <span class="number">10.0</span>;</span><br><span class="line">        y[i] = <span class="number">20.0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 申请device内存</span></span><br><span class="line">    <span class="keyword">float</span> *d_x, *d_y, *d_z;</span><br><span class="line">    cudaMalloc((<span class="keyword">void</span>**)&amp;d_x, nBytes);</span><br><span class="line">    cudaMalloc((<span class="keyword">void</span>**)&amp;d_y, nBytes);</span><br><span class="line">    cudaMalloc((<span class="keyword">void</span>**)&amp;d_z, nBytes);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将host数据拷贝到device</span></span><br><span class="line">    cudaMemcpy((<span class="keyword">void</span>*)d_x, (<span class="keyword">void</span>*)x, nBytes, cudaMemcpyHostToDevice);</span><br><span class="line">    cudaMemcpy((<span class="keyword">void</span>*)d_y, (<span class="keyword">void</span>*)y, nBytes, cudaMemcpyHostToDevice);</span><br><span class="line">    <span class="comment">// 定义kernel的执行配置</span></span><br><span class="line">    <span class="function">dim3 <span class="title">blockSize</span><span class="params">(<span class="number">256</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">gridSize</span><span class="params">((N + blockSize.x - <span class="number">1</span>) / blockSize.x)</span></span>;</span><br><span class="line">    <span class="comment">// 执行kernel</span></span><br><span class="line">    add &lt;&lt; &lt; gridSize, blockSize &gt;&gt; &gt;(d_x, d_y, d_z, N);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将device得到的结果拷贝到host</span></span><br><span class="line">    cudaMemcpy((<span class="keyword">void</span>*)z, (<span class="keyword">void</span>*)d_z, nBytes, cudaMemcpyHostToDevice);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 检查执行结果</span></span><br><span class="line">    <span class="keyword">float</span> maxError = <span class="number">0.0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N; i++)</span><br><span class="line">        maxError = fmax(maxError, <span class="built_in">fabs</span>(z[i] - <span class="number">30.0</span>));</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"最大误差: "</span> &lt;&lt; maxError &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 释放device内存</span></span><br><span class="line">    cudaFree(d_x);</span><br><span class="line">    cudaFree(d_y);</span><br><span class="line">    cudaFree(d_z);</span><br><span class="line">    <span class="comment">// 释放host内存</span></span><br><span class="line">    <span class="built_in">free</span>(x);</span><br><span class="line">    <span class="built_in">free</span>(y);</span><br><span class="line">    <span class="built_in">free</span>(z);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里我们的向量大小为1&lt;&lt;20，而block大小为256，那么grid大小是4096，kernel的线程层级结构如下图所示：</p>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/Kernel线程层级结构.jpg">

</div>
<p><strong>（2）矩阵乘法</strong></p>
<p>最后我们再实现一个稍微复杂一些的例子，就是两个矩阵的乘法，设输入矩阵为 A 和 B ，要得到 C = A x B 。实现思路是每个线程计算 C 的一个元素值 <span class="math inline">\(C_{i, j}\)</span> ，对于矩阵运算，应该选用 grid 和block为2-D的。首先定义矩阵的结构体：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 矩阵类型，行优先，M(row, col) = *(M.elements + row * M.width + col)</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Matrix</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="keyword">int</span> width;</span><br><span class="line">    <span class="keyword">int</span> height;</span><br><span class="line">    <span class="keyword">float</span> *elements;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>然后实现矩阵乘法的核函数，这里我们定义了两个辅助的<code>__device__</code>函数分别用于获取矩阵的元素值和为矩阵元素赋值，具体代码如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取矩阵A的(row, col)元素</span></span><br><span class="line">__<span class="function">device__ <span class="keyword">float</span> <span class="title">getElement</span><span class="params">(Matrix *A, <span class="keyword">int</span> row, <span class="keyword">int</span> col)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">return</span> A-&gt;elements[row * A-&gt;width + col];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 为矩阵A的(row, col)元素赋值</span></span><br><span class="line">__<span class="function">device__ <span class="keyword">void</span> <span class="title">setElement</span><span class="params">(Matrix *A, <span class="keyword">int</span> row, <span class="keyword">int</span> col, <span class="keyword">float</span> value)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	A-&gt;elements[row * A-&gt;width + col] = value;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 矩阵相乘kernel，2-D，每个线程计算一个元素</span></span><br><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">matMulKernel</span><span class="params">(Matrix *A, Matrix *B, Matrix *C)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">float</span> Cvalue = <span class="number">0.0</span>;</span><br><span class="line">	<span class="keyword">int</span> row = threadIdx.y + blockIdx.y * blockDim.y;</span><br><span class="line">	<span class="keyword">int</span> col = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; A-&gt;width; ++i)</span><br><span class="line">	&#123;</span><br><span class="line">		Cvalue += getElement(A, row, i) * getElement(B, i, col);</span><br><span class="line">	&#125;</span><br><span class="line">	setElement(C, row, col, Cvalue);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>最后我们采用统一内存编写矩阵相乘的测试实例：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> width = <span class="number">1</span> &lt;&lt; <span class="number">10</span>;</span><br><span class="line">    <span class="keyword">int</span> height = <span class="number">1</span> &lt;&lt; <span class="number">10</span>;</span><br><span class="line">    Matrix *A, *B, *C;</span><br><span class="line">    <span class="comment">// 申请托管内存</span></span><br><span class="line">    cudaMallocManaged((<span class="keyword">void</span>**)&amp;A, <span class="keyword">sizeof</span>(Matrix));</span><br><span class="line">    cudaMallocManaged((<span class="keyword">void</span>**)&amp;B, <span class="keyword">sizeof</span>(Matrix));</span><br><span class="line">    cudaMallocManaged((<span class="keyword">void</span>**)&amp;C, <span class="keyword">sizeof</span>(Matrix));</span><br><span class="line">    <span class="keyword">int</span> nBytes = width * height * <span class="keyword">sizeof</span>(<span class="keyword">float</span>);</span><br><span class="line">    cudaMallocManaged((<span class="keyword">void</span>**)&amp;A-&gt;elements, nBytes);</span><br><span class="line">    cudaMallocManaged((<span class="keyword">void</span>**)&amp;B-&gt;elements, nBytes);</span><br><span class="line">    cudaMallocManaged((<span class="keyword">void</span>**)&amp;C-&gt;elements, nBytes);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化数据</span></span><br><span class="line">    A-&gt;height = height;</span><br><span class="line">    A-&gt;width = width;</span><br><span class="line">    B-&gt;height = height;</span><br><span class="line">    B-&gt;width = width;</span><br><span class="line">    C-&gt;height = height;</span><br><span class="line">    C-&gt;width = width;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; width * height; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        A-&gt;elements[i] = <span class="number">1.0</span>;</span><br><span class="line">        B-&gt;elements[i] = <span class="number">2.0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 定义kernel的执行配置</span></span><br><span class="line">    <span class="function">dim3 <span class="title">blockSize</span><span class="params">(<span class="number">32</span>, <span class="number">32</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">gridSize</span><span class="params">((width + blockSize.x - <span class="number">1</span>) / blockSize.x, </span></span></span><br><span class="line"><span class="function"><span class="params">        (height + blockSize.y - <span class="number">1</span>) / blockSize.y)</span></span>;</span><br><span class="line">    <span class="comment">// 执行kernel</span></span><br><span class="line">    matMulKernel &lt;&lt; &lt; gridSize, blockSize &gt;&gt; &gt;(A, B, C);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 同步device 保证结果能正确访问</span></span><br><span class="line">    cudaDeviceSynchronize();</span><br><span class="line">    <span class="comment">// 检查执行结果</span></span><br><span class="line">    <span class="keyword">float</span> maxError = <span class="number">0.0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; width * height; ++i)</span><br><span class="line">        maxError = fmax(maxError, <span class="built_in">fabs</span>(C-&gt;elements[i] - <span class="number">2</span> * width));</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"最大误差: "</span> &lt;&lt; maxError &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="常考问题">*常考问题</h3>
<h4 id="全连接层作用">全连接层作用</h4>
<p><strong>(1)</strong> <strong>全连接层</strong>：在整个卷积神经网络中起到“分类器”的作用。如果说卷积层、池化层和激活函数层等操作是将原始数据映射到隐层特征空间的话，全连接层则起到<strong>将学到的“分布式特征表示”映射到样本标记空间的作用。</strong></p>
<p><strong>(2)</strong> 全连接是个矩阵乘法，相当于一个特征空间变换，可以把有用的信息提取整合。再加上激活函数的非线性映射，多层全连接层理论上可以模拟任何非线性变换。</p>
<p><strong>(3)</strong> 全连接还有一个作用是维度变换，尤其是可以把高维变到低维，同时把有用的信息保留下来。</p>
<p>但全连接层的缺点也很明显：</p>
<ul>
<li>参数数量太大</li>
<li>无法保持空间结构（没有利用像素之间的位置信息）</li>
<li>网络层数限制</li>
</ul>
<blockquote>
<p>N个节点的全连接可近似为N个模板卷积后的均值池化(GAP)<br>
不同channel同一位置上的全连接等价与1x1的卷积</p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">以VGG<span class="number">-16</span>为例，对<span class="number">224</span>x224x3的输入，最后一层卷积可得输出为<span class="number">7</span>x7x512，如后层是一层含<span class="number">4096</span>个神经元的FC，则可用卷积核为<span class="number">7</span>x7x512x4096的全局卷积来实现这一全连接运算过程，其中该卷积核参数如下：“filter size = <span class="number">7</span>, padding = <span class="number">0</span>, stride = <span class="number">1</span>, D_in = <span class="number">512</span>, D_out = <span class="number">4096</span>”经过此卷积操作后可得输出为<span class="number">1</span>x1x4096</span><br><span class="line"></span><br><span class="line">如需再次叠加一个<span class="number">2048</span>的FC，则可设定参数为“filter size = <span class="number">1</span>, padding = <span class="number">0</span>, stride = <span class="number">1</span>, D_in = <span class="number">4096</span>, D_out = <span class="number">2048</span>”的卷积层操作。</span><br></pre></td></tr></table></figure>
<h4 id="卷积的意义与作用">卷积的意义与作用</h4>
<p><strong>卷积层</strong>：可以看做是全连接的一种简化形式：通过 局部连接 + 权值共享，大大减少了参数并且使得训练变得可控，同时还保留了空间位置信息。通过使用多个filter来提取图片的不同特征，再加上Pooling 层下采样，进一步减少参数数量，同时还可以提升模型的鲁棒性。</p>
<blockquote>
<p><a href="https://www.zhihu.com/question/22298352" target="_blank" rel="noopener">数学、物理角度理解卷积</a></p>
</blockquote>
<h4 id="卷积层参数计算量flops">卷积层参数、计算量(FLOPs)</h4>
<blockquote>
<p><strong>深度学习框架FLOPs</strong>： Floating point operations，浮点运算数量。</p>
</blockquote>
<p>输入特征尺寸为：<span class="math inline">\(H_1 \times W_1 \times C_{in}\)</span></p>
<p>卷积核尺寸为：<span class="math inline">\(K \times K \times C_{in} \times C_{out}\)</span></p>
<p>输出特征尺寸为：<span class="math inline">\(H_2 \times W_2 \times C_{out}\)</span></p>
<blockquote>
<p>Feature map 计算公式：<span class="math inline">\(H_2 = \frac{H_1 - K + 2P}{S} + 1\)</span></p>
</blockquote>
<ul>
<li>卷积层参数计算 <strong>Parameters</strong></li>
<li><span class="math inline">\((K \times K \times C_{in}) \times C_{out}\)</span> (不考虑 <span class="math inline">\(bias\)</span>)</li>
<li><span class="math inline">\((K \times K \times C_{in} + 1) \times C_{out}\)</span> (考虑 <span class="math inline">\(bias\)</span>)</li>
<li>卷积计算量 <strong>FLOPs</strong></li>
<li><span class="math inline">\((K \times K \times C_{in}) \times C_{out} \times W_2 \times H_2\)</span> (不考虑 <span class="math inline">\(bias\)</span>)</li>
</ul>
<blockquote>
<p>Reference: <a href="https://zhuanlan.zhihu.com/p/31575074" target="_blank" rel="noopener">卷积神经网络的复杂度分析</a></p>
</blockquote>
<h4 id="d卷积">2D卷积</h4>
<p>平时用的卷积就是2D卷积，即滤波器深度与输入层深度一样。</p>
<p><img src="/2018/04/07/Try-your-best!You-will-find-the-job/2D卷积.jpg" width="30%" height="200px" alt="2D卷积" align="center"></p>
<h4 id="d-卷积">3D 卷积</h4>
<p>滤波器的深度小于输入层深度（核大小<通道大小）。因此，3d 1 过滤器可以在所有三个方向（图像的高度、宽度、通道）上移动。在每个位置，逐元素的乘法和加法都会提供一个数值。因为过滤器是滑过一个 3d 空间，所以输出数值也按 空间排布。也就是说输出是一个 数据。3d 卷积可以描述 空间中目标的空间关系。对某些应用（比如生物医学影像中的 分割 重构）而言，这样的 关系很重要，比如在 ct 和 mri 中，血管之类的目标会在 空间中蜿蜒曲折。 ![3d卷积](加油！一定能找到好工作！ 3d卷积.png) #### x 卷积 进行 **升维** **降维** 的作用，也就是通过控制卷积核（通道数）实现。这个可以帮助减少模型参数，也可以对不同特征进行尺寸的归一化；同时也可以用于不同channel上特征的融合。如，rpn网络中，网络输出，接入了两个并行的1x1的卷积层，来回归anchor的所属类别以及需要做的平移缩放参数。conv5_3 输出[38x50x512]，输入到cls_layer 得到 [38x50x18]，以及 loc_layer [38x50x4]. ![1*1卷积](加油！一定能找到好工作！ 1*1卷积.png) 转置卷积（反卷积、去卷积）> <strong>反卷积</strong>：是一种特殊的正向卷积，先按照一定的比例通过补 0 来扩大输入图像的尺寸，接着将卷积核进行转置，再进行正向卷积。<br>
&gt; 作用：实现图像由小分辨率到到大分辨率的映射操作，其在在 <strong>FCN 语义分割网络</strong>中有应用。<a href="https://zhuanlan.zhihu.com/p/48501100" target="_blank" rel="noopener">1</a> <a href="https://zhuanlan.zhihu.com/p/57575810" target="_blank" rel="noopener">2</a> <a href="https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215" target="_blank" rel="noopener">3</a></通道大小）。因此，3d></p>
<blockquote>
<p><strong>上采样</strong>： 由于输入图像通过卷积神经网络(CNN)提取特征后，输出的尺寸往往会变小，而有时我们需要将图像恢复到原来的尺寸以便进行进一步的计算(e.g.:图像的语义分割)，这个采用扩大图像尺寸，实现图像由小分辨率到大分辨率的映射的操作，叫做上采样(Upsample)<br>
上采样有3种常见的方法：</p>
<ul>
<li>双线性插值(bilinear)</li>
<li>反卷积(Transposed Convolution)</li>
<li>反池化(Unpooling)</li>
</ul>
</blockquote>
<p><strong>1. 解释：</strong></p>
<p>在卷积中，我们定义 C 为卷积核，Large 为输入图像，Small 为输出图像。经过卷积（矩阵乘法）后，我们将大图像下采样为小图像。这种矩阵乘法的卷积的实现遵照：C x Large = Small。</p>
<p>下面的例子展示了这种运算的工作方式。它将输入平展为 16×1 的矩阵，并将卷积核转换为一个稀疏矩阵（4×16）。然后，在稀疏矩阵和平展的输入之间使用矩阵乘法。之后，再将所得到的矩阵（4×1）转换为 2×2 的输出。</p>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/转置卷积3.jpeg" alt="卷积的矩阵乘法：将 Large 输入图像（4×4）转换为 Small 输出图像（2×2）">
<p class="caption">卷积的矩阵乘法：将 Large 输入图像（4×4）转换为 Small 输出图像（2×2）</p>
</div>
<p>现在，如果我们在等式的两边都乘上矩阵的转置 <span class="math inline">\(C^T\)</span>，并借助「一个矩阵与其转置矩阵的乘法得到一个单位矩阵」这一性质，那么我们就能得到公式 <span class="math inline">\(C^T\)</span> x Small = Large，如下图所示。</p>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/转置卷积4.jpeg" alt="卷积的矩阵乘法：将 Small 输入图像（2×2）转换为 Large 输出图像（4×4）">
<p class="caption">卷积的矩阵乘法：将 Small 输入图像（2×2）转换为 Large 输出图像（4×4）</p>
</div>
<p><strong>正向卷积的计算公式</strong>为：</p>
<p>步长 <span class="math inline">\(strides = 1\)</span>，填充 <span class="math inline">\(padding = 0\)</span> ，即 <span class="math inline">\(i=4, k=3, s=1, p=0\)</span> ，</p>
<p>则按照卷积计算公式 <span class="math inline">\(o=\frac{i+2 p-k}{s}+1\)</span> ，输出图像 <span class="math inline">\(output\)</span> 的尺寸为 <span class="math inline">\(2 * 2\)</span>。</p>
<p><strong>反卷积的输入输出尺寸关系</strong>：</p>
<blockquote>
<ol style="list-style-type: decimal">
<li>若：<span class="math inline">\((o+2 p-k) \% s=0\)</span></li>
</ol>
<p>此时反卷积的输入尺寸为：</p>
<p><span class="math inline">\(o=s(i-1)-2 p+k\)</span></p>
</blockquote>
<blockquote>
<ol start="2" style="list-style-type: decimal">
<li>若：<span class="math inline">\((o+2 p-k) \% s \neq 0\)</span></li>
</ol>
<p>此时反卷积的输入输出尺寸关系为：</p>
<p><span class="math inline">\(o=s(i-1)-2 p+k+(o+2 p-k) \% s\)</span></p>
</blockquote>
<p>（具体的图像待补充！）</p>
<p><strong>2. 举例</strong>：通过应用各种填充和步长，我们可以将同样的 2×2 输入图像映射到不同的图像尺寸。</p>
<p><strong>（1）</strong>将 2 x 2 的输入上采样成 4 x 4的输出</p>
<p>在一个 2×2 的输入（周围加了 2×2 的单位步长的零填充）上应用一个 3×3 核的转置卷积。上采样输出的大小是 4×4。</p>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/转置卷积1.gif" alt="转置卷积(反卷积)">
<p class="caption">转置卷积(反卷积)</p>
</div>
<p><strong>（2）</strong>将 2×2 的输入上采样成 5×5 的输出</p>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/转置卷积2.gif" alt="转置卷积(反卷积)">
<p class="caption">转置卷积(反卷积)</p>
</div>
<h4 id="扩张卷积空洞卷积dilated-convolution">扩张卷积（空洞卷积）dilated convolution <a href="https://www.zhihu.com/question/54149221" target="_blank" rel="noopener">*</a></h4>
<p>（1）标准的离散卷积<br>
<span class="math display">\[
(F * k)(\boldsymbol{p})=\sum_{\boldsymbol{s}+\boldsymbol{t}=\boldsymbol{p}} F(\boldsymbol{s}) k(\boldsymbol{t})
\]</span><br>
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/标准离散卷积.gif" alt="扩张卷积"></p>
<p>（2）扩张卷积如下：<br>
<span class="math display">\[
\left(F *_{l} k\right)(\boldsymbol{p})=\sum_{\boldsymbol{s}+l \boldsymbol{t}=\boldsymbol{p}} F(\boldsymbol{s}) k(\boldsymbol{t})
\]</span><br>
当 l=1 时，扩张卷积会变得和标准卷积一样。</p>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/扩张卷积.gif" alt="扩张卷积">
<p class="caption">扩张卷积</p>
</div>
<p>直观而言，<strong>扩张卷积就是通过在核元素之间插入空格来使核「膨胀」</strong>。新增的参数 <span class="math inline">\(l\)</span>（扩张率）表示我们希望将核加宽的程度。具体实现可能各不相同，但通常是在核元素之间插入 <span class="math inline">\(l-1\)</span> 个空格。下面展示了 <span class="math inline">\(l = 1, 2, 4\)</span> 时的核大小。<img src="/2018/04/07/Try-your-best!You-will-find-the-job/扩张卷积2.jpeg" alt="扩张卷积的感受野。我们基本上无需添加额外的成本就能有较大的感受野"></p>
<p>在这张图像中，3×3 的红点表示经过卷积后，输出图像是 3×3 像素。尽管所有这三个扩张卷积的输出都是同一尺寸，但模型观察到的感受野有很大的不同。<span class="math inline">\(l=1\)</span> 时感受野为 3×3，<span class="math inline">\(l=2\)</span> 时为 7×7。<span class="math inline">\(l=3\)</span> 时，感受野的大小就增加到了 15×15。有趣的是，与<strong>这些操作相关的参数的数量是相等的</strong>。我们<strong>「观察」更大的感受野不会有额外的成本</strong>。因此，扩张卷积可用于廉价地增大输出单元的感受野，而不会增大其核大小，这在多个扩张卷积彼此堆叠时尤其有效。</p>
<h4 id="分组卷积group-convolution">分组卷积(Group convolution)</h4>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/分组卷积.jpg">

</div>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/分组卷积2.jpg">

</div>
<p>第一张图代表 <strong>标准卷积操作</strong>。若输入特征图尺寸为 <span class="math inline">\(H \times W \times c_1\)</span> ，卷积核尺寸为 <span class="math inline">\(h_{1} \times w_{1} \times c_{1}\)</span> ，输出特征图尺寸为 <span class="math inline">\(H \times W \times c_{2}\)</span> ，标准卷积层的参数量为： <span class="math inline">\(\left(\boldsymbol{h}_{\mathbf{1}} \times \boldsymbol{w}_{\mathbf{1}} \times \boldsymbol{c}_{\mathbf{1}}\right) \times \boldsymbol{c}_{2}\)</span> 。<em>（一个滤波器在输入特征图 <span class="math inline">\(h_{1} \times w_{1} \times c_{1}\)</span> 大小的区域内操作，输出结果为1个数值，所以需要 <span class="math inline">\(c_2\)</span> 个滤波器。）</em></p>
<p>第二张图代表 <strong>分组卷积操作</strong>。将输入特征图按照通道数分成 <span class="math inline">\(g\)</span> 组，则每组输入特征图的尺寸为 <span class="math inline">\(H \times W \times\left(\frac{c_{1}}{g}\right)\)</span> ，对应的卷积核尺寸为 <span class="math inline">\(h_{1} \times w_{1} \times\left(\frac{c_{1}}{g}\right)\)</span> ，每组输出特征图尺寸为 <span class="math inline">\(H \times W \times\left(\frac{c_{2}}{g}\right)\)</span>。将 <span class="math inline">\(g\)</span> 组结果拼接(concat)，得到最终尺寸为 <span class="math inline">\(H \times W \times c_{2}\)</span> 的输出特征图。分组卷积层的参数量为 <span class="math inline">\(h_{1} \times w_{1} \times\left(\frac{c_{1}}{g}\right) \times\left(\frac{c_{2}}{g}\right) \times g=h_{1} \times w_{1} \times c_{1} \times c_{2} \times \frac{1}{g}\)</span> 。</p>
<blockquote>
<p>深入思考一下，常规卷积输出的特征图上，每一个点是由输入特征图 <span class="math inline">\(h_{1} \times w_{1} \times c_{1}\)</span> 个点计算得到的；而分组卷积输出的特征图上，每一个点是由输入特征图 <span class="math inline">\(h_{1} \times w_{1} \times\left(\frac{c_{1}}{g}\right)\)</span> 个点计算得到的。自然，分组卷积的参数量是标准卷积的 1<span class="math inline">\(/ g\)</span> 。</p>
</blockquote>
<h4 id="深度可分卷积depthwise-separable-convolution">深度可分卷积(Depthwise separable convolution)</h4>
<blockquote>
<p>主要用于 MobileNet 和 Xception</p>
</blockquote>
<p>这张图怎么少的了呢：</p>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/深度可分离卷积.jpg">

</div>
<p>图(a)代表标准卷积。假设输入特征图尺寸为 <span class="math inline">\(D_{F} \times D_{F} \times M\)</span> ，卷积核尺寸为 <span class="math inline">\(D_{K} \times D_{K} \times M\)</span> ，输出特征图尺寸为 <span class="math inline">\(D_{F} \times D_{F} \times N\)</span>，标准卷积层的参数量为： <span class="math inline">\(\left(\boldsymbol{D}_{\boldsymbol{K}} \times \boldsymbol{D}_{\boldsymbol{K}} \times \boldsymbol{M}\right) \times \boldsymbol{N}\)</span> 。</p>
<p>图(b)代表深度卷积，图(c)代表逐点卷积，两者合起来就是深度可分离卷积。深度卷积负责滤波，尺寸为 <span class="math inline">\((D_{K} , D_{K}, 1)\)</span>，共 <span class="math inline">\(M\)</span> 个，作用在输入的每个通道上；逐点卷积负责转换通道，尺寸为 <span class="math inline">\((1,1,M)\)</span>，共 <span class="math inline">\(N\)</span> 个，作用在深度卷积的输出特征映射上。</p>
<p>深度卷积参数量为 <span class="math inline">\(\left(\boldsymbol{D}_{\boldsymbol{K}} \times \boldsymbol{D}_{\boldsymbol{K}} \times \mathbf{1}\right) \times \boldsymbol{M}\)</span> ，逐点卷积参数量为 <span class="math inline">\((\mathbf{1} \times \mathbf{1} \times \boldsymbol{M}) \times \boldsymbol{N}\)</span> ，所以深度可分离卷积参数量是标准卷积的 <span class="math inline">\(\frac{D_{K} \times D_{K} \times M+M \times N}{D_{K} \times D_{K} \times M \times N}=\frac{1}{N}+\frac{1}{D_{K}^{2}}\)</span> 。</p>
<blockquote>
<p>为了便于理解、便于和分组卷积类比，假设 <span class="math inline">\(M = N\)</span> 。深度卷积其实就是 <span class="math inline">\(g=M=N\)</span> 的分组卷积，只不过没有直接将 <span class="math inline">\(g\)</span> 组结果拼接，所以深度卷积参数量是标准卷积的 1<span class="math inline">\(/ N\)</span> 。逐点卷积其实就是把 <span class="math inline">\(g\)</span> 组结果用 <span class="math inline">\(1 \times 1\)</span> conv 拼接起来，所以逐点卷积参数量是标准卷积的 <strong>1<span class="math inline">\(/ D_{K}^{2}\)</span> </strong>。(只考虑逐点卷积，之前输出的特征图上每一个点是由输入特征图 <span class="math inline">\(D_{K} \times D_{K}\)</span> 区域内的点计算得到的；而逐点卷积输出上每一个点是由 <span class="math inline">\(1 \times 1\)</span> 区域内的点计算得到的)。自然，深度可分离卷积参数量是标准卷积的 <span class="math inline">\(1 / N+1 / D_{K}^{2}\)</span> 。</p>
</blockquote>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/深度可分离卷积_0.jpg">

</div>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/65377955" target="_blank" rel="noopener">1</a></p>
</blockquote>
<h4 id="空间可分卷积">空间可分卷积</h4>
<p>主要思想是将卷积核拆分成两个更小的核，然后依次进行卷积，可以节省一部分成本，但深度学习却很少使用它。一大主要原因是并非所有的核都能分成两个更小的核。如果我们用空间可分卷积替代所有的传统卷积，那么我们就限制了自己在训练过程中搜索所有可能的核。这样得到的训练结果可能是次优的。（具体的有时间再补充）</p>
<hr>
<h4 id="感受野">感受野</h4>
<p>感受野是卷积神经网络(CNN)每一层输出的特征图(feature map)上的像素点在原始输入图像上映射的区域大小。</p>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/感受野.png">

</div>
<ul>
<li><strong>感受野的计算</strong></li>
</ul>
<p>原始计算： <span class="math inline">\(H_2 = \frac{H_1 - K + 2P}{S} + 1\)</span></p>
<p>感受野计算：<span class="math inline">\(H_1 = (H_2 - 1) * S + K - 2P\)</span></p>
<ul>
<li><strong>增大感受野的方法</strong></li>
<li>增加pooling层，但是会降低准确率</li>
<li>增大卷积核 kernel size，但是会增加参数</li>
<li>增加卷积层的个数，但是会面临梯度消失的问题</li>
<li>使用<strong>空洞卷积</strong></li>
</ul>
<blockquote>
<p><a href="https://blog.csdn.net/jiachen0212/article/details/78548667" target="_blank" rel="noopener">1</a> <a href="https://blog.csdn.net/a529975125/article/details/80888463" target="_blank" rel="noopener">2</a></p>
</blockquote>
<h4 id="pooling层反向传播">Pooling层反向传播</h4>
<p><strong>1 . mean pooling</strong></p>
<p>mean pooling的前向传播就是把一个patch中的值求取平均来做pooling，那么反向传播的过程也就是把某个元素的梯度等分为 <span class="math inline">\(N\)</span> 份分配给前一层，这样就保证池化前后的梯度（残差）之和保持不变，还是比较理解的，图示如下:<br>
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/mean_pooling.jpeg"><br>
mean pooling比较容易让人理解错的地方就是会简单的认为直接把梯度复制 <span class="math inline">\(N\)</span> 遍之后直接反向传播回去，但是这样会造成loss之和变为原来的 <span class="math inline">\(N\)</span> 倍，网络是会产生梯度爆炸的。</p>
<p><strong>2 . max pooling</strong></p>
<p>首先考虑普通 <code>max pooling</code> 层如何求导，如何求导。设 <span class="math inline">\(x_i\)</span> 为输入层的节点，<span class="math inline">\(y_j\)</span> 为输出层的节点，那么损失函数 <span class="math inline">\(L\)</span> 对输入层节点 <span class="math inline">\(x_i\)</span> 的梯度为：<br>
<span class="math display">\[
\frac{\partial L}{\partial x_i} = \left\{
\begin{matrix}
 &amp;0                              &amp;\delta(i,j) = false  \\ 
 &amp;\frac{\partial L}{\partial y_j}      &amp;\delta(i,j) = true
\end{matrix}\right.
\]</span><br>
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/max_pooling.png"></p>
<p>其中判决函数 <span class="math inline">\(δ(i,j)\)</span> 表示 <span class="math inline">\(i\)</span> 节点是否被输出 <span class="math inline">\(j\)</span> 节点选为最大值输出。不被选中 <span class="math inline">\(\delta(i,j) = false\)</span> 有两种可能： <span class="math inline">\(x_i\)</span> 不在 <span class="math inline">\(y_j\)</span> 范围内，或者<span class="math inline">\(x_i\)</span> 不是最大值。</p>
<ul>
<li>若选中，<span class="math inline">\(\delta(i,j) = true\)</span> ，则由链式法则可知： 损失函数 <span class="math inline">\(L\)</span> 相对于 <span class="math inline">\(x_i\)</span> 的梯度 <span class="math inline">\(\frac{\partial L}{\partial x_i}\)</span> = <span class="math inline">\(\frac{\partial L}{\partial y_i} \frac{\partial y_i}{\partial x_i}\)</span> ，选中时 <span class="math inline">\(\frac{\partial y_i}{\partial x_i} 恒等于1\)</span></li>
<li>若不选中，<span class="math inline">\(\delta(i,j) = false\)</span> ，<span class="math inline">\(\frac{\partial L}{\partial x_i}\)</span> = <span class="math inline">\(\frac{\partial L}{\partial y_i} \frac{\partial y_i}{\partial x_i}\)</span> ，此时 <span class="math inline">\(\frac{\partial y_i}{\partial x_i} 恒等于0\)</span> ，综上可得上述公式</li>
</ul>
<p><strong>3 . roi max pooling</strong></p>
<p>对于<code>roi max pooling</code>，设 <span class="math inline">\(x_i\)</span> 为输入层的节点，<span class="math inline">\(y_{rj}\)</span> 为第 <span class="math inline">\(r\)</span> 个候选区域的第 <span class="math inline">\(j\)</span> 个输出节点。 一个输入节点可能和多个输出节点相连，如下图所示，输入节点7和两个候选区域输出节点相关连：</p>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/roi_max_pooling1.png">

</div>
<p>该输入节点7的反向传播如下图所示。对于不同候选区域，节点7都存在梯度，所以反向传播中损失函数 <span class="math inline">\(L\)</span> 对输入层节点 <span class="math inline">\(x_i\)</span> 的梯度为损失函数 <span class="math inline">\(L\)</span> 对各个有可能的候选区域 <span class="math inline">\(r\)</span> （ <span class="math inline">\(x_i\)</span> 被候选区域 <span class="math inline">\(r\)</span> 的第 <span class="math inline">\(j\)</span> 个输出节点选为最大值）输出 <span class="math inline">\(y_{ri}\)</span> 梯度的累加，具体如下公式所示：</p>
<p><img src="/2018/04/07/Try-your-best!You-will-find-the-job/roi_max_pooling2.png"><br>
<span class="math display">\[
\frac{\partial L}{\partial x_{i}}=\sum_{r} \sum_{j}\left[i=i^{*}(r, j)\right] \frac{\partial L}{\partial y_{rj}}
\]</span></p>
<p><span class="math display">\[
\left[i=i^{*}(r, j)\right]=\left\{\begin{array}{ll}{1,} &amp; {i=i^{*}(r, j) \geq 1} \\ {0,} &amp; {\text { otherwise }}\end{array}\right.
\]</span></p>
<p>判决函数 <span class="math inline">\([i=i^{*}(r, j)]\)</span> 表示 <span class="math inline">\(i\)</span> 节点是否被候选区域 <span class="math inline">\(r\)</span> 的第 <span class="math inline">\(j\)</span> 个节点选为最大值输出。若是，则由链式规则可知损失函数 <span class="math inline">\(L\)</span> 相对 <span class="math inline">\(x_i\)</span> 的梯度等于损失函数 <span class="math inline">\(L\)</span> 相对 <span class="math inline">\(y_{rj}\)</span> 的梯度 ×（ <span class="math inline">\(y_{rj}\)</span> 对 <span class="math inline">\(x_i\)</span> 的梯度-&gt;恒等于1)，上图已然解释该输入节点可能会和不同的 <span class="math inline">\(y_{rj}\)</span> 有关系，故损失函数 <span class="math inline">\(L\)</span> 相对 <span class="math inline">\(x_i\)</span> 的梯度为求和形式。</p>
<h4 id="为什么用smoothl1-loss而不用-l2-loss">为什么用SmoothL1 loss而不用 L2 loss</h4>
<p><span class="math inline">\(Smooth_{L_1} Loss\)</span> 对于离群点更加鲁棒，其相比于 <span class="math inline">\(L2\)</span> 损失函数，其对离群点，异常值(outlier)不敏感，可控制梯度的量级使训练时不容易跑飞，从梯度的角度进行分析。</p>
<ul>
<li><a href="https://www.zhihu.com/question/58200555" class="uri" target="_blank" rel="noopener">https://www.zhihu.com/question/58200555</a></li>
</ul>
<h4 id="iou">IOU</h4>
<h4 id="nms">NMS</h4>
<p>目标检测算法RCNN系列中，会从一张图片中找出n多个可能是物体的矩形框bounding box，每个矩形框都会有一个类别分类概率(得分)：</p>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/NMS.png" alt="img">
<p class="caption">img</p>
</div>
<p>就像上面的图片一样，定位一个车辆，最后算法就找出了一堆的方框，我们需要判别哪些矩形框是没用的。</p>
<p><strong>(1) 算法思想</strong></p>
<blockquote>
<ol style="list-style-type: decimal">
<li>按得分从低到高将 bbox 排序，例如：A、B、C、D、E、F</li>
</ol>
</blockquote>
<blockquote>
<ol start="2" style="list-style-type: decimal">
<li>假设 F 的得分最高，保留。将A~E分别与F计算重叠度IoU，判断IoU是否大于某个设定的阈值。假设B、D与 F 的IoU大于阈值，那么B和D可认为是重复标记的，舍去。</li>
</ol>
</blockquote>
<blockquote>
<ol start="3" style="list-style-type: decimal">
<li>余下A、C、E重复前面两步骤，找到所有被保留下来的bbox矩形框。</li>
</ol>
</blockquote>
<p><strong>(2) 算法实现</strong></p>
<ul>
<li><code>python</code> 实现：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li><code>MATLAB</code> 实现：</li>
</ul>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">pick</span> = <span class="title">nms</span><span class="params">(boxes, overlap)</span></span></span><br><span class="line"><span class="comment">% boxes: 	M x 5 表示有M个框，维度为5 [x1, y1, x2, y2, score]</span></span><br><span class="line"><span class="comment">% overlap:  IoU 阈值</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">isempty</span>(boxes)		<span class="comment">% 输入为空，直接返回</span></span><br><span class="line">  pick = [];</span><br><span class="line">  <span class="keyword">return</span>;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 取出坐标与得分</span></span><br><span class="line">x1 = boxes(:,<span class="number">1</span>);	</span><br><span class="line">y1 = boxes(:,<span class="number">2</span>);</span><br><span class="line">x2 = boxes(:,<span class="number">3</span>);</span><br><span class="line">y2 = boxes(:,<span class="number">4</span>);</span><br><span class="line">s = boxes(:,<span class="keyword">end</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">% 计算每一个框的面积，并将得分按升序排列</span></span><br><span class="line">area = (x2-x1+<span class="number">1</span>) .* (y2-y1+<span class="number">1</span>);</span><br><span class="line">[vals, I] = <span class="built_in">sort</span>(s);    </span><br><span class="line"></span><br><span class="line">pick = s*<span class="number">0</span>;</span><br><span class="line">counter = <span class="number">1</span>;</span><br><span class="line"><span class="comment">% 循环直至所有框处理完</span></span><br><span class="line"><span class="keyword">while</span> ~<span class="built_in">isempty</span>(I)       </span><br><span class="line">  last = <span class="built_in">length</span>(I);		<span class="comment">% 当前剩余框的数量</span></span><br><span class="line">  <span class="built_in">i</span> = I(last);    		<span class="comment">% 选中最后一个，即等分最高的框      </span></span><br><span class="line">  pick(counter) = <span class="built_in">i</span>;    <span class="comment">% 每次保存得分最高的框</span></span><br><span class="line">  counter = counter + <span class="number">1</span>;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">% 计算相交面积，画个图就面白坐标的意思</span></span><br><span class="line">  xx1 = <span class="built_in">max</span>(x1(<span class="built_in">i</span>), x1(I(<span class="number">1</span>:last<span class="number">-1</span>)));</span><br><span class="line">  yy1 = <span class="built_in">max</span>(y1(<span class="built_in">i</span>), y1(I(<span class="number">1</span>:last<span class="number">-1</span>)));</span><br><span class="line">  xx2 = <span class="built_in">min</span>(x2(<span class="built_in">i</span>), x2(I(<span class="number">1</span>:last<span class="number">-1</span>)));</span><br><span class="line">  yy2 = <span class="built_in">min</span>(y2(<span class="built_in">i</span>), y2(I(<span class="number">1</span>:last<span class="number">-1</span>)));</span><br><span class="line">  </span><br><span class="line">  w = <span class="built_in">max</span>(<span class="number">0.0</span>, xx2-xx1+<span class="number">1</span>);</span><br><span class="line">  h = <span class="built_in">max</span>(<span class="number">0.0</span>, yy2-yy1+<span class="number">1</span>);</span><br><span class="line">  inter = w.*h;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">% 不同定义下的IOU</span></span><br><span class="line">  <span class="keyword">if</span> strcmp(<span class="built_in">type</span>,<span class="string">'Min'</span>)</span><br><span class="line">  	<span class="comment">% 重叠面积与最小框面积的比值</span></span><br><span class="line">  	o = inter ./ <span class="built_in">min</span>(area(<span class="built_in">i</span>), area(I(<span class="number">1</span>:last<span class="number">-1</span>)));</span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">  	<span class="comment">% 交集/并集</span></span><br><span class="line">  	o = inter ./ (area(<span class="built_in">i</span>) + area(I(<span class="number">1</span>:last<span class="number">-1</span>)) - inter);</span><br><span class="line">  <span class="keyword">end</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">% 保留所有重叠面积小于阈值的框，留作下次处理</span></span><br><span class="line">  I = I(<span class="built_in">find</span>(o&lt;=overlap));</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">pick = pick(<span class="number">1</span>:(counter<span class="number">-1</span>));</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>C++</code> 实现</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @brief DetInference::NMS</span></span><br><span class="line"><span class="comment"> * @param bboxes:       Descend sorted bboxes</span></span><br><span class="line"><span class="comment"> * @param scores:       Descend sorted scores</span></span><br><span class="line"><span class="comment"> * @param overlap:      Overlap of NMS</span></span><br><span class="line"><span class="comment"> * @return              Remain bboxes' index</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; DetInference::NMS(<span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">float</span>&gt; &gt; bboxes, <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">float</span>&gt; scores, <span class="keyword">float</span> overlap) &#123;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; pick_vec;</span><br><span class="line">    <span class="keyword">int</span> n_sample = bboxes.size();</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">float</span>&gt; area_vec(n_sample);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n_sample; i++) &#123;</span><br><span class="line">        area_vec[i] = (bboxes[i][<span class="number">2</span>] - bboxes[i][<span class="number">0</span>] + <span class="number">1</span>) * (bboxes[i][<span class="number">3</span>] - bboxes[i][<span class="number">1</span>] + <span class="number">1</span>);</span><br><span class="line">        <span class="keyword">if</span> (area_vec[i] &lt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"Boxes area must &gt;= 0."</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">            <span class="keyword">return</span> pick_vec;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">multimap</span>&lt;<span class="keyword">float</span>, <span class="keyword">int</span>&gt; nms_score;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n_sample; i++) &#123;</span><br><span class="line">        nms_score.insert(<span class="built_in">std</span>::pair&lt;<span class="keyword">float</span>, <span class="keyword">int</span>&gt;(scores[i], i));  <span class="comment">// Note: insert in position 0</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">do</span> &#123;</span><br><span class="line">        <span class="keyword">int</span> last = nms_score.rbegin()-&gt;second;</span><br><span class="line">        pick_vec.push_back(last);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Get the biggest score map iterator for this literation, </span></span><br><span class="line">        <span class="comment">// whose idx has save in pick_vec. And Then erase the max score map element.</span></span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">multimap</span>&lt;<span class="keyword">float</span>, <span class="keyword">int</span>&gt;::iterator lit = nms_score.find(nms_score.rbegin()-&gt;first);</span><br><span class="line">        nms_score.erase(lit);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="built_in">std</span>::<span class="built_in">multimap</span>&lt;<span class="keyword">float</span>, <span class="keyword">int</span>&gt;::iterator it = nms_score.begin(); it != nms_score.end(); ) &#123;</span><br><span class="line">            <span class="keyword">int</span> it_idx = it-&gt;second;</span><br><span class="line">            <span class="keyword">float</span> xx1 = maxx(bboxes[last][<span class="number">0</span>], bboxes[it_idx][<span class="number">0</span>]);</span><br><span class="line">            <span class="keyword">float</span> yy1 = maxx(bboxes[last][<span class="number">1</span>], bboxes[it_idx][<span class="number">1</span>]);</span><br><span class="line">            <span class="keyword">float</span> xx2 = minn(bboxes[last][<span class="number">2</span>], bboxes[it_idx][<span class="number">2</span>]);</span><br><span class="line">            <span class="keyword">float</span> yy2 = minn(bboxes[last][<span class="number">3</span>], bboxes[it_idx][<span class="number">3</span>]);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">float</span> w = maxx(<span class="keyword">float</span>(<span class="number">0.0</span>), xx2-xx1+<span class="number">1</span>);</span><br><span class="line">            <span class="keyword">float</span> h = maxx(<span class="keyword">float</span>(<span class="number">0.0</span>), yy2-yy1+<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">float</span> ov = w * h / (area_vec[last] + area_vec[it_idx] - w * h + ESP);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (ov &gt; overlap) &#123;</span><br><span class="line">                it = nms_score.erase(it);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                it ++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">while</span> (nms_score.size() != <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">return</span> pick_vec;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="softmax-实现">Softmax 实现</h4>
<h4 id="卷积实现">卷积实现</h4>
<h4 id="yolo">YOLO</h4>
<h4 id="section"></h4>
<h4 id="工程部署">工程部署</h4>
<p>将模型提取出来，然后封装相应的C++接口，然后编写CUDA代码进行加速优化。</p>
<h4 id="移动端部署-优化">移动端部署 + 优化</h4>
<h3 id="数据结构">数据结构</h3>
<h4 id="kmp算法">KMP算法</h4>
<p>时间复杂度为： <span class="math inline">\(O(n+m)\)</span> ，朴素的模式匹配时间复杂度为：<span class="math inline">\(O(n*m)\)</span></p>
<p><strong>（1）计算模式串的next数组方式</strong></p>
<blockquote>
<p>例题：在用KMP算法进行模式匹配时，求模式串 “ababaaababaa” 的next数组<br>
<strong>next数组</strong>：中储存的是这个字符串前缀和后缀中相同字符串的最长长度，即<strong>模式串前面与后面匹配的位数，即为next的值</strong></p>
</blockquote>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/KMP.png">

</div>
<blockquote>
<p><strong>C++简单实现</strong></p>
</blockquote>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * KMP算法</span></span><br><span class="line"><span class="comment"> * @param str:      主串</span></span><br><span class="line"><span class="comment"> * @param substr:   模式串</span></span><br><span class="line"><span class="comment"> */</span> </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">KMP</span><span class="params">(<span class="built_in">string</span> str, <span class="built_in">string</span> substr, <span class="keyword">int</span> next[])</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i = <span class="number">0</span>, j = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span> (i &lt; str.length() &amp;&amp; j &lt; substr.length()) &#123;</span><br><span class="line">        <span class="keyword">if</span> (j == <span class="number">-1</span> || str[i] == substr[j]) &#123; <span class="comment">// 字符匹配，或者j == -1，这个是由next数组产生的</span></span><br><span class="line">            i++;</span><br><span class="line">            j++;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;                                <span class="comment">// 字符不匹配</span></span><br><span class="line">            j = next[j];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (j == substr.length())                   <span class="comment">// 匹配成功，返回主串开始匹配的索引</span></span><br><span class="line">        <span class="keyword">return</span> i - substr.length();</span><br><span class="line">    <span class="keyword">else</span>                                        <span class="comment">// 匹配不成功</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 计算模式串的next数组</span></span><br><span class="line"><span class="comment"> * @param substr:   模式串</span></span><br><span class="line"><span class="comment"> * @param next[]:   next数组</span></span><br><span class="line"><span class="comment"> * 思想：（自己手动结合md笔记例题好好Debug一遍就理解了）</span></span><br><span class="line"><span class="comment"> */</span> </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">GetNext</span><span class="params">(<span class="built_in">string</span> substr, <span class="keyword">int</span> next[])</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i = <span class="number">0</span>, j = <span class="number">-1</span>;</span><br><span class="line">    next[<span class="number">0</span>] = <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">while</span> (i &lt; substr.length()) &#123;          </span><br><span class="line">        <span class="keyword">if</span> (j == <span class="number">-1</span> || substr[i] == substr[j]) &#123;<span class="comment">// 若 j==-1，或者该字符匹配时，则i,j都后移一位</span></span><br><span class="line">            i++;</span><br><span class="line">            j++;</span><br><span class="line">            next[i] = j;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;            <span class="comment">// 若 j!=-1，或者该字符不匹配时，此时，substr[i]即与substr[next[j]]进行比较</span></span><br><span class="line">            j = next[j];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="built_in">string</span> str = <span class="string">"ababababca"</span>;</span><br><span class="line">    <span class="built_in">string</span> strsub = <span class="string">"abababca"</span>;</span><br><span class="line">    <span class="comment">// 计算模式串的next数组</span></span><br><span class="line">    <span class="keyword">int</span> *next = <span class="keyword">new</span> <span class="keyword">int</span>[strsub.size()];</span><br><span class="line">    GetNext(strsub, next);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; strsub.size(); i++) &#123;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; next[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 使用KMP算法进行匹配</span></span><br><span class="line">    <span class="keyword">int</span> start_idx = KMP(str, strsub, next);</span><br><span class="line">    <span class="keyword">int</span> end_idx = start_idx + strsub.size();</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = start_idx; i &lt; end_idx; i++) &#123;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; str[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">delete</span> [] next;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>Python简单实现</strong></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">KMP</span><span class="params">(str, substr, next)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    KMP 匹配算法</span></span><br><span class="line"><span class="string">    :param str:     主串</span></span><br><span class="line"><span class="string">    :param substr:  模式串</span></span><br><span class="line"><span class="string">    :param next:    模式串的next数组</span></span><br><span class="line"><span class="string">    :return:        返回匹配成功主串的起始匹配地址，或者不匹配时返回-1</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    j = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> i &lt; len(str) <span class="keyword">and</span> j &lt; len(strsub):</span><br><span class="line">        <span class="keyword">if</span> j == <span class="number">-1</span> <span class="keyword">or</span> str[i] == strsub[j]:  <span class="comment"># 字符匹配，或者j == -1，这个是由next数组产生的</span></span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">            j += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:                               <span class="comment"># 字符不匹配</span></span><br><span class="line">            j = next[j]</span><br><span class="line">    <span class="keyword">if</span> j == len(substr):                    <span class="comment"># 匹配成功，返回主串开始匹配的索引</span></span><br><span class="line">        <span class="keyword">return</span> i - len(substr)</span><br><span class="line">    <span class="keyword">else</span>:                                   <span class="comment"># 匹配不成功</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">GetNext</span><span class="params">(substr, next)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    计算模式串的next数组</span></span><br><span class="line"><span class="string">    :param substr:  模式串</span></span><br><span class="line"><span class="string">    :param next:    next数组，这里传入的是空的</span></span><br><span class="line"><span class="string">    :return:        返回计算后的next数组</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    next[<span class="number">0</span>] = <span class="number">-1</span></span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    j = <span class="number">-1</span></span><br><span class="line">    <span class="keyword">while</span> i &lt; len(substr):          </span><br><span class="line">        <span class="keyword">if</span> j == <span class="number">-1</span> <span class="keyword">or</span> substr[i] == substr[j]:</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">            j += <span class="number">1</span></span><br><span class="line">            next[i] = j</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            j = next[j]</span><br><span class="line">    <span class="keyword">return</span> next</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    str = <span class="string">"ababababca"</span></span><br><span class="line">    strsub = <span class="string">"abababca"</span></span><br><span class="line">    next = [<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(len(strsub))]</span><br><span class="line">    next = GetNext(strsub, next)</span><br><span class="line">    start_idx = KMP(str, strsub, next)</span><br><span class="line">    <span class="keyword">if</span> start_idx == <span class="number">-1</span>:</span><br><span class="line">        print(<span class="string">'字符串匹配失败'</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        end_idx = start_idx + len(strsub)</span><br><span class="line">    print(str[start_idx:end_idx])</span><br></pre></td></tr></table></figure>
<h4 id="几大排序算法">几大排序算法</h4>
<p><strong>（1）时间复杂度</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">排序方法</th>
<th align="center">平均时间</th>
<th align="center">最好时间</th>
<th align="center">最坏时间</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><strong>桶排序</strong>(<span class="math inline">\(\color{red}不稳定\)</span>)</td>
<td align="center">O(n)</td>
<td align="center">O(n)</td>
<td align="center">O(n)</td>
</tr>
<tr class="even">
<td align="left"><strong>基数排序</strong>(<span class="math inline">\(\color{green}稳定\)</span>)</td>
<td align="center">O(n)</td>
<td align="center">O(n)</td>
<td align="center">O(n)</td>
</tr>
<tr class="odd">
<td align="left"><strong>归并排序</strong>(<span class="math inline">\(\color{green}稳定\)</span>)</td>
<td align="center">O(nlogn)</td>
<td align="center">O(nlogn)</td>
<td align="center">O(nlogn)</td>
</tr>
<tr class="even">
<td align="left"><strong>快速排序</strong>(<span class="math inline">\(\color{red}不稳定\)</span>)</td>
<td align="center">O(nlogn)</td>
<td align="center">O(nlogn)</td>
<td align="center">O(n^2)</td>
</tr>
<tr class="odd">
<td align="left"><strong>堆排序</strong>(<span class="math inline">\(\color{red}不稳定\)</span>)</td>
<td align="center">O(nlogn)</td>
<td align="center">O(nlogn)</td>
<td align="center">O(nlogn)</td>
</tr>
<tr class="even">
<td align="left"><strong>希尔排序</strong>(<span class="math inline">\(\color{red}不稳定\)</span>)</td>
<td align="center">O(n^1.25)</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left"><strong>冒泡排序</strong>(<span class="math inline">\(\color{green}稳定\)</span>)</td>
<td align="center">O(n^2)</td>
<td align="center">O(n)</td>
<td align="center">O(n^2)</td>
</tr>
<tr class="even">
<td align="left"><strong>选择排序</strong>(<span class="math inline">\(\color{red}不稳定\)</span>)</td>
<td align="center">O(n^2)</td>
<td align="center">O(n^2)</td>
<td align="center">O(n^2)</td>
</tr>
<tr class="odd">
<td align="left"><strong>直接插入排序</strong>(<span class="math inline">\(\color{green}稳定\)</span>)</td>
<td align="center">O(n^2)</td>
<td align="center">O(n)</td>
<td align="center">O(n^2)</td>
</tr>
</tbody>
</table>
<p><strong>（2）空间复杂度</strong></p>
<p><strong>冒泡排序</strong>，<strong>简单选择排序</strong>，<strong>堆排序</strong>，<strong>直接插入排序</strong>，<strong>希尔排序</strong>的空间复杂度为 O(1)，因为需要一个临时变量来交换元素位置 (另外遍历序列时自然少不了用一个变量来做索引)</p>
<p><strong>快速排序</strong>空间复杂度为 logn (因为递归调用了)，<strong>归并排序</strong>空间复杂是O(n)，需要一个大小为n的临时数组.</p>
<p><strong>（3）最快的排序算法是桶排序</strong></p>
<p>所有排序算法中最快的应该是桶排序(很多人误以为是快速排序,实际上不是.不过实际应用中快速排序用的多)但桶排序一般用的不多,因为有几个比较大的缺陷.</p>
<ol style="list-style-type: decimal">
<li>待排序的元素不能是负数,小数.</li>
<li>空间复杂度不确定,要看待排序元素中最大值是多少.</li>
</ol>
<p>所需要的辅助数组大小即为最大元素的值.</p>
<p>归并排序会造成内存溢出，因为归并排序必须开额外的空间，而且空间开销还比较大。</p>
<blockquote>
<p><strong>题目</strong> : <a href="https://www.nowcoder.com/questionTerminal/dfe1f8e38d1f47909ea48e1c8592ac75" target="_blank" rel="noopener">牛客</a></p>
</blockquote>
<p>n个数值选出最大m个数（3<m<n）的最小算法复杂度是： $o(n)$ #### c++常见容器的时间复杂度 `map` ，`set`，`multimap`，`multiset` ：这4种容器是采用红黑树实现，红黑树是平衡二叉树的一种。不同操作的时间复杂度近似为: - 插入：$o(logn)$ 查找：$o(logn)$ 删除：$o(logn)$ `hash_map`，`hash_set`，`hash_multimap`，`hash_multiset`：这4种容器采用哈希表实现。 插入：o(1)，最坏情况 o(n) 查找：o(1)，最坏情况 删除：o(1)，最坏情况 **标准库提供的8个关联容器** ![](加油！一定能找到好工作！ associative.png) map 和 set 的原理 [*](https: blog.csdn.net v_july_v article details 6105630) map和set的底层实现主要通过 **红黑树** 来实现 红黑树是一种特殊的二叉查找树 1）每个结点要么是红的要么是黑的 2）根节点是黑色 3） 每个叶子节点（叶结点即指树尾端nil指针或null结点）是黑色 4）如果一个节点是红色的，则它的子节点必须是黑色的 5） 对于任意结点而言，其到叶结点树尾端nil指针的每条路径都包含相同数目的黑结点 特性4）5）决定了没有一条路径会比其他路径长出2倍，因此**红黑树是接近平衡的二叉树**。> 正是红黑树的这5条性质，使一棵n个结点的红黑树始终保持了 <span class="math inline">\(logn\)</span> 的高度，从而也就解释了上面所说的 “红黑树” 的查找、插入、删除的时间复杂度最坏为 <span class="math inline">\(O(log n)\)</span></m<n）的最小算法复杂度是：></p>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/红黑树.png">

</div>
<h4 id="map-和-unordered_map-区别">map 和 unordered_map 区别</h4>
<p>map 是STL中的一个关联容器，提供键值对的数据管理。底层通过红黑树来实现，实际上是二叉排序树和非严格意义上的二叉平衡树。所以在map内部所有的数据都是有序的，且map的查询、插入、删除操作的时间复杂度都是 <span class="math inline">\(O(logn)\)</span></p>
<p>unordered_map 和 map 类似，都是存储 key-value 对，可以通过key快速索引到value，不同的是unordered_map不会根据 key 进行排序。unordered_map 底层是一个 <strong>防冗余的哈希表</strong>，存储时根据key的hash值判断元素是否相同，即 unoredered_map 内部是无序的。</p>
<h4 id="二叉树分类">二叉树分类</h4>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/二叉树定义1.jpg">

</div>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/二叉树定义2.jpg">

</div>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/二叉排序树1.jpg">

</div>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/二叉排序树插入.jpg">

</div>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/二叉排序树删除.jpg">

</div>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/二叉排序树查找效率.jpg">

</div>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/平衡二叉树.jpg">

</div>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/平衡二叉树插入.jpg">

</div>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/平衡二叉树查找+哈夫曼树.jpg">

</div>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/哈夫曼树.jpg">

</div>
<div class="figure">
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/哈夫曼编码.jpg">

</div>
<h4 id="二叉查找排序树">二叉查找(排序)树</h4>
<p>二叉查找树，也称有序二叉树（ordered binary tree），或已排序二叉树（sorted binary tree），是指一棵空树或者具有下列性质的二叉树：</p>
<ul>
<li>若任意节点的左子树不空，则左子树上所有结点的值均小于它的根结点的值</li>
<li>若任意节点的右子树不空，则右子树上所有结点的值均大于它的根结点的值</li>
<li>任意节点的左、右子树也分别为二叉查找树</li>
<li>没有键值相等的节点（no duplicate nodes）</li>
</ul>
<p>二叉排序数是一个递归的数据结构，可以很方便的使用递归算法对二叉排序树进行各种预算。由于 <strong>左子树节点值 &lt; 根节点值 &lt; 右子树节点值</strong> ，所以可以通过<strong>中序遍历</strong>，得到一个<u>递增的有序序列</u>。</p>
<h4 id="b-树-和-b树">B 树 和 B+树</h4>
<blockquote>
<p><a href="https://blog.csdn.net/yutianxin123/article/details/52269810" target="_blank" rel="noopener">1</a> <a href="https://blog.csdn.net/yu876876/article/details/84896789" target="_blank" rel="noopener">2</a></p>
</blockquote>
<hr>
<h3 id="面试题目">面试题目</h3>
<h4 id="色彩空间色彩空间的转换">色彩空间，色彩空间的转换</h4>
<h4 id="膨胀-与-腐蚀">膨胀 与 腐蚀</h4>
<p>其是形态学最基本的两种操作。膨胀与腐蚀能实现多种多样的功能，主要如下：</p>
<ul>
<li>消除噪声</li>
<li>分割(isolate)出独立的图像元素，在图像中连接(join)相邻的元素。</li>
<li>寻找图像中的明显的极大值区域或极小值区域</li>
<li>求出图像的梯度</li>
</ul>
<blockquote>
<p><strong>膨胀和腐蚀是对白色部分（高亮部分）而言的，不是黑色部分。膨胀就是图像中的高亮部分进行膨胀，“领域扩张”，效果图拥有比原图更大的高亮区域。腐蚀就是原图中的高亮部分被腐蚀，“领域被蚕食”，效果图拥有比原图更小的高亮区域。</strong></p>
</blockquote>
<p><strong>膨胀</strong></p>
<p><strong>膨胀就是求局部最大值的操作</strong>。按数学方面来说，膨胀操作就是将图像（或图像的一部分区域，我们称之为A）与核（我们称之为B）进行卷积。计算核B覆盖的区域的像素点的最大值，并把这个最大值赋值给参考点指定的像素。这样就会使图像中的高亮区域逐渐增长。</p>
<div class="figure">
<embed src="加油！一定能找到好工作！/膨胀腐蚀.webp">

</div>
<p>注意：其实右图要比左图大了一圈</p>
<p>效果图，高亮部分膨胀</p>
<div class="figure">
<embed src="加油！一定能找到好工作！/膨胀腐蚀2.webp">

</div>
<p>膨胀数学公式：<br>
<span class="math display">\[
\operatorname{dst}(x, y)=\max _{\left(x^{\prime}, y^{\prime}\right) : \text { element }\left(x^{\prime}, y^{\prime}\right) \neq 0} \operatorname{src}\left(x+x^{\prime}, y+y^{\prime}\right)
\]</span><br>
用<code>(x, y)</code>周边区域<code>(x+x', y+y')</code>内的最大值代替<code>(x, y)</code>的值。</p>
<p><strong>腐蚀</strong></p>
<p>腐蚀与膨胀是相反的操作，腐蚀是求局部最小值。腐蚀操作就是将图像A与核B进行卷积。计算核B覆盖的区域的像素点的最小值，并把这个最小值赋值给参考点指定的像素。这样就会使图像中的高亮区域逐渐被腐蚀。</p>
<div class="figure">
<embed src="加油！一定能找到好工作！/膨胀腐蚀3.webp">

</div>
<p><strong>注意：其实右图要比左图小了一圈</strong></p>
<p>A中能完全包含B的像素被留下来了。</p>
<p>效果图，高亮部分被腐蚀</p>
<div class="figure">
<embed src="加油！一定能找到好工作！/膨胀腐蚀4.webp">

</div>
<hr>
<h4 id="边缘检测算子">边缘检测算子</h4>
<blockquote>
<p><strong>边缘</strong>其实就是图像上灰度级变化很快的点的集合</p>
</blockquote>
<p>主要可以通过计算梯度(连续函数中叫微分；图像是离散的，叫差分，意思是一样的，也是求变化率)</p>
<p>差分的定义：<span class="math inline">\(f &#39;(x) = f(x + 1) - f(x)\)</span>，用后一项减前一项。<br>
按先后排列：<span class="math inline">\(-f(x) + f(x + 1)\)</span><br>
提出系数 [-1, 1] 作为滤波模板，跟原图 <span class="math inline">\(f(x)\)</span> 做卷积运算就可以检测边缘了。</p>
<p><strong>Roberts算子</strong> <a href="https://en.wikipedia.org/wiki/Roberts_cross" target="_blank" rel="noopener">*</a></p>
<blockquote>
<p><strong>Roberts算子是一种最简单的算子，是一种利用局部差分算子寻找边缘的算子。他采用对角线方向相邻两像素之差，来近似梯度幅值检测边缘。定位精度高，对噪声敏感，无法抑制噪声的影响。</strong></p>
</blockquote>
<p>具体公式：<br>
<span class="math display">\[
g(\mathrm{x}, \mathrm{y})= \sqrt{[\sqrt{f(x, y)}-\sqrt{f(x+1, y+1)}]^{2}+[\sqrt{f(x + 1, y)}-\sqrt{f(x, y + 1)} ]^{2}}
\]</span><br>
以上的 $f(x, y), f(x + 1, y), … $ 分别代表了图像中上下左右四个相邻像素：<br>
<span class="math display">\[
\begin{bmatrix}
f(x, y) &amp;f(x + 1, y)   \\ 
f(x, y + 1) &amp; f(x + 1, y + 1)
\end{bmatrix}
\]</span><br>
下面的两个2x2的卷积核构成了 Roberts 算子<br>
<span class="math display">\[
g_x= \begin{bmatrix}
1 &amp;0   \\ 
0 &amp;-1
\end{bmatrix}  \ \ \  
g_y= \begin{bmatrix}
0 &amp;1   \\ 
-1 &amp; 0
\end{bmatrix}
\]</span><br>
计算图像每个像素对应的梯度值：<br>
<span class="math display">\[
G_x = g_x * A  \ \ \ \  and  \ \ \ \  G_y = g_y * A \\
G = \sqrt{ {G_{x}}^2 +{G_y}^2}
\]</span><br>
以及梯度的方向：<br>
<span class="math display">\[
\Theta  = arctan(\frac{G_y}{G_x})
\]</span><br>
<strong>Prewitt算子</strong></p>
<blockquote>
<p><strong>Prewitt 算子是加权平均算子，对噪声有抑制作用，但是像素平均相当于对图像进行的低通滤波，所以Prewitt 算子对边缘的定位不如 Robert 算子。</strong></p>
</blockquote>
<p><span class="math display">\[
\begin{bmatrix}
f(x-1, y - 1) &amp; f(x, y-1) &amp;f(x+1, y-1) \\
f(x-1, y)    &amp;f(x, y)  &amp;f(x+1, y) \\
f(x-1, y+1)  &amp;f(x, y+1) &amp;f(x+1, y+1) \\
\end{bmatrix}
\]</span></p>
<p><span class="math inline">\(x\)</span> 水平方向和 <span class="math inline">\(y\)</span> 垂直方向的算子分别为：<br>
<span class="math display">\[
g_x= \begin{bmatrix}
-1 &amp;0 &amp; 1   \\ 
-1 &amp;0 &amp; 1   \\ 
-1 &amp;0 &amp; 1 
\end{bmatrix}  \ \ \  
g_y= \begin{bmatrix}
-1 &amp; -1 &amp; -1  \\ 
0 &amp; 0 &amp; 0 \\
1 &amp; 1 &amp; 1
\end{bmatrix}
\]</span><br>
<strong>Sobel算子（又称加权平均差分法）</strong></p>
<blockquote>
<p><strong>Sobel算子和Prewitt算子都是加权平均，但是Sobel算子对于像素位置的影响作了加权，认为像素越近，影响越大。因此，效果也相对更好。</strong></p>
</blockquote>
<p><span class="math display">\[
G_x = 
\begin{bmatrix}
-1 &amp;0  &amp;1 \\ 
-2 &amp;0  &amp;2 \\ 
-1 &amp;0  &amp;1 
\end{bmatrix} * A 
 \ \ \ \  and  \ \ \ \ 
G_y = 
\begin{bmatrix}
-1 &amp;-2  &amp;-1 \\ 
0  &amp;0   &amp;0 \\ 
1 &amp;2  &amp;1 
\end{bmatrix} * A
\]</span></p>
<p>然后再通过如下计算得到每个像素点对应的梯度值：<br>
<span class="math display">\[
G = \sqrt{ {G_{x}}^2 +{G_y}^2}
\]</span><br>
以及梯度的方向 [Compute gradient direction]：<br>
<span class="math display">\[
\Theta  = arctan(\frac{G_y}{G_x})
\]</span></p>
<blockquote>
<p>以上的算子都是利用一阶导数的信息，下面是利用二阶导数的边缘检测算子。</p>
</blockquote>
<p><strong>Laplace算子</strong></p>
<p>Laplace算子是一种各向同性算子，二阶微分算子，具有旋转不变性。在只关心边缘的位置而不考虑其周围的象素灰度差值时比较合适。Laplace算子对孤立象素的响应要比对边缘或线的响应要更强烈，因此只适用于无噪声图象。<br>
<span class="math display">\[
\left[\begin{array}{ccc}{0} &amp; {1} &amp; {0} \\ {1} &amp; {-4} &amp; {1} \\ {0} &amp; {1} &amp; {0}\end{array}\right]
\]</span><br>
<strong>Canny算子</strong></p>
<p>Canny算子是一个具有滤波，增强，检测的多阶段的优化算子，在进行处理前，Canny算子先利用高斯平滑滤波器来平滑图像以除去噪声，Canny分割算法采用一阶偏导的有限差分来计算梯度幅值和方向，在处理过程中，Canny算子还将经过一个非极大值抑制的过程，最后Canny算子还采用两个阈值来连接边缘。Canny边缘检测法利用高斯函数的一阶微分，它能在噪声抑制和边缘检测之间取得较好的平衡。</p>
<p><strong>Canny边缘检测基本原理</strong></p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span>高斯滤波器平滑图像，除去噪声。</span><br><span class="line"><span class="number">2.</span>一阶差分偏导计算梯度值和方向。</span><br><span class="line"><span class="number">3.</span>对梯度值不是极大值的地方进行抑制。</span><br><span class="line"><span class="number">4.</span>用双阈值连接图上的连通点。</span><br></pre></td></tr></table></figure>
<table style="width:69%;">
<colgroup>
<col width="12%">
<col width="56%">
</colgroup>
<thead>
<tr class="header">
<th>算子</th>
<th>优缺点</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Roberts</td>
<td>对具有陡峭的低噪声的图像处理效果较好，但提取的边缘结果较粗糙，边缘定位精度不是很高</td>
</tr>
<tr class="even">
<td>Prewitt</td>
<td>对灰度渐变和噪声较多的图像处理较好，对噪声有一定的抑制，抑制的原理是通过像素平均，像素平均相当于对图像的低通滤波，所以定位不如Roberts</td>
</tr>
<tr class="odd">
<td>Soble</td>
<td>是对Prewitt的加权平均，认为不同距离的像素对当前像素的影响是不一样的，像素越近，影响越大。对灰度渐变和噪声较多的图像处理较好，对边缘定位比较准确</td>
</tr>
<tr class="even">
<td>Lapacian</td>
<td>具有同向性，对图像中的阶跃性边缘定位准确，对噪声敏感，一般进行检测是要先滤波。但也会丢失一部分边缘方向信息，造成一些不连续的检测边缘</td>
</tr>
<tr class="odd">
<td>Candy</td>
<td>有好的信噪比，能够有效的抑制噪声，定位精度也高。</td>
</tr>
</tbody>
</table>
<blockquote>
<p>Reference: <a href="https://www.jianshu.com/p/2334bee37de5" target="_blank" rel="noopener">1</a> <a href="https://blog.csdn.net/swj110119/article/details/51777422" target="_blank" rel="noopener">2</a> <a href="https://blog.csdn.net/zlrai5895/article/details/78596433" target="_blank" rel="noopener">3</a> <a href="https://blog.csdn.net/songzitea/article/details/14108503" target="_blank" rel="noopener">4</a> <a href="https://blog.csdn.net/ViatorSun/article/details/82351222" target="_blank" rel="noopener">5</a> <a href="https://blog.csdn.net/swj110119/article/details/51777422" target="_blank" rel="noopener">6</a> <a href="https://blog.csdn.net/LilyNothing/article/details/78996239" target="_blank" rel="noopener">7</a></p>
</blockquote>
<p><strong>典型例题：</strong></p>
<p>下列关于图像边缘检测所用到的算子说法错误的是（B） A. Sobel算子是把各个方向上的灰度值加权之和作为输出 B. Robinson算子是一个边缘模板算子，由八个方向的样板组成，225度角模板为{2,1,0;1,2,-1;0,-1,-2} C. Kirsch算子是一个边缘模板算子，由八个方向的样板组成，255度角模板为{-3，-3，-3；5，0，-3,；5，5，-3} D. Prewitt算子是一个边缘模板算子，由八个方向的样板组成，255度角模板为 {-1,-1,1;-1,-2,1;1,1,1}</p>
<hr>
<h4 id="常见的特征点">常见的特征点</h4>
<p><strong>Harris 角点</strong>：角点一般是对灰度图像或者二值图像进行处理，角点和边缘最大的区别就是角点在两个方向上都有较为剧烈的灰度变化，而边缘只在一个方向上有剧烈的灰度变化，如下图所示：</p>
<p><a href="http://simtalk.cn/img/%E7%89%B9%E5%BE%81%E4%B8%8E%E5%8C%B9%E9%85%8D/corner.png" target="_blank" rel="noopener"><img src="http://simtalk.cn/img/%E7%89%B9%E5%BE%81%E4%B8%8E%E5%8C%B9%E9%85%8D/corner.png" alt="img"></a></p>
<p><strong>缺点：</strong>不具有尺度不变性</p>
<p><strong>Sift (尺度不变特征变换，Scale-Invariant Feature Transform) 特征点</strong>：优势在于<strong>(尺度)缩放变换、平移变换 和 旋转变换</strong>的不变性，对光照变化、视角变化、仿射变换、噪声也保持一定程度的稳定性。<strong>缺点：</strong>计算量比较大，只利用了灰度性质算法，忽略了色彩信息。</p>
<figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">SIFT算法的流程分别为：</span><br><span class="line"><span class="comment">(1)</span> 尺度空间极点检测</span><br><span class="line"><span class="comment">(2)</span> 关键点精确定位</span><br><span class="line"><span class="comment">(3)</span> 关键点的方向确定</span><br><span class="line"><span class="comment">(4)</span> 特征向量的生成</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Reference: <a href="https://blog.csdn.net/zddblog/article/details/7521424" target="_blank" rel="noopener">1</a> <a href="https://blog.csdn.net/lhanchao/article/details/52345845" target="_blank" rel="noopener">2</a></p>
</blockquote>
<p><strong>Surf 特征点</strong>：是在Sift特征上进行的改进。主要特点是快速性，同时具有尺度不变性，对光照变化和仿射、透视变化也具有较强的鲁棒性。<strong>缺点：</strong>在求主方向阶段太依赖局部区域像素的梯度方向，有可能使得找到的主方向不准确。另外图像金字塔层取的不足够紧密也会使得尺度有误差。</p>
<p><strong>Fast 特征点</strong>：主要检测局部像素灰度变化明显的地方，以速度快著称。它的思想是：如果一个像素与它邻域的像素差别较大（过亮或过暗） , 那它更可能是角点。相比于其他角点检测算法， FAST 只需比较像素亮度的大小，十分快捷。<strong>缺点：</strong>特征点数目很大且不缺定，并且Fast角点不具有方向信息。</p>
<p><strong>ORB 特征点：</strong> 其是对FAST特征点与BREIF特征描述子的一种结合与改进，具体算法步骤如下：</p>
<ol style="list-style-type: decimal">
<li>利用FAST特征点检测的方法来检测特征点</li>
<li>利用Harris角点的度量方法，从FAST特征点从挑选出Harris角点响应值最大的N个特征点。</li>
</ol>
<p>为FAST特征添加尺度不变性和旋转不变性，用BRIEF特征作为特征描述方法。速度最快，可用于实时性特征检测。</p>
<blockquote>
<p>Reference : <a href="http://simtalk.cn/2017/08/18/%E7%89%B9%E5%BE%81%E4%B8%8E%E5%8C%B9%E9%85%8D/#ORB" target="_blank" rel="noopener">特征匹配</a></p>
</blockquote>
<hr>
<h4 id="随机洗牌法">随机洗牌法</h4>
<h4 id="蓄水池算法">蓄水池算法</h4>
<blockquote>
<p>Reference: <a href="https://www.cnblogs.com/snowInPluto/p/5996269.html" target="_blank" rel="noopener">1</a> <a href="https://blog.csdn.net/huagong_adu/article/details/7619665#commentsedit" target="_blank" rel="noopener">2</a></p>
</blockquote>
<p><strong>原理</strong> ：直接先看代码！</p>
<p>假设数据序列的规模为 <span class="math inline">\(n\)</span>，需要采样的数量的为 <span class="math inline">\(k\)</span><br>
首先构建一个可容纳 <span class="math inline">\(k\)</span> 个元素的数组，将序列的前 <span class="math inline">\(k\)</span> 个元素放入数组中。<br>
然后从第 <span class="math inline">\(k+1\)</span> 个元素开始，以 <span class="math inline">\(k/j\)</span> <span class="math inline">\((其中,j &gt; k)\)</span> 的概率来决定该元素是否被替换到数组中（数组中的元素被替换的概率是相同的）。 当遍历完所有元素之后，数组中剩下的元素即为所需采取的样本。</p>
<p><strong>证明</strong></p>
<blockquote>
<p>记忆：第m个元素被选中的概率为：<span class="math inline">\(\frac{k}{m}\)</span> （这里的 <span class="math inline">\(ｍ\)</span> 你可以替换成下面 <span class="math inline">\(i, j\)</span> 来理解）</p>
</blockquote>
<p>（1）对于第 <span class="math inline">\(i\)</span> 个数（<span class="math inline">\(i≤k\)</span>）。在 <span class="math inline">\(k\)</span> 步之前，被选中的概率为 1。当走到第 <span class="math inline">\(k+1\)</span> 步时，第 <span class="math inline">\(i\)</span> 个元素被 <span class="math inline">\(k+1\)</span> 个元素替换的概率 = <span class="math inline">\(k+1\)</span> 个元素被选中的概率 * <span class="math inline">\(i\)</span> 被选中替换的概率，即为 <span class="math inline">\(\frac{k}{k+1} \times \frac{1}{k}=\frac{1}{k+1}\)</span>。则被保留的概率为 <span class="math inline">\(1-\frac{1}{k+1}=\frac{k}{k+1}\)</span>。依次类推，不被 <span class="math inline">\(k+2\)</span> 个元素替换的概率为 <span class="math inline">\(1-\frac{k}{k+2} \times \frac{1}{k}=\frac{k+1}{k+2}\)</span> 。则运行到第 <span class="math inline">\(n\)</span> 步时，第 <span class="math inline">\(i\)</span> 个数被保留的概率 = 被选中的概率 * 不被替换的概率，即：<br>
<span class="math display">\[
1 \times \frac{k}{k+1} \times \frac{k+1}{k+2} \times \frac{k+2}{k+3} \times \ldots \times \frac{n-1}{n}=\frac{k}{n}
\]</span><br>
（2）对于第 <span class="math inline">\(j\)</span> 个数（<span class="math inline">\(j&gt;k\)</span>）。在第 <span class="math inline">\(j\)</span> 步被选中的概率为 <span class="math inline">\(\frac{k}{j}\)</span>，之后判断后面的数是否会替换该第 <span class="math inline">\(j\)</span> 个数。第 <span class="math inline">\(j\)</span> 个数不被 <span class="math inline">\(j+1\)</span> 个元素替换的概率为 <span class="math inline">\(1-\frac{k}{j+1} \times \frac{1}{k}=\frac{j}{j+1}\)</span>。则运行到第 <span class="math inline">\(n\)</span> 步时，被保留的概率 = 被选中的概率 * 不被替换的概率，即：<br>
<span class="math display">\[
\frac{k}{j} \times \frac{j}{j+1} \times \frac{j+1}{j+2} \times \frac{j+2}{j+3} \times \ldots \times \frac{n-1}{n}=\frac{k}{n}
\]</span><br>
所以对于其中每个元素，被保留的概率都为 <span class="math inline">\(\frac{k}{n}\)</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reservoirSampling</span><span class="params">(seq, k)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    蓄水池算法：先从 n 个元素中取前 k 个，然后对后面的(k+1 ~ n)个数，每个以 k / j 的概率替换前面的</span></span><br><span class="line"><span class="string">    前 k 个数，这样我们得到的 k 个数就能保证是等概率 k / n 取出的</span></span><br><span class="line"><span class="string">    (结合代码和笔记理解)</span></span><br><span class="line"><span class="string">    :param seq: 长度为n的序列</span></span><br><span class="line"><span class="string">    :param k:   int</span></span><br><span class="line"><span class="string">    :return:    返回等概论取的 n 个序列</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    local_seq = copy.deepcopy(seq)  <span class="comment"># 深拷贝</span></span><br><span class="line">    n = len(local_seq)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(k, n):           <span class="comment"># 对后面的(n - k)个数，每次以（k / j）的概率的替换前k个数</span></span><br><span class="line">        m = int(random.uniform(<span class="number">0</span>, j))</span><br><span class="line">        <span class="keyword">if</span> m &lt; k:</span><br><span class="line">            temp = copy.deepcopy(local_seq[m])</span><br><span class="line">            local_seq[m] = copy.deepcopy(local_seq[j])</span><br><span class="line">            local_seq[j] = temp</span><br><span class="line">    <span class="keyword">return</span> local_seq[<span class="number">0</span>:k]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    seq = [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">7</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">9</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">45</span>, <span class="number">3</span>, <span class="number">23</span>, <span class="number">44</span>, <span class="number">55</span>, <span class="number">33</span>, <span class="number">5</span>, <span class="number">8</span>]</span><br><span class="line">    k = <span class="number">5</span></span><br><span class="line">    print(reservoirSampling(seq, k))</span><br></pre></td></tr></table></figure>
<hr>
<h4 id="广义逆阵"><a href="https://zh.wikipedia.org/wiki/%E5%B9%BF%E4%B9%89%E9%80%86%E9%98%B5" target="_blank" rel="noopener">广义逆阵</a></h4>
<p>关于矩阵的广义逆，下列表述不正确的是：</p>
<ul>
<li><strong>提出广义逆阵的原因</strong></li>
</ul>
<p>考虑以下的线性方程<br>
<span class="math display">\[
Ax = y
\]</span><br>
其中 <span class="math inline">\(A\)</span> 为 <span class="math inline">\(n \times m\)</span> 的矩阵，而 <span class="math inline">\(y \in \mathcal{R}(A), A\)</span> 的列空间。若矩阵 <span class="math inline">\(A\)</span> 为 <span class="math inline">\(A\)</span> 为可逆矩阵，则 <span class="math inline">\(x = A^{-1}y\)</span> 即为方程式的解。而若矩阵 <span class="math inline">\(A\)</span> 为可逆矩阵，则有：<br>
<span class="math display">\[
AA^{-1}A = A
\]</span><br>
假设矩阵 <span class="math inline">\(A\)</span> 不是可逆或是 <span class="math inline">\(n \neq m\)</span> ，需要一个合适的 <span class="math inline">\(m \times n\)</span> 矩阵 <span class="math inline">\(G\)</span> 使得下式成立：<br>
<span class="math display">\[
AGy = y
\]</span><br>
因为 <span class="math inline">\(Gy\)</span> 为线性系统 <span class="math inline">\(Ax = y\)</span> 的解。而同样的， <span class="math inline">\(m \times n\)</span> 的阶的矩阵 <span class="math inline">\(G\)</span> 也会使下式成立：<br>
<span class="math display">\[
AGA = A
\]</span><br>
因此可以用以下的方式定义<strong>广义逆阵</strong>：假设一个 <span class="math inline">\(n \times m\)</span> 的矩阵 <span class="math inline">\(A\)</span> ，<span class="math inline">\(m \times n\)</span> 的矩阵 <span class="math inline">\(G\)</span> 若可以使下式成立，矩阵 <span class="math inline">\(G\)</span> 即为 <span class="math inline">\(A\)</span> 的广义逆阵。<br>
<span class="math display">\[
AGA = A
\]</span></p>
<ul>
<li><strong>产生广义逆阵</strong></li>
</ul>
<p>以下是一种产生广义逆阵的方式[3]：</p>
<ol style="list-style-type: decimal">
<li>若 <span class="math inline">\(A=BC\)</span> 为其秩分解，则 <span class="math inline">\(G=C_{r}^{-}B_{l}^{-}\)</span> 为 <span class="math inline">\(A\)</span> 的广义逆阵，其中 <span class="math inline">\(C_{r}^{-}\)</span> 为 <span class="math inline">\(C\)</span> 的右逆矩阵，而 <span class="math inline">\(B_{l}^{-}\)</span> 为 <span class="math inline">\(B\)</span> 的左逆矩阵。</li>
<li>若 <span class="math inline">\(A=P{\begin{bmatrix}I_{r}&amp;0\\0&amp;0\end{bmatrix}}Q\)</span> ，其中 <span class="math inline">\(P\)</span> 及 <span class="math inline">\(Q\)</span> 为可逆矩阵，则 <span class="math inline">\(G=Q^{-1}{\begin{bmatrix}I_{r}&amp;U\\W&amp;V\end{bmatrix}}P^{-1}\)</span> 是 <span class="math inline">\(A\)</span> 的广义逆阵，其中 <span class="math inline">\(U,V\)</span> 及 <span class="math inline">\(W\)</span> 均为任意矩阵。</li>
<li>令 <span class="math inline">\(A\)</span> 为秩为 <span class="math inline">\(r\)</span> 的矩阵，在不失一般性的情形下，令 <span class="math inline">\(A={\begin{bmatrix}B&amp;C\\D&amp;E\end{bmatrix}}\)</span> ，其中 <span class="math inline">\(B_{r\times r}\)</span> 为 <span class="math inline">\(A\)</span> 的可逆子矩阵，则 <span class="math inline">\(G={\begin{bmatrix}B^{-1}&amp;0\\0&amp;0\end{bmatrix}}\)</span> 为 <span class="math inline">\(A\)</span> 的广义逆阵。</li>
</ol>
<ul>
<li><strong>广义逆阵的种类</strong></li>
</ul>
<p>彭若斯条件可以用来定义不同的广义逆阵：针对 <span class="math inline">\(A \in \mathbb{R}^{n \times m} 及 A^{\mathrm{g}} \in \mathbb{R}^{m \times n}\)</span> ，<br>
<span class="math display">\[
\begin{array}{l}{\text { 1.) } A A^{\mathrm{g}} A=A} \\ {\text { 2.) } A^{\mathrm{g}} A A^{\mathrm{g}}=A^{\mathrm{g}}} \\ {\text { 3.) }\left(A A^{\mathrm{g}}\right)^{\mathrm{T}}=A A^{\mathrm{g}}} \\ {\text { 4.) }\left(A^{\mathrm{g}} A\right)^{\mathrm{T}}=A^{\mathrm{g}} A}\end{array}
\]</span><br>
若 <span class="math inline">\(A^{\mathrm{g}}\)</span> 满足条件(1.)，即为 <span class="math inline">\(A\)</span> 的广义逆阵，若满足条件 (1.) 和 (2.)，则为 <span class="math inline">\(A\)</span> 的广义反身逆阵。若四个条件都满足，则为 <span class="math inline">\(A\)</span> 的<a href="https://zh.wikipedia.org/wiki/%E6%91%A9%E5%B0%94%EF%BC%8D%E5%BD%AD%E8%8B%A5%E6%96%AF%E5%B9%BF%E4%B9%89%E9%80%86" target="_blank" rel="noopener">摩尔－彭若斯广义逆</a>。</p>
<hr>
<h4 id="海量数据处理问题">海量数据处理问题</h4>
<blockquote>
<p>Reference: <a href="https://blog.csdn.net/v_july_v/article/details/7382693" target="_blank" rel="noopener">0</a> <a href="https://blog.csdn.net/v_JULY_v/article/details/6256463" target="_blank" rel="noopener">1</a> <a href="https://blog.csdn.net/v_JULY_v/article/details/6279498" target="_blank" rel="noopener">2</a> <a href="https://my.oschina.net/hunglish/blog/741710" target="_blank" rel="noopener">4</a> <a href="https://juejin.im/entry/5a27cb796fb9a045104a5e8c" target="_blank" rel="noopener">3</a> <a href="https://github.com/linw7/Skill-Tree/blob/master/%E6%B5%B7%E9%87%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86.md" target="_blank" rel="noopener">5</a></p>
</blockquote>
<h5 id="一top-k问题">一、top K问题</h5>
<p>在大规模数据处理中，经常会遇到的一类问题：</p>
<ul>
<li>从海量数据中找出最大的前k个数</li>
<li>或者在海量数据中找出出现频率最高的前k个数</li>
</ul>
<p>这类问题通常被称为 <strong>top K问题</strong>。例如，在搜索引擎中，统计搜索最热门的10个查询词；在歌曲库中统计下载最高的前10首歌等。</p>
<blockquote>
<p>处理方法</p>
</blockquote>
<p>针对top K类问题，通常比较好的方案是：<strong>分治/hash映射 + Trie树/hash统计 + 小顶堆</strong>：说白了，就是 <span class="math inline">\({\color{red}先映射，而后统计，最后排序。}\)</span><br>
即先将数据集按照hash方法分解成多个小数据集，然后使用Trie树 或者 hash统计每个小数据集中的query词频，之后用小顶堆求出每个数据集中出现频率最高的前K个数，最后在所有top K中求出最终的top K。</p>
<blockquote>
<p>面试例题</p>
</blockquote>
<p><strong>(1) 10亿个数中找出最大的10000个数</strong></p>
<p>先拿10000个数建小根堆，然后一次次添加剩余的元素，如果大于堆顶的数（10000中最小的），将这个数替换堆顶，并调整结构使之任然是一个小根堆，这样，遍历完后，堆中的10000个数就是所需的最大的10000个。构建堆时间复杂度是 <span class="math inline">\(O(m)\)</span> ，调整堆的时间复杂度为 <span class="math inline">\(O(logm)\)</span>，所以总的时间复杂度为：1次建堆 + n 次调整 = <span class="math inline">\(O(m + nlogm) = {\color{red}{O(nlogm)}}\)</span>（ <span class="math inline">\(n\)</span> 为10亿，<span class="math inline">\(m\)</span> 为10000）</p>
<p><strong>优化方法：</strong>可以把所有10亿个数据分组存放，比如分别放在1000个文件中。这样处理就可以分别在每个文件的 <span class="math inline">\(10^6\)</span> 个数据中找出最大的10000个数，合并到一起在再找出最终的结果。<br>
分治后再统计的时间复杂度依然是 <span class="math inline">\(O(nlogm)\)</span> = 1000个文件找每个文件的前10000个的时间复杂度 <span class="math inline">\(O(1000 * 10^6log10000)\)</span> + 加上1000个前10000个找最后的前10000个 <span class="math inline">\(O(1000*10000*log10000)\)</span> = <span class="math inline">\(O(10^9log10000)\)</span> = <span class="math inline">\(O(nlogm)\)</span></p>
<p><strong>(2) 10亿个数中找出频率最高的10000个数</strong></p>
<p>采用分治思想，先将10亿个数做hash求模映射到1000个小文件（<span class="math inline">\(hash(x)\)</span> % 1000 得到该数 <span class="math inline">\(x\)</span> 对应放在哪个文件 <a href="https://blog.csdn.net/v_JULY_v/article/details/6256463" target="_blank" rel="noopener">*</a>），使用 <span class="math inline">\(hash\_map\)</span> 统计每个文件中各个数出现的频率，然后再使用小根堆的方法，求出每个小文件中出现频率最高的10000个，最后求出所有文件频率最高的10000个。</p>
<p><strong>(3) 海量日志数据，提取出某日访问百度次数最多的那个IP</strong></p>
<p>百度作为国内第一大搜索引擎，每天访问它的IP数量巨大，如果想一次性把所有IP数据装进内存处理，则内存容量明显不够，故针对数据太大，内存受限的情况，可以把大文件转化成（取模映射）小文件，从而大而化小，逐个处理。</p>
<ul>
<li>分而治之/hash映射</li>
</ul>
<p>首先把这一天访问百度日志的所有IP提取出来，然后逐个写入到一个大文件中，接着采用映射的方法，比如hash(IP) %1000，把整个大文件映射为1000个小文件，同一个IP在hash后，落在同一个文件中。</p>
<ul>
<li>hash_map统计</li>
</ul>
<p>然后使用hash_map(ip, value)来分别对1000个小文件中的IP进行频率统计，再找出每个小文件中出现频率最大的IP。</p>
<ul>
<li>堆/快速排序</li>
</ul>
<p>统计出最后1000个频率最大的IP后，依据各自频率的大小进行排序(可采取堆排序)，找出那个频率最大的IP，即为所求. 总共的时间复杂度为：<span class="math inline">\(O(nlogm)\)</span></p>
<p><strong>(4) 有一千万个记录（这些查询串的重复度比较高，虽然总数是1千万，但如果除去重复后，不超过3百万个。一个查询串的重复度越高，说明查询它的用户越多，也就是越热门），请你统计最热门的10个查询串，要求使用的内存不能超过1G</strong></p>
<p>对于本题，数据规模比较小，能一次性装入内存，因为根据题目描述，虽然有一千万个Query，但是由于重复度比较高，故去除重复后，事实上只有300万的Query，每个Query255Byte，因此我们可以考虑把他们都放进内存中去（300万个字符串假设没有重复，都是最大长度，那么最多占用内存3M*1K/4=0.75G。所以可以将所有字符串都存放在内存中进行处理）</p>
<p>我们直接使用 hash_map(Query, Value) 进行统计，时间复杂度为O(n)，然后再使用之前的小根堆方式，得到频率最高的 10个查询串。时间负责度为 <span class="math inline">\(O(10^6log10)\)</span></p>
<p><strong>(5) 有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词</strong></p>
<p>顺序读文件中，对于每个词x，取hash(x)%5000，然后按照该值存到5000个小文件（记为x0,x1,…x4999）中。这样每个文件大概是200k左右。对超过1M的文件继续分割直到小于200K。使用hash_map统计各个词出现的频率，或者使用Trie数（时间复杂度为O(nle) :le为平均字符串长度）。对5000词使用堆排序或归并排序。</p>
<h5 id="二分布式-top-k问题">二、分布式 top K问题</h5>
<p><strong>(6) 分布在100台电脑的海量数据，统计前10</strong></p>
<blockquote>
<p>如果同一个数据元素只出现在某一台机器中</p>
</blockquote>
<ul>
<li><strong>堆排序：</strong>在每台电脑上求出Top 10，构造包含10个元素的最小堆，然后同 (1)。</li>
<li><strong>组合归并：</strong>然后将这100台电脑的Top 10组合起来，共1000个数据，再使用上面的方法求出 Top10即可，和上面说的都是一样的。</li>
</ul>
<blockquote>
<p>如果同一个元素重复出现在不同的电脑中<br>
比如：第一台机器数据分布及各自出现的频率为：a(50)，b(50)，c(49)，d(49) ，e(0)，f(0)<br>
​ 第一台机器数据分布及各自出现的频率为：a(0)，b(0)，c(49)，d(49)，e(50)，f(50)</p>
</blockquote>
<p>方法一：</p>
<ul>
<li>遍历所有数据，重新hash取模，使同一个元素只出现在单独的一台电脑中，然后采用上面方法先统计每台电脑Top10再汇总起来求最终的Top10</li>
</ul>
<p>方法二：</p>
<ul>
<li>暴力求解：直接统计统计每台电脑中各个元素的出现次数，然后把同一个元素在不同机器中的出现次数相加，最终从所有数据中找出Top10</li>
</ul>
<h5 id="三公共数据问题">三、公共数据问题</h5>
<p><strong>(7) 给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url</strong></p>
<p>可以估计每个文件的大小为5G×64=320G，远远大于内存限制的4G。所以不可能将其完全加载到内存中处理。考虑采取分而治之的方法。</p>
<ul>
<li>分而治之/hash映射：遍历文件a，对每个url求取 hash(url)%1000，然后根据所取得的值将url分别存储到1000个小文件中（记为<span class="math inline">\(a_0, a_1, ..., a_{999}\)</span>）这样每个小文件的大约为300M。遍历文件b，采取和a相同的方式将url分别存储到1000小文件中（记为 <span class="math inline">\(b_0, b_1, ... , b_{999}\)</span>）。这样处理后，所有可能相同的url都在对应的小文件（ <span class="math inline">\(a_{0}\)</span> vs <span class="math inline">\(b_{0}, a_{1}\)</span> vs <span class="math inline">\(b_{1}, \ldots, a_{0}\)</span> vs <span class="math inline">\(b_{999}\)</span>）中，不对应的小文件不可能有相同的url。然后我们只要求出1000对小文件中相同的url即可。</li>
<li>hash_set统计：求每对小文件中相同的url时，可以把其中一个小文件的url存储到hash_set中。然后遍历另一个小文件的每个url，看其是否在刚才构建的hash_set中，如果是，那么就是共同的url，存到文件里面就可以了。</li>
</ul>
<p><strong>(8) 1000w有重字符串，对字符串去重</strong></p>
<ul>
<li>先hash分为多个文件</li>
<li>逐个文件检查并插入set中</li>
<li>多个set取交集</li>
</ul>
<h5 id="四位图法">四、位图法</h5>
<p><strong>(9) 在2.5亿数字中找出不重复的整数</strong></p>
<ul>
<li>使用2-Bit位图法，00表示不存在，01表示出现一次，10表示出现多次，11无意义。这样共需要<span class="math inline">\(2^{32} * 2\)</span> = 1G 内存，还可以接受。然后扫描这2.5亿个整数，查看Bitmap中相对应位，如果是00变01，01变10，10保持不变。所描完事后，查看bitmap，把对应位是01的整数输出即可。</li>
<li>或者hash划分小文件，小文件使用hash_set检查各个元素，得到的。</li>
</ul>
<p><strong>(10) 给40亿个不重复的unsigned int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？</strong></p>
<ul>
<li>位图法：申请 <span class="math inline">\(2^{32} = 512M\)</span> 的内存，一个bit位代表一个unsigned int值。读入40亿个数，设置相应的 bit位，读入要查询的数，查看相应 bit 位是否为1，为1表示存在，为0表示不存在。</li>
</ul>
<h5 id="五trie树">五、<strong>Trie树</strong></h5>
<p>字典树、前缀树、单词查找树或键树。Trie树的核心思想是空间换时间。利用字符串的公共前缀来降低查询时间的开销以达到提高效率的目的。<br>
<img src="/2018/04/07/Try-your-best!You-will-find-the-job/Trie.png"></p>
<ul>
<li><strong>基本性质</strong></li>
</ul>
<p>（1）根节点不包含字符，除根节点以外每个节点只包含一个字符。<br>
（2）从根节点到某一个节点，路径上经过的字符连接起来，为该节点对应的字符串。<br>
（3）每个节点的所有子节点包含的字符串不相同。</p>
<p>通常在实现的时候，会在节点结构中设置一个标志，用来标记该结点处是否构成一个单词（关键字）。Trie树的关键字一般都是字符串，而且<strong>Trie树把每个关键字保存在一条路径上</strong>，而不是一个结点中。</p>
<ul>
<li><strong>优点</strong></li>
</ul>
<p>（1）插入和查询的效率很高，都为O(m)，其中 m 是待插入/查询的字符串的长度。<br>
（2）Trie树中不同的关键字不会产生冲突。<br>
（3）Trie树只有在允许一个关键字关联多个值的情况下才有类似hash碰撞发生。<br>
（4）Trie树可以对关键字按<strong>字典序</strong>排序。</p>
<ul>
<li><strong>缺点</strong></li>
</ul>
<p>（1）空间消耗比较大<br>
（2）当 hash 函数很好时，Trie树的查找效率会低于哈希搜索</p>
<ul>
<li><strong>插入过程</strong></li>
</ul>
<p>对于一个单词，从根开始，沿着单词的各个字母所对应的树中的节点分支向下走，直到单词遍历完，将最后的节点exist 标记为 true，表示该单词已近插入Trie树。时间复杂度为 <strong>O(m)</strong>：m为单词长度</p>
<ul>
<li><strong>查找过程</strong></li>
</ul>
<p>同样的，从根开始按照单词的字母顺序向下遍历trie树，一旦发现某个节点标记不存在或者单词遍历完成而最后的节点未标记为exist=true，则表示该单词不存在，若最后的节点标记为exist=true，表示该单词存在。时间复杂度为 <strong>O(m)</strong>：m为单词长度</p>
<ul>
<li><strong>使用范围</strong>：</li>
</ul>
<p><strong>1 . 字符串检索</strong></p>
<p><strong>思路</strong>就是从根节点开始一个一个字符进行比较<br>
（1）如果沿路比较，发现不同的字符，则表示该字符串不在该集合中。<br>
（2）如果所有的字符全部比较完并且全部相同，还需判断最后一个节点的标志位（标记该节点是否代表一个关键字）。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">trie_node</span> &#123;</span></span><br><span class="line">    <span class="keyword">bool</span> isKey;   <span class="comment">// 标记该节点是否代表一个关键字</span></span><br><span class="line">    trie_node *children[<span class="number">26</span>]; <span class="comment">// 各个子节点 </span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p><strong>2 . 词频统计</strong></p>
<p>思路：为了实现词频统计，我们修改了节点结构，用一个整型变量 <code>count</code> 来计数。对每一个关键字执行插入操作，若已存在，计数加1，若不存在，插入后 <code>count</code> 置1</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">trie_node</span> &#123;</span></span><br><span class="line">    <span class="keyword">int</span> count;   <span class="comment">// 记录该节点代表的单词的个数</span></span><br><span class="line">    trie_node *children[<span class="number">26</span>]; <span class="comment">// 各个子节点 </span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p><strong>3 . 字符串排序</strong></p>
<p>Trie树可以对大量字符串按字典序进行排序，思路也很简单：遍历一次所有关键字，将它们全部插入trie树，树的每个结点的所有儿子很显然地按照字母表排序，然后先序遍历输出Trie树中所有关键字即可。</p>
<p><strong>4 . 前缀匹配</strong></p>
<p>例如：找出一个字符串集合中所有以<code>ab</code>开头的字符串。我们只需要用所有字符串构造一个trie树，然后输出以<code>a-&gt;b-&gt;</code>开头的路径上的关键字即可。</p>
<p>trie树前缀匹配常用于搜索提示。如当输入一个网址，可以自动搜索出可能的选择。当没有完全匹配的搜索结果，可以返回前缀最相似的可能。</p>
<ul>
<li><strong>问题实例</strong></li>
</ul>
<blockquote>
<p>Reference: <a href="https://blog.csdn.net/Hackbuteer1/article/details/7964147" target="_blank" rel="noopener">1</a> <a href="https://www.cnblogs.com/huangxincheng/archive/2012/11/25/2788268.html" target="_blank" rel="noopener">2</a> <a href="https://blog.csdn.net/lisonglisonglisong/article/details/45584721" target="_blank" rel="noopener">3</a> <a href="https://wizardforcel.gitbooks.io/the-art-of-programming-by-july/content/06.09.html" target="_blank" rel="noopener">4</a></p>
</blockquote>
<hr>
<h3 id="面试算法题目">面试算法题目</h3>
<ul>
<li><p>给定一个词典，和两个单词A, B. 每次只能改变一个字母，求A在词典中变换为B所需的最小次数</p></li>
<li><p>数组形式的<code>a[i][j][k]</code> 改用指针形式来访问</p></li>
<li><p>有序数组，旋转后查找一个数</p></li>
<li><p>判断一个数是不是2的指数次方的值</p></li>
</ul>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">如果是<span class="number">2</span>的倍数，那么该数的二进制位只有<span class="number">1</span>位为<span class="number">1</span>，所以只需要使用 n =  n &amp; (n - <span class="number">1</span>) 即可，其可以掉最右边的一个<span class="number">1</span>，若此时，n为<span class="number">0</span>，则符合，否则，不符合。</span><br></pre></td></tr></table></figure>
<ul>
<li>最长公共子串 (商汤)</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>找到一个数组中唯一一个出现奇数次数的数（商汤）</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>给两个有序数组，求第3大的数（头条）</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="概率智力题">概率智力题</h3>
<ul>
<li>两个人拿金币，一次只能拿一个或者两个，最后拿光的人赢，怎样才能保证赢？</li>
</ul>
<blockquote>
<p>思想：这类题一般从后往前推，确定剩下多少个物品时，自己一定会赢，然后制定相应的策略。</p>
</blockquote>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">假设总共有<span class="number">50</span>块金币，A要获胜，就必须要保证他在倒数第二次拿到金币之后还剩下<span class="number">3</span>个，这样无论B拿<span class="number">1</span>个还是<span class="number">2</span>个，最后一个都是A拿到。A要保证最后剩<span class="number">3</span>个，则A需要第一拿金币后，剩下的金币是<span class="number">3</span>的倍数，这样A可以控制每次A和B拿的金币之和为<span class="number">3</span>，最后一定会剩下<span class="number">3</span>个。所以<span class="number">50</span>块金币，A只需刚开始拿<span class="number">2</span>个，然后每次保证以后每次拿的个数与B拿的个数之和为<span class="number">3</span>即可。</span><br></pre></td></tr></table></figure>
<ul>
<li>A、B两个人从一堆玻璃球（共100个）中向外拿球，规则如下：① A先拿，然后一人一次交替进行；② 每次只能拿1个、2个或者4个。③ 谁拿最后一个球，谁就是最后的失败者。请问，A、B两个人谁将是失败者？</li>
</ul>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">和上一题类似，由于<span class="number">100</span> = <span class="number">16</span>*<span class="number">5</span>+<span class="number">4</span>，由于A先拿，B只要保证最后剩下的是<span class="number">1</span>个或者<span class="number">4</span>个，那么最后一个就一定是A拿。具体的策略是B要保证每次A和B拿玻璃球的个数之和为<span class="number">3</span>个或者<span class="number">6</span>个，这样B拿玻璃球之后剩下的玻璃球就是<span class="number">1</span>个或者<span class="number">4</span>个。所以A一定输。</span><br></pre></td></tr></table></figure>
<ul>
<li>有8瓶试剂，有一个有毒，用几只老鼠可以找出？</li>
</ul>
<blockquote>
<p>方法一：二分思想</p>
</blockquote>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">每次把试剂分成两份[<span class="number">4</span>, <span class="number">4</span>]，然后用两只老鼠来测试，试剂喝一小口就行，如果其中一只死掉了，就可以确定哪一堆有毒，然后继续将没毒的分成两份[<span class="number">2</span>, <span class="number">2</span>]，用两只老鼠吃。<span class="number">8</span> = <span class="number">2</span>^<span class="number">3</span>，所以只需要分<span class="number">3</span>次，即<span class="number">3</span>只老鼠就可以确定哪瓶试剂有毒。</span><br></pre></td></tr></table></figure>
<blockquote>
<p>方法二：位操作</p>
</blockquote>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">8</span> = <span class="number">2</span>^<span class="number">3</span>，按 _ _ _ 位来表示可以有<span class="number">8</span>种状态。然后从低到高进行排列，用<span class="number">0</span><span class="number">-7</span>表示<span class="number">8</span>瓶试剂，<span class="number">3</span>位中每位表示一个老鼠。</span><br><span class="line"><span class="comment">// [3, 2, 1]号老鼠</span></span><br><span class="line"><span class="number">000</span> = <span class="number">0</span> <span class="comment">// 第0号试剂</span></span><br><span class="line"><span class="number">001</span> = <span class="number">1</span></span><br><span class="line"><span class="number">010</span> = <span class="number">2</span></span><br><span class="line"><span class="number">011</span> = <span class="number">3</span></span><br><span class="line"><span class="number">100</span> = <span class="number">4</span></span><br><span class="line"><span class="number">101</span> = <span class="number">5</span></span><br><span class="line"><span class="number">110</span> = <span class="number">6</span></span><br><span class="line"><span class="number">111</span> = <span class="number">7</span></span><br><span class="line">然后我们每一列看，即按照每个老鼠看，比如：</span><br><span class="line"><span class="number">1</span>号老鼠，我们就将<span class="number">1</span>、<span class="number">3</span>、<span class="number">5</span>、<span class="number">7</span>号试剂混合给<span class="number">1</span>号老鼠喝；</span><br><span class="line"><span class="number">2</span>号老鼠，我们就将<span class="number">2</span>、<span class="number">3</span>、<span class="number">6</span>、<span class="number">7</span>号试剂混合给<span class="number">2</span>号老鼠喝；</span><br><span class="line"><span class="number">3</span>号老鼠，我们就将<span class="number">4</span>、<span class="number">5</span>、<span class="number">6</span>、<span class="number">7</span>号试剂混合给<span class="number">2</span>号老鼠喝；</span><br><span class="line">最后，如果<span class="number">1</span>,<span class="number">3</span>号老鼠死了，<span class="number">2</span>号活着，我们对应找到<span class="number">101</span>，即第<span class="number">5</span>号试剂有毒（因为<span class="number">1</span>,<span class="number">3</span>号老鼠都吃了第<span class="number">5</span>号试剂）。</span><br><span class="line">如果<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>号老鼠都死了，则对应找到<span class="number">111</span>，即<span class="number">7</span>号试剂有毒。</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意，如果这里有时间限制，比如，吃了毒试剂要24小时后才会死，那么第一种方法则需要3天，第2种方法只需要1天即可</p>
</blockquote>
<hr>
<h3 id="待整理">待整理</h3>
<ul>
<li>PCA 的两种解释</li>
<li>降为作用</li>
<li>另一种作用</li>
<li>SVM中的核函数有哪些</li>
<li><p>SVM中的核函数你用过么，存在的问题是什么</p></li>
<li>1000个样本，100维度，使用SVM计算内存占用情况</li>
<li>当数据的维度很大的，使用SVM会运行的很慢，你会如何处理</li>
<li>核函数的作用（自己要补充！）</li>
<li>核函数的引入避免了“维数灾难”，大大减小了计算量。</li>
<li>无需知道非线性变换函数Φ的形式和参数。</li>
<li><p>集成学习知道么</p></li>
</ul>
<p>自己讲了一下 Boosting方法和Bagging方法</p>
<ul>
<li>Kaggle比赛介绍一下</li>
<li>这种表格型比赛具体流程是怎么样的</li>
<li>错误错误结果如何分析</li>
<li>训练集和测试集如何划分的</li>
<li>介绍一下 主流 网络发展</li>
</ul>
<h4 id="矩阵相乘最小乘法次数">矩阵相乘最小乘法次数</h4>
<h4 id="svm-和-softmax-区别">SVM 和 Softmax 区别</h4>
<h4 id="仿射变换和透视变化区别">仿射变换和透视变化区别</h4>
<h4 id="透视变换矩阵的形状">透视变换矩阵的形状</h4>
<h4 id="svmlr-和-softmax-区别">SVM、LR 和 Softmax 区别</h4>
<h4 id="svm寻参问题">SVM寻参问题</h4>
<h4 id="svm核函数解释">SVM核函数解释</h4>
<h4 id="交叉熵损失-可以用-l2损失替换么">交叉熵损失 可以用 L2损失替换么</h4>
<h4 id="先把yolo看完500问中有检测论文需要看的顺序">先把YOLO看完（500问中有检测论文需要看的顺序）</h4>
<h4 id="简述-yolo-和-ssd">简述 YOLO 和 SSD</h4>
<h4 id="yolov2"><a href="https://zhuanlan.zhihu.com/p/25167153" target="_blank" rel="noopener">YOLOv2</a></h4>
<h4 id="batch_size-和-learning-rate的关系怎么平衡和调整二者">batch_size 和 learning rate的关系（怎么平衡和调整二者）</h4>
<h4 id="inception-v1-v4的演变">Inception v1-v4的演变</h4>
<h4 id="简述-cnn-的演变">简述 CNN 的演变</h4>
<h4 id="roi-pooling-和-roi-align">ROI Pooling 和 ROI Align</h4>
<h4 id="cnn为什么有效-1-2">CNN为什么有效 <a href="https://lguduy.github.io/2017/07/02/CNN%E4%B8%BA%E4%BB%80%E4%B9%88work/" target="_blank" rel="noopener">1</a> <a href="https://www.zhihu.com/question/39022858" target="_blank" rel="noopener">2</a></h4>
<h4 id="cnn在图像上表现好的原因">CNN在图像上表现好的原因</h4>
<h4 id="对迁移学习的理解为什么能work">对迁移学习的理解，为什么能work</h4>
<h4 id="实现一个卷积操作-另一个简单实现卷积操作"><a href="https://blog.csdn.net/huachao1001/article/details/79120521" target="_blank" rel="noopener">实现一个卷积操作</a> <a href="https://zhuanlan.zhihu.com/p/31575074" target="_blank" rel="noopener">另一个简单实现卷积操作</a></h4>
<h4 id="模型精简加速">模型精简加速</h4>
<h4 id="图像处理中的常用算子"><a href="https://blog.csdn.net/yyywww666/article/details/78117595" target="_blank" rel="noopener">图像处理中的常用算子</a></h4>
<h4 id="卷积与反卷积">卷积与反卷积</h4>
<h4 id="lstm-与-rnn-模型">LSTM 与 RNN 模型</h4>
<h4 id="lstm的结构其相对于rnn的好处">LSTM的结构，其相对于RNN的好处</h4>
<h4 id="bn原理与实战"><a href="https://zhuanlan.zhihu.com/p/34879333" target="_blank" rel="noopener">BN原理与实战</a></h4>
<hr>
<h3 id="面经集合">面经集合</h3>
<ul>
<li><a href="https://blog.csdn.net/liuxiao214/article/details/83043170" target="_blank" rel="noopener">超级多的面试相关题目，基本自己要都会才行！</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzUxNjcxMjQxNg==&amp;mid=2247489308&amp;idx=3&amp;sn=2ab79b30a0617acd27b4d0d008048a72&amp;chksm=f9a26593ced5ec85cde15405b917a4ad649bd3e17e1b6986220d9f0c3fb5440924e9f0ffdf9a&amp;mpshare=1&amp;scene=1&amp;srcid=&amp;pass_ticket=GNqW1dUZqNOlXRVxSJ2ZJyI8kYIhyz3BdG1xJe%2FAyL46O%2FfHd%2BkyZhc51dt3UTV1#rd" target="_blank" rel="noopener">跟自己很相关的一个面经,已经剪切到印象笔记之中</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/64080430" target="_blank" rel="noopener">面试经验贴</a></li>
<li><a href="https://blog.csdn.net/francislucien2017/article/details/87936928" target="_blank" rel="noopener">2019春 计算机视觉方向实习面试总结 （商汤 / 搜狗 / 纽劢 / 普华永道）</a></li>
<li><a href="https://www.jianshu.com/p/58855c6971e5" target="_blank" rel="noopener">2019秋招CV算法面经</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MjM5ODU3OTIyOA==&amp;mid=2650667647&amp;idx=1&amp;sn=142c2aa04e2af0f700a0c8bce08c1de0&amp;chksm=bec1c70c89b64e1a8e6fb091da6c74c152469469d2e28ee3301078602e8032eba8df75fc0b81&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">机器学习工程师面试!!!</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MjM5ODU3OTIyOA==&amp;mid=2650667685&amp;idx=1&amp;sn=b7e119cd87dd36361202aac99b73d8a9&amp;chksm=bec1c7d689b64ec0bbac4edec8b7102f38c751b21bd3789970527c13ccbb48c3403c74913180&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">深度学习面试你必须知道这些答案</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/32981626" target="_blank" rel="noopener">大佬面试总结：图像处理/CV/ML/DL到HR面</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/63755692" target="_blank" rel="noopener">SLAM面试问题</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MjM5ODU3OTIyOA==&amp;mid=2650668193&amp;idx=1&amp;sn=2efc45adea26c8ebd6efc5e14509180c&amp;chksm=bec1c1d289b648c43537fa297504f2cddf1568586da02becca515671fb095d7e591c33594c66&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">深度学习面试题目</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MjM5ODU3OTIyOA==&amp;mid=2650666049&amp;idx=1&amp;sn=94d9aca0a894418f4c66e9b5363e0498&amp;chksm=bec1c93289b640240af8f55c301a63897d0c52d3ea7b96da969ff160c17b2d52c4a04204634b&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">面试官如何判断面试者的机器学习水平？</a></li>
<li><a href="https://blog.csdn.net/yyywww666/article/details/78117595" target="_blank" rel="noopener">2017百度计算机视觉题目</a></li>
<li><a href="https://www.nowcoder.com/discuss/112562" target="_blank" rel="noopener">腾讯 MIG / 网易互娱 / 今日头条 从春招到秋招的面经</a></li>
<li><a href="https://www.nowcoder.com/discuss/128148" target="_blank" rel="noopener">计算机视觉岗位面经</a></li>
<li><a href="https://www.nowcoder.com/discuss/36815" target="_blank" rel="noopener">机器学习岗面试</a></li>
<li><a href="https://www.nowcoder.com/discuss/3453" target="_blank" rel="noopener">机器学习-相似性度量</a></li>
<li><a href="https://blog.csdn.net/zmazon/article/details/8262185" target="_blank" rel="noopener">优秀程序员不得不知道的20个位运算技巧</a></li>
<li><a href="https://blog.csdn.net/sinat_36161667/article/details/81159566" target="_blank" rel="noopener">机器学习算法工程师</a></li>
<li><a href="https://byjiang.com/2017/09/24/Interview/" target="_blank" rel="noopener">百度/腾讯/阿里等面试</a></li>
</ul>
<hr>
<h3 id="思维导图"><a href="https://blog.csdn.net/woaidapaopao/article/details/77806273" target="_blank" rel="noopener">思维导图</a></h3>
<p><a href="https://www.nowcoder.com/discuss/242172?type=post&amp;order=create&amp;pos" class="uri" target="_blank" rel="noopener">https://www.nowcoder.com/discuss/242172?type=post&amp;order=create&amp;pos</a>=&amp;page=0</p>
<p>技术面：</p>
<p>1、自我介绍</p>
<p>2、项目介绍</p>
<p>3、互斥锁和条件变量如何实现线程同步，线程通信的方式</p>
<p>4、tcp/ip分层模型，tcp为什么是可靠的，如何实现可靠传输</p>
<p>5、内存泄漏，原因及解决方法</p>
<p>6、快排的实现</p>
<p>7、数据结构，map，set底层实现</p>
<p>8、c++的四个智能指针</p>
<p>9、描述一下输入url到页面返回的过程</p>
<p>HR面：</p>
<p>1、就业方向和就业地点</p>
<p>2、不是科班为什么选择互联网方向</p>
<p>2、职业规划</p>
<p>3、期望薪资</p>
<p>4、了解苏宁吗？ 这个比较尴尬（因为不太了解，感觉添的不行）</p>
<p>晚上8点-10点笔试</p>
<p>1、技术题，选择加问答，无编程</p>
<p>2、苏宁企业文化等</p>
<p>3、行测，言语理解与表达，资料分析，逻辑推理</p>
<hr>
<p><a href="https://www.nowcoder.com/discuss/244320?type=post&amp;order=time&amp;pos" class="uri" target="_blank" rel="noopener">https://www.nowcoder.com/discuss/244320?type=post&amp;order=time&amp;pos</a>=&amp;page=1</p>
<p>9.3下午 苏宁数据挖掘算法管培生面试：</p>
<p>时间：近30min</p>
<p>形式：2V1，1技术官+1HR</p>
<p>整体总结：</p>
<p>面试比较水，只停留在简历的宏观层面（不知道对我不感兴趣，还是看我是妹子😂）。感觉面试官觉得我做的东西比较浅显，不大满意。有点小紧张，全程语速过快，该突出的点轻描淡写带过了，缺少深度、逻辑比较😂。9月底给通知。</p>
<p>一、技术面</p>
<p>. 自我介绍</p>
<p>. 重点介绍下项目，做了什么，怎么做的，具体用了什么技术/算法？</p>
<p>. 研究课题中具体使用了什么算法？</p>
<p>那些算法是我自己设计的，不是大家广知的，就简单带过了。说了数据预处理和聚类算法，真傻😂</p>
<p>. 在数据挖掘方面都了解什么算法？</p>
<p>我说了分类、聚类、关联规则、离群点检测。就问我离群点检测具体方法，额，我不知道…</p>
<p>. 了解哪些二分类算法？</p>
<p>. 说说贝叶斯算法思想？</p>
<p>HR面：</p>
<p>. 对苏宁有什么了解？</p>
<p>. 他们对苏宁有什么评价？</p>
<p>. 在苏宁的职业规划？</p>
<p>. 期望薪资是多少？</p>
<p>. 未来的定居计划？</p>
<p>. 都面了哪些公司？</p>
<p>. 你有什么想问我们的？</p>
</body>
</html>
</span>
      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Machine-Learning/" rel="tag">#Machine Learning</a>
          
            <a href="/tags/Deeping-Learning/" rel="tag">#Deeping Learning</a>
          
            <a href="/tags/job/" rel="tag">#job</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/04/08/RCNN/" rel="prev">
                <i class="fa fa-chevron-left"></i> 【目标检测】RCNN (Rich feature hierarchies for accurate object detection and semantic segmentation)
              </a>
            
          </div>

          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/04/07/Interview-preparation-of-cpp/" rel="next">
                C++面试准备 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>

 </div>

        

        
          <div class="comments" id="comments">
            
          </div>
        
      

        
          
  
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">
      
      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table Of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview" sidebar-panel >
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" src="/images/avatar.jpg" alt="SmileLingyong" itemprop="image"/>
          <p class="site-author-name" itemprop="name">SmileLingyong</p>
        </div>
        <p class="site-description motion-element" itemprop="description">向上，向阳！</p>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">32</span>
              <span class="site-state-item-name">posts</span>
            </a>
          </div>

          <div class="site-state-item site-state-categories">
            <a href="/categories">
              <span class="site-state-item-count">14</span>
              <span class="site-state-item-name">categories</span>
              </a>
          </div>

          <div class="site-state-item site-state-tags">
            <a href="/tags">
              <span class="site-state-item-count">22</span>
              <span class="site-state-item-name">tags</span>
              </a>
          </div>

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="smilelingyong@gmail.com" target="_blank">
                  <i class="fa fa-e-mail"></i> E-Mail
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://github.com/SmileLingyong" target="_blank">
                  <i class="fa fa-github"></i> Github
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://blog.csdn.net/forever__1234" target="_blank">
                  <i class="fa fa-csdn"></i> CSDN
                </a>
              </span>
            
          
        </div>

        
        

        <div class="links-of-author motion-element">
          
        </div>

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel-active">
          <div class="post-toc-indicator-top post-toc-indicator"></div>
          <div class="post-toc">
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#模型评估"><span class="nav-text">模型评估</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#模型评估常用方法"><span class="nav-text">模型评估常用方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#机器学习中的bias和variance有什么区别和联系"><span class="nav-text">机器学习中的Bias和Variance有什么区别和联系</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#经验误差与泛化误差"><span class="nav-text">经验误差与泛化误差</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#深度学习中的偏差与方差"><span class="nav-text">深度学习中的偏差与方差</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#偏差方差与boosting和bagging联系"><span class="nav-text">偏差方差与Boosting和Bagging联系</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#偏差与方差的计算公式"><span class="nav-text">偏差与方差的计算公式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#偏差与方差的权衡过拟合与模型复杂度的权衡"><span class="nav-text">偏差与方差的权衡（过拟合与模型复杂度的权衡）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#欠拟合与过拟合"><span class="nav-text">欠拟合与过拟合</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#根据不同的坐标方式图解欠拟合与过拟合"><span class="nav-text">根据不同的坐标方式，图解欠拟合与过拟合</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#降低-过拟合-的方法"><span class="nav-text">降低 过拟合 的方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#降低-欠拟合-的方法"><span class="nav-text">降低 欠拟合 的方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#l1l2-范数正则化"><span class="nav-text">L1/L2 范数正则化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#l1l2-范数的作用异同"><span class="nav-text">L1/L2 范数的作用、异同</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么-l1-和-l2-正则化可以防止过拟合"><span class="nav-text">为什么 L1 和 L2 正则化可以防止过拟合？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么-l1-正则化可以产生稀疏权值而-l2-不会"><span class="nav-text">为什么 L1 正则化可以产生稀疏权值，而 L2 不会？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#交叉验证的主要作用"><span class="nav-text">交叉验证的主要作用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#k-折交叉验证"><span class="nav-text">\(k\) 折交叉验证</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#precision和recall"><span class="nav-text">Precision和Recall</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#p-r曲线"><span class="nav-text">P-R曲线</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#roc与auc"><span class="nav-text">ROC与AUC</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#map"><span class="nav-text">mAP</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么要设置单一数字评估指标设置指标的意义"><span class="nav-text">为什么要设置单一数字评估指标，设置指标的意义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#训练验证测试集的定义及划分"><span class="nav-text">训练/验证/测试集的定义及划分</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#什么是top5错误率"><span class="nav-text">什么是TOP5错误率</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#如何通过模型重新观察数据"><span class="nav-text">如何通过模型重新观察数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#有哪些改善模型的思路"><span class="nav-text">有哪些改善模型的思路</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#激活函数"><span class="nav-text">激活函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么需要激活函数"><span class="nav-text">为什么需要激活函数？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么要使用非线性激活函数"><span class="nav-text">为什么要使用非线性激活函数？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#什么时候可以用线性激活函数"><span class="nav-text">什么时候可以用线性激活函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#常见的激活函数"><span class="nav-text">常见的激活函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么tanh收敛速度比sigmoid快"><span class="nav-text">为什么Tanh收敛速度比Sigmoid快</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#怎样理解-relu-0-时是非线性激活函数"><span class="nav-text">怎样理解 ReLU（&lt; 0 时）是非线性激活函数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#损失函数代价函数"><span class="nav-text">损失函数/代价函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么需要损失代价函数"><span class="nav-text">为什么需要损失/代价函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#损失代价函数的作用及原理"><span class="nav-text">损失/代价函数的作用及原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么损失代价函数要非负"><span class="nav-text">为什么损失/代价函数要非负</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#常用的损失函数"><span class="nav-text">常用的损失函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#softmax损失函数"><span class="nav-text">Softmax损失函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#softmax-loss损失函数"><span class="nav-text">Softmax-Loss损失函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#熵条件熵kl散度交叉熵"><span class="nav-text">熵、条件熵、KL散度、交叉熵</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么用交叉熵代替二次代价均方误差函数"><span class="nav-text">为什么用交叉熵代替二次代价(均方误差)函数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#范数"><span class="nav-text">范数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#l-p范数"><span class="nav-text">L-P范数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#l0范数"><span class="nav-text">L0范数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#l1范数"><span class="nav-text">L1范数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#l2范数"><span class="nav-text">L2范数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#l-范数"><span class="nav-text">L-∞范数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#欧式距离-与-余弦相似度"><span class="nav-text">欧式距离 与 余弦相似度</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#线性回归"><span class="nav-text">线性回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#逻辑回归"><span class="nav-text">逻辑回归</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#回归划分"><span class="nav-text">回归划分</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#逻辑回归适用性"><span class="nav-text">逻辑回归适用性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#逻辑回归与朴素贝叶斯的区别"><span class="nav-text">逻辑回归与朴素贝叶斯的区别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#逻辑回归-与-线性回归-的联系与区别"><span class="nav-text">逻辑回归 与 线性回归 的联系与区别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#逻辑回归-与-svm-的联系与区别"><span class="nav-text">逻辑回归 与 SVM 的联系与区别 *</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#逻辑回归-与-随机森林-区别"><span class="nav-text">逻辑回归 与 随机森林 区别</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#svm"><span class="nav-text">SVM</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#什么是支持向量"><span class="nav-text">什么是支持向量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#支持向量机的分类"><span class="nav-text">支持向量机的分类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#最大间隔超平面背后的原理"><span class="nav-text">最大间隔超平面背后的原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#支持向量机推导"><span class="nav-text">支持向量机推导</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#一线性可分支持向量机推导"><span class="nav-text">一、线性可分支持向量机推导</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#svm-标准问题的推导"><span class="nav-text">SVM 标准问题的推导</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#svm-对偶算法的推导"><span class="nav-text">SVM 对偶算法的推导</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#二线性支持向量机"><span class="nav-text">二、线性支持向量机</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#三核函数"><span class="nav-text">三、核函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#svm为什么用对偶问题来求解"><span class="nav-text">SVM为什么用对偶问题来求解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#为何令间隔为1"><span class="nav-text">为何令间隔为1</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#什么是拉格朗日对偶"><span class="nav-text">什么是拉格朗日对偶</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#什么是kkt条件"><span class="nav-text">什么是KKT条件</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#反向传播"><span class="nav-text">* 反向传播</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#knn"><span class="nav-text">KNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#决策树"><span class="nav-text">决策树</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#决策树的三要素"><span class="nav-text">决策树的三要素</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#剪枝处理的作用及策略"><span class="nav-text">剪枝处理的作用及策略</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#决策树学习基本算法"><span class="nav-text">决策树学习基本算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#分类树-id3-决策树与-c4.5-决策树"><span class="nav-text">[分类树] ID3 决策树与 C4.5 决策树</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#回归树---cart-决策树"><span class="nav-text">[回归树] - CART 决策树</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#熵"><span class="nav-text">熵</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#条件熵"><span class="nav-text">条件熵</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#信息增益"><span class="nav-text">信息增益</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#信息增益率"><span class="nav-text">信息增益率</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#最大基尼指数gini"><span class="nav-text">最大基尼指数（Gini）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#id3c4.5cart决策树区别与联系"><span class="nav-text">ID3、C4.5、CART决策树区别与联系</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#决策树算法优缺点"><span class="nav-text">决策树算法优缺点</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#朴素贝叶斯"><span class="nav-text">朴素贝叶斯</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#概率知识"><span class="nav-text">概率知识</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#图解极大似然估计"><span class="nav-text">图解极大似然估计</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#极大似然估计原理"><span class="nav-text">极大似然估计原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#朴素贝叶斯-1"><span class="nav-text">朴素贝叶斯</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#举例说明"><span class="nav-text">举例说明</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#k-means"><span class="nav-text">K-means</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#算法流程"><span class="nav-text">算法流程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#伪代码实现"><span class="nav-text">伪代码实现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#k值的选择-手肘法"><span class="nav-text">K值的选择 手肘法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pca"><span class="nav-text">PCA</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#思想总结"><span class="nav-text">思想总结</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#算法总结"><span class="nav-text">算法总结</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#层次聚类"><span class="nav-text">层次聚类</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lda"><span class="nav-text">LDA</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#lda-分类思想"><span class="nav-text">LDA 分类思想</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#lda-降维算法流程"><span class="nav-text">LDA 降维算法流程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#lda-降维算法流程总结"><span class="nav-text">LDA 降维算法流程总结</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#lda-和-pca区别"><span class="nav-text">LDA 和 PCA区别</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#em"><span class="nav-text">EM</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#em算法基本思想"><span class="nav-text">EM算法基本思想</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#em算法推导"><span class="nav-text">EM算法推导</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#图解em算法"><span class="nav-text">图解EM算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#em算法流程"><span class="nav-text">EM算法流程</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#集成学习"><span class="nav-text">集成学习</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#集成学习的基本思想"><span class="nav-text">集成学习的基本思想</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#集成学习为什么有效"><span class="nav-text">集成学习为什么有效</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#boosting-方法"><span class="nav-text">Boosting 方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#bagging-方法"><span class="nav-text">Bagging 方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么使用决策树作为基学习器"><span class="nav-text">为什么使用决策树作为基学习器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么不稳定的学习器更适合作为基学习器"><span class="nav-text">为什么不稳定的学习器更适合作为基学习器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#还有哪些模型也适合作为基学习器"><span class="nav-text">还有哪些模型也适合作为基学习器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#boosting-方法中能使用线性分类器作为基学习器吗-bagging-呢"><span class="nav-text">Boosting 方法中能使用线性分类器作为基学习器吗？ Bagging 呢？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#boostingbagging-与-偏差方差-的关系"><span class="nav-text">Boosting/Bagging 与 偏差/方差 的关系</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#bagging与dropout区别与联系"><span class="nav-text">Bagging与Dropout区别与联系</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#adaboost"><span class="nav-text">AdaBoost</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#gbdt"><span class="nav-text">GBDT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#rf-随机森林"><span class="nav-text">RF 随机森林</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#xgboost"><span class="nav-text">XGBoost</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#神经网络"><span class="nav-text">神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#超参数"><span class="nav-text">超参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#如何寻找超参数的最优值"><span class="nav-text">如何寻找超参数的最优值</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#超参数搜索一般过程"><span class="nav-text">超参数搜索一般过程</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度下降"><span class="nav-text">梯度下降</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#机器学习中为什么需要梯度下降"><span class="nav-text">机器学习中为什么需要梯度下降</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#梯度下降法缺点"><span class="nav-text">梯度下降法缺点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#梯度下降法直观理解"><span class="nav-text">梯度下降法直观理解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#梯度下降法算法描述"><span class="nav-text">梯度下降法算法描述</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#如何对梯度下降法进行调优"><span class="nav-text">如何对梯度下降法进行调优</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#随机梯度和批量梯度区别"><span class="nav-text">随机梯度和批量梯度区别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#各种梯度下降法性能比较"><span class="nav-text">各种梯度下降法性能比较</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#梯度爆炸与梯度消失"><span class="nav-text">梯度爆炸与梯度消失</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#梯度消失爆炸的解决方案"><span class="nav-text">梯度消失、爆炸的解决方案</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#深度学习为什么要使用梯度更新规则"><span class="nav-text">深度学习为什么要使用梯度更新规则</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#如何防止梯度下降陷入局部最优解"><span class="nav-text">如何防止梯度下降陷入局部最优解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#梯度下降与正规方程的比较"><span class="nav-text">梯度下降与正规方程的比较</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#最小二乘法和梯度下降法区别"><span class="nav-text">最小二乘法和梯度下降法区别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#基于二阶梯度的优化算法"><span class="nav-text">基于二阶梯度的优化算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#牛顿法"><span class="nav-text">牛顿法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#为什么牛顿法比梯度下降收敛更快"><span class="nav-text">为什么牛顿法比梯度下降收敛更快？</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#牛顿法的优缺点"><span class="nav-text">牛顿法的优缺点</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#拟牛顿法-todo"><span class="nav-text">拟牛顿法 TODO</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#梯度检验"><span class="nav-text">梯度检验</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#batch_size"><span class="nav-text">batch_size</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么需要-batch_size"><span class="nav-text">为什么需要 batch_size</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#在合理范围内增大batch_size有何好处"><span class="nav-text">在合理范围内，增大Batch_Size有何好处</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#盲目增大-batch_size-有何坏处"><span class="nav-text">盲目增大 Batch_Size 有何坏处</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#学习率-learning-rate"><span class="nav-text">学习率 learning rate</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据处理"><span class="nav-text">数据处理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#类别不平衡问题"><span class="nav-text">类别不平衡问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#处理数据中的缺失值"><span class="nav-text">处理数据中的缺失值</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#训练样本少的问题"><span class="nav-text">训练样本少的问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#数据增强方法"><span class="nav-text">数据增强方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#权值初始化方法"><span class="nav-text">权值初始化方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#深度学习是否能胜任所有数据集"><span class="nav-text">深度学习是否能胜任所有数据集</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#共线性如何判断和解决共线性问题"><span class="nav-text">共线性，如何判断和解决共线性问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ones-hot-编码"><span class="nav-text">Ones-Hot 编码</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#特征选择feature-selection"><span class="nav-text">特征选择(feature selection)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#特征类型有哪些"><span class="nav-text">特征类型有哪些</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#如何考虑特征选择"><span class="nav-text">如何考虑特征选择</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#特征选择方法分类"><span class="nav-text">特征选择方法分类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#特征选择目的"><span class="nav-text">特征选择目的</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#归一化-与-标准化"><span class="nav-text">归一化 与 标准化</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#batch-normalization批标准化"><span class="nav-text">Batch Normalization(批标准化)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#bn层作用以及如何使用bn层"><span class="nav-text">BN层作用，以及如何使用BN层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#动机"><span class="nav-text">动机</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#基本原理"><span class="nav-text">基本原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么训练时不采用移动平均"><span class="nav-text">为什么训练时不采用移动平均？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#bnlningn的异同"><span class="nav-text">BN、LN、IN、GN的异同</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dropout"><span class="nav-text">Dropout</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#dropout-具体实现"><span class="nav-text">Dropout 具体实现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#dropput-为何能解决过拟合"><span class="nav-text">Dropput 为何能解决过拟合？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#dropout率的选择"><span class="nav-text">Dropout率的选择</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#dropout-缺点"><span class="nav-text">Dropout 缺点</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#优化算法"><span class="nav-text">优化算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#gradient-descent"><span class="nav-text">Gradient Descent</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#gradient-descent-和其算法变种"><span class="nav-text">Gradient Descent 和其算法变种</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#vanilla-sgd"><span class="nav-text">Vanilla SGD</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#sgd-with-momentum"><span class="nav-text">SGD with Momentum</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#nesterov-accelerated-gradient"><span class="nav-text">Nesterov Accelerated Gradient</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#adagrad"><span class="nav-text">Adagrad</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#adadelta"><span class="nav-text">AdaDelta</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#rmsprop"><span class="nav-text">RMSprop</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#adam"><span class="nav-text">Adam</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#深度学习为什么不用二阶优化"><span class="nav-text">深度学习为什么不用二阶优化</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#预训练与微调fine-tuning"><span class="nav-text">预训练与微调(fine tuning)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#什么是模型微调-fine-tuning"><span class="nav-text">什么是模型微调 fine tuning</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#微调时候网络参数是否更新"><span class="nav-text">微调时候网络参数是否更新</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#fine-tuning-模型的三种状态"><span class="nav-text">fine-tuning 模型的三种状态</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#resnet"><span class="nav-text">ResNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#caffe"><span class="nav-text">Caffe</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#caffe卷积层的实现"><span class="nav-text">Caffe卷积层的实现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#caffe-结构"><span class="nav-text">Caffe 结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#cuda编程"><span class="nav-text">CUDA编程 *</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#常考问题"><span class="nav-text">*常考问题</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#全连接层作用"><span class="nav-text">全连接层作用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#卷积的意义与作用"><span class="nav-text">卷积的意义与作用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#卷积层参数计算量flops"><span class="nav-text">卷积层参数、计算量(FLOPs)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#d卷积"><span class="nav-text">2D卷积</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#d-卷积"><span class="nav-text">3D 卷积</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#扩张卷积空洞卷积dilated-convolution"><span class="nav-text">扩张卷积（空洞卷积）dilated convolution *</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#分组卷积group-convolution"><span class="nav-text">分组卷积(Group convolution)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#深度可分卷积depthwise-separable-convolution"><span class="nav-text">深度可分卷积(Depthwise separable convolution)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#空间可分卷积"><span class="nav-text">空间可分卷积</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#感受野"><span class="nav-text">感受野</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#pooling层反向传播"><span class="nav-text">Pooling层反向传播</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么用smoothl1-loss而不用-l2-loss"><span class="nav-text">为什么用SmoothL1 loss而不用 L2 loss</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#iou"><span class="nav-text">IOU</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#nms"><span class="nav-text">NMS</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#softmax-实现"><span class="nav-text">Softmax 实现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#卷积实现"><span class="nav-text">卷积实现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#yolo"><span class="nav-text">YOLO</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#section"><span class="nav-text"></span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#工程部署"><span class="nav-text">工程部署</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#移动端部署-优化"><span class="nav-text">移动端部署 + 优化</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据结构"><span class="nav-text">数据结构</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#kmp算法"><span class="nav-text">KMP算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#几大排序算法"><span class="nav-text">几大排序算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#map-和-unordered_map-区别"><span class="nav-text">map 和 unordered_map 区别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#二叉树分类"><span class="nav-text">二叉树分类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#二叉查找排序树"><span class="nav-text">二叉查找(排序)树</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#b-树-和-b树"><span class="nav-text">B 树 和 B+树</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#面试题目"><span class="nav-text">面试题目</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#色彩空间色彩空间的转换"><span class="nav-text">色彩空间，色彩空间的转换</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#膨胀-与-腐蚀"><span class="nav-text">膨胀 与 腐蚀</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#边缘检测算子"><span class="nav-text">边缘检测算子</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#常见的特征点"><span class="nav-text">常见的特征点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#随机洗牌法"><span class="nav-text">随机洗牌法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#蓄水池算法"><span class="nav-text">蓄水池算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#广义逆阵"><span class="nav-text">广义逆阵</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#海量数据处理问题"><span class="nav-text">海量数据处理问题</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#一top-k问题"><span class="nav-text">一、top K问题</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#二分布式-top-k问题"><span class="nav-text">二、分布式 top K问题</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#三公共数据问题"><span class="nav-text">三、公共数据问题</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#四位图法"><span class="nav-text">四、位图法</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#五trie树"><span class="nav-text">五、Trie树</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#面试算法题目"><span class="nav-text">面试算法题目</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#概率智力题"><span class="nav-text">概率智力题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#待整理"><span class="nav-text">待整理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#矩阵相乘最小乘法次数"><span class="nav-text">矩阵相乘最小乘法次数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#svm-和-softmax-区别"><span class="nav-text">SVM 和 Softmax 区别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#仿射变换和透视变化区别"><span class="nav-text">仿射变换和透视变化区别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#透视变换矩阵的形状"><span class="nav-text">透视变换矩阵的形状</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#svmlr-和-softmax-区别"><span class="nav-text">SVM、LR 和 Softmax 区别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#svm寻参问题"><span class="nav-text">SVM寻参问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#svm核函数解释"><span class="nav-text">SVM核函数解释</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#交叉熵损失-可以用-l2损失替换么"><span class="nav-text">交叉熵损失 可以用 L2损失替换么</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#先把yolo看完500问中有检测论文需要看的顺序"><span class="nav-text">先把YOLO看完（500问中有检测论文需要看的顺序）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#简述-yolo-和-ssd"><span class="nav-text">简述 YOLO 和 SSD</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#yolov2"><span class="nav-text">YOLOv2</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#batch_size-和-learning-rate的关系怎么平衡和调整二者"><span class="nav-text">batch_size 和 learning rate的关系（怎么平衡和调整二者）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#inception-v1-v4的演变"><span class="nav-text">Inception v1-v4的演变</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#简述-cnn-的演变"><span class="nav-text">简述 CNN 的演变</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#roi-pooling-和-roi-align"><span class="nav-text">ROI Pooling 和 ROI Align</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#cnn为什么有效-1-2"><span class="nav-text">CNN为什么有效 1 2</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#cnn在图像上表现好的原因"><span class="nav-text">CNN在图像上表现好的原因</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#对迁移学习的理解为什么能work"><span class="nav-text">对迁移学习的理解，为什么能work</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#实现一个卷积操作-另一个简单实现卷积操作"><span class="nav-text">实现一个卷积操作 另一个简单实现卷积操作</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#模型精简加速"><span class="nav-text">模型精简加速</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#图像处理中的常用算子"><span class="nav-text">图像处理中的常用算子</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#卷积与反卷积"><span class="nav-text">卷积与反卷积</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#lstm-与-rnn-模型"><span class="nav-text">LSTM 与 RNN 模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#lstm的结构其相对于rnn的好处"><span class="nav-text">LSTM的结构，其相对于RNN的好处</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#bn原理与实战"><span class="nav-text">BN原理与实战</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#面经集合"><span class="nav-text">面经集合</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#思维导图"><span class="nav-text">思维导图</span></a></li></ol></div>
            
          </div>
          <div class="post-toc-indicator-bottom post-toc-indicator"></div>
        </section>
      

    </div>
  </aside>


        
	  </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner"> <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="icon-next-heart fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">SmileLingyong</span>
</div>

<div class="powered-by">
  Powered by <a class="theme-link" href="http://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="#">
    FreeSky
  </a>(Reserved)

  
  <span id="busuanzi_container_site_uv">
     &nbsp; | &nbsp;  用户量: <span id="busuanzi_value_site_uv"></span>
  </span>
  <span id="busuanzi_container_site_pv">
    &nbsp; | &nbsp;  总访问量: <span id="busuanzi_value_site_pv"></span>
  </span>

  
</div>


<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
 </div>
    </footer>

    <div class="back-to-top"></div>
  </div>

  <script type="text/javascript" src="/others/jquery/index.js?v=2.1.3"></script>

  
  
  
    
    

  


  
  
  <script type="text/javascript" src="/others/fancybox/source/jquery.fancybox.pack.js"></script>
  <script type="text/javascript" src="/js/fancy-box.js?v=0.4.5.2"></script>


  <script type="text/javascript" src="/js/helpers.js?v=0.4.5.2"></script>
  

  <script type="text/javascript" src="/others/velocity/velocity.min.js"></script>
  <script type="text/javascript" src="/others/velocity/velocity.ui.min.js"></script>

  <script type="text/javascript" src="/js/motion_global.js?v=0.4.5.2" id="motion.global"></script>




  <script type="text/javascript" src="/js/nav-toggle.js?v=0.4.5.2"></script>
  <script type="text/javascript" src="/others/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  
<script type="text/javascript" src="/js/bootstrap.scrollspy.js?v=0.4.5.2" id="bootstrap.scrollspy.custom"></script>


<script type="text/javascript" id="sidebar.toc.highlight">
  $(document).ready(function () {
    var tocSelector = '.post-toc';
    var $tocSelector = $(tocSelector);
    var activeCurrentSelector = '.active-current';

    $tocSelector
      .on('activate.bs.scrollspy', function () {
        var $currentActiveElement = $(tocSelector + ' .active').last();

        removeCurrentActiveClass();
        $currentActiveElement.addClass('active-current');

        $tocSelector[0].scrollTop = $currentActiveElement.position().top;
      })
      .on('clear.bs.scrollspy', function () {
        removeCurrentActiveClass();
      });

    function removeCurrentActiveClass () {
      $(tocSelector + ' ' + activeCurrentSelector)
        .removeClass(activeCurrentSelector.substring(1));
    }

    function processTOC () {
      getTOCMaxHeight();
      toggleTOCOverflowIndicators();
    }

    function getTOCMaxHeight () {
      var height = $('.sidebar').height() -
                   $tocSelector.position().top -
                   $('.post-toc-indicator-bottom').height();

      $tocSelector.css('height', height);

      return height;
    }

    function toggleTOCOverflowIndicators () {
      tocOverflowIndicator(
        '.post-toc-indicator-top',
        $tocSelector.scrollTop() > 0 ? 'show' : 'hide'
      );

      tocOverflowIndicator(
        '.post-toc-indicator-bottom',
        $tocSelector.scrollTop() >= $tocSelector.find('ol').height() - $tocSelector.height() ? 'hide' : 'show'
      )
    }

    $(document).on('sidebar.motion.complete', function () {
      processTOC();
    });

    $('body').scrollspy({ target: tocSelector });
    $(window).on('resize', function () {
      if ( $('.sidebar').hasClass('sidebar-active') ) {
        processTOC();
      }
    });

    onScroll($tocSelector);

    function onScroll (element) {
      element.on('mousewheel DOMMouseScroll', function (event) {
          var oe = event.originalEvent;
          var delta = oe.wheelDelta || -oe.detail;

          this.scrollTop += ( delta < 0 ? 1 : -1 ) * 30;
          event.preventDefault();

          toggleTOCOverflowIndicators();
      });
    }

    function tocOverflowIndicator (indicator, action) {
      var $indicator = $(indicator);
      var opacity = action === 'show' ? 0.4 : 0;
      $indicator.velocity ?
        $indicator.velocity('stop').velocity({
          opacity: opacity
        }, { duration: 100 }) :
        $indicator.stop().animate({
          opacity: opacity
        }, 100);
    }

  });
</script>

<script type="text/javascript" id="sidebar.nav">
  $(document).ready(function () {
    var html = $('html');
    var TAB_ANIMATE_DURATION = 200;
    var hasVelocity = $.isFunction(html.velocity);

    $('.sidebar-nav li').on('click', function () {
      var item = $(this);
      var activeTabClassName = 'sidebar-nav-active';
      var activePanelClassName = 'sidebar-panel-active';
      if (item.hasClass(activeTabClassName)) {
        return;
      }

      var currentTarget = $('.' + activePanelClassName);
      var target = $('.' + item.data('target'));

      hasVelocity ?
        currentTarget.velocity('transition.slideUpOut', TAB_ANIMATE_DURATION, function () {
          target
            .velocity('stop')
            .velocity('transition.slideDownIn', TAB_ANIMATE_DURATION)
            .addClass(activePanelClassName);
        }) :
        currentTarget.animate({ opacity: 0 }, TAB_ANIMATE_DURATION, function () {
          currentTarget.hide();
          target
            .stop()
            .css({'opacity': 0, 'display': 'block'})
            .animate({ opacity: 1 }, TAB_ANIMATE_DURATION, function () {
              currentTarget.removeClass(activePanelClassName);
              target.addClass(activePanelClassName);
            });
        });

      item.siblings().removeClass(activeTabClassName);
      item.addClass(activeTabClassName);
    });

    $('.post-toc a').on('click', function (e) {
      e.preventDefault();
      var targetSelector = escapeSelector(this.getAttribute('href'));
      var offset = $(targetSelector).offset().top;
      hasVelocity ?
        html.velocity('stop').velocity('scroll', {
          offset: offset  + 'px',
          mobileHA: false
        }) :
        $('html, body').stop().animate({
          scrollTop: offset
        }, 500);
    });

    // Expand sidebar on post detail page by default, when post has a toc.
    var $tocContent = $('.post-toc-content');
    var $aboutContent = $('#posts-about');
    if (isDesktop() && CONFIG.sidebar === 'post') {
      if ($tocContent.length > 0 && $tocContent.html().trim().length > 0 && $aboutContent.length === 0) {
        displaySidebar();
      }
    }
  });
</script>



  <script type="text/javascript">
    $(document).ready(function () {
      if (CONFIG.sidebar === 'always') {
        displaySidebar();
      }
      if (isMobile()) {
        FastClick.attach(document.body);
      }

      motionIntegrator.bootstrap();
    });
  </script>

  
  

  
  

  
  <script type="text/javascript" src="/js/lazyload.js"></script>
  <script type="text/javascript">
    $(function () {
      $("#posts").find('img').lazyload({
        placeholder: "/images/loading.gif",
        effect: "fadeIn"
      });
    });
  </script>
  
     <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
<script>AV.initialize("9QoQXWnRR4zwSFuxRv52kUpi-gzGzoHsz", "zlgcRzgHF7AHu8TKLJUwCAjw");</script>
<script>
function showTime(Counter) {
  var query = new AV.Query(Counter);
  $(".leancloud_visitors").each(function() {
    var url = $(this).attr("id").trim();
    query.equalTo("url", url);
    query.find({
      success: function(results) {
        if (results.length == 0) {
          var content = $(document.getElementById(url)).text() + ': 0';
          $(document.getElementById(url)).text(content);
          return;
        }
        for (var i = 0; i < results.length; i++) {
          var object = results[i];
          var content = $(document.getElementById(url)).text() + ': ' + object.get('time');
          $(document.getElementById(url)).text(content);
        }
      },
      error: function(object, error) {
        console.log("Error: " + error.code + " " + error.message);
      }
    });

  });
}

function addCount(Counter) {
  var Counter = AV.Object.extend("Counter");
  url = $(".leancloud_visitors").attr('id').trim();
  title = $(".leancloud_visitors").attr('data-flag-title').trim();
  var query = new AV.Query(Counter);
  query.equalTo("url", url);
  query.find({
    success: function(results) {
      if (results.length > 0) {
        var counter = results[0];
        counter.fetchWhenSave(true);
        counter.increment("time");
        counter.save(null, {
          success: function(counter) {
            var content = $(document.getElementById(url)).text() + ': ' + counter.get('time');
            $(document.getElementById(url)).text(content);
          },
          error: function(counter, error) {
            console.log('Failed to save Visitor num, with error message: ' + error.message);
          }
        });
      } else {
        var newcounter = new Counter();
        newcounter.set("title", title);
        newcounter.set("url", url);
        newcounter.set("time", 1);
        newcounter.save(null, {
          success: function(newcounter) {
              console.log("newcounter.get('time')="+newcounter.get('time'));
            var content = $(document.getElementById(url)).text() + ': ' + newcounter.get('time');
            $(document.getElementById(url)).text(content);
          },
          error: function(newcounter, error) {
            console.log('Failed to create');
          }
        });
      }
    },
    error: function(error) {
      console.log('Error:' + error.code + " " + error.message);
    }
  });
}
$(function() {
  var Counter = AV.Object.extend("Counter");
  if ($('.leancloud_visitors').length == 1) {
    addCount(Counter);
  } else if ($('.post-title-link').length > 1) {
    showTime(Counter);
  }
}); 
</script>
  
</body>
</html>
