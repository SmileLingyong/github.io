<!doctype html>
<html class="theme-next use-motion ">
<head>
  

<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>


<script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
<link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet">
<style>
    .pace .pace-progress {
        background: #1E92FB; /*进度条颜色*/
        height: 3px;
    }
    .pace .pace-progress-inner {
         box-shadow: 0 0 10px #1E92FB, 0 0 5px     #1E92FB; /*阴影颜色*/
    }
    .pace .pace-activity {
        border-top-color: #1E92FB;    /*上边框颜色*/
        border-left-color: #1E92FB;    /*左边框颜色*/
    }
</style>




<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />








  <link rel="stylesheet" type="text/css" href="/others/fancybox/source/jquery.fancybox.css?v=2.1.5"/>






  <link href="/vendors/googleapis/css/Lato.css" rel="stylesheet" type="text/css">




<link rel="stylesheet" type="text/css" href="/others/font-awesome/css/font-awesome.min.css?v=4.4.0" />

<link rel="stylesheet" type="text/css" href="/css/main.css?v=0.4.5.2"/>


    <meta name="description" content="向上，向阳！" />



  <meta name="keywords" content="Machine Learning,Deeping Learning,job," />





  <link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico?v=0.4.5.2" />


<meta name="description" content="code{white-space: pre;}">
<meta name="keywords" content="Machine Learning,Deeping Learning,job">
<meta property="og:type" content="article">
<meta property="og:title" content="加油！一定能找到好工作！">
<meta property="og:url" content="http://yoursite.com/2019/05/08/Try-your-best!You-will-find-the-job/index.html">
<meta property="og:site_name" content="SmileLingyong">
<meta property="og:description" content="code{white-space: pre;}">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/2019/05/08/Try-your-best!You-will-find-the-job/TIM截图20180817192259.png">
<meta property="og:image" content="http://yoursite.com/2019/05/08/Try-your-best!You-will-find-the-job/TIM截图20180817214034.png">
<meta property="og:image" content="http://yoursite.com/2019/05/08/Try-your-best!You-will-find-the-job/2.16.4.1.jpg">
<meta property="og:image" content="http://yoursite.com/2019/05/08/Try-your-best!You-will-find-the-job/2.16.4.2.png">
<meta property="og:image" content="http://yoursite.com/2019/05/08/Try-your-best!You-will-find-the-job/2.16.4.3.png">
<meta property="og:image" content="http://yoursite.com/2019/05/08/Try-your-best!You-will-find-the-job/TIM截图20180608171710.png">
<meta property="og:image" content="http://yoursite.com/2019/05/08/Try-your-best!You-will-find-the-job/TIM截图20180608172312.png">
<meta property="og:image" content="http://yoursite.com/2019/05/08/Try-your-best!You-will-find-the-job/k-折交叉验证.png">
<meta property="og:image" content="http://yoursite.com/2019/05/08/Try-your-best!You-will-find-the-job/precision_recall.png">
<meta property="og:image" content="http://yoursite.com/2019/05/08/Try-your-best!You-will-find-the-job/mAP.png">
<meta property="og:image" content="http://yoursite.com/2019/05/08/Try-your-best!You-will-find-the-job/result.png">
<meta property="og:image" content="http://yoursite.com/2019/05/08/Try-your-best!You-will-find-the-job/sigmod_01.png">
<meta property="og:image" content="http://yoursite.com/2019/05/08/Try-your-best!You-will-find-the-job/sigmod_02.png">
<meta property="og:image" content="http://yoursite.com/2019/05/08/Try-your-best!You-will-find-the-job/tanh_01.png">
<meta property="og:image" content="http://yoursite.com/2019/05/08/Try-your-best!You-will-find-the-job/tanh_02.png">
<meta property="og:image" content="http://yoursite.com/2019/05/08/Try-your-best!You-will-find-the-job/ReLU_01.png">
<meta property="og:image" content="http://yoursite.com/2019/05/08/Try-your-best!You-will-find-the-job/ReLU_02.png">
<meta property="og:image" content="http://yoursite.com/2019/05/08/Try-your-best!You-will-find-the-job/Leaky_ReLU_01.png">
<meta property="og:image" content="http://yoursite.com/2019/05/08/Try-your-best!You-will-find-the-job/梯度下降.png">
<meta property="og:image" content="http://yoursite.com/2019/05/08/Try-your-best!You-will-find-the-job/范数.png">
<meta property="og:image" content="http://yoursite.com/2019/05/08/Try-your-best!You-will-find-the-job/BN.png">
<meta property="og:image" content="http://yoursite.com/2019/05/08/Try-your-best!You-will-find-the-job/BN_02.png">
<meta property="og:image" content="http://yoursite.com/2019/05/08/Try-your-best!You-will-find-the-job/GN.png">
<meta property="og:image" content="http://yoursite.com/2019/05/08/Try-your-best!You-will-find-the-job/BN_croped.png">
<meta property="og:image" content="http://yoursite.com/2019/05/08/Try-your-best!You-will-find-the-job/caffe_conv_02.jpg">
<meta property="og:image" content="http://yoursite.com/2019/05/08/Try-your-best!You-will-find-the-job/caffe_conv_01.png">
<meta property="og:updated_time" content="2019-05-23T06:07:13.104Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="加油！一定能找到好工作！">
<meta name="twitter:description" content="code{white-space: pre;}">
<meta name="twitter:image" content="http://yoursite.com/2019/05/08/Try-your-best!You-will-find-the-job/TIM截图20180817192259.png">


<script type="text/javascript" id="hexo.configuration">
  var CONFIG = {
    scheme: '',
    sidebar: 'always'
  };
</script>



  <title> 加油！一定能找到好工作！ | SmileLingyong </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="">

  <!--[if lte IE 8]>
  <div style=' clear: both; height: 59px; padding:0 0 0 15px; position: relative;margin:0 auto;'>
    <a href="http://windows.microsoft.com/en-US/internet-explorer/products/ie/home?ocid=ie6_countdown_bannercode">
      <img src="http://7u2nvr.com1.z0.glb.clouddn.com/picouterie.jpg" border="0" height="42" width="820"
           alt="You are using an outdated browser. For a faster, safer browsing experience, upgrade for free today or use other browser ,like chrome firefox safari."
           style='margin-left:auto;margin-right:auto;display: block;'/>
    </a>
  </div>
<![endif]-->
  






  <div id="container" class="container one-column page-post-detail">

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  
  
    <ul id="menu" class="menu ">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-user"></i> <br />
            
            About
          </a>
        </li>
      

      
      
	  	<span style="font-size:14px;float:right;padding:39px 40px 0 0;">——穷则独善其身，达则兼济天下.</span>
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">

        	<div id="content" class="content"> 

  <div id="posts" class="posts-expand">
    

  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                加油！一定能找到好工作！
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            Posted on
            <time itemprop="dateCreated" datetime="2019-05-08T23:18:23+08:00" content="2019-05-08">
              2019-05-08 23:18
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp; In
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/ML/" itemprop="url" rel="index">
                    <span itemprop="name">ML</span>
                  </a>
                </span>

                
                
                  , 
                

              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/DL/" itemprop="url" rel="index">
                    <span itemprop="name">DL</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
            <span id="/2019/05/08/Try-your-best!You-will-find-the-job/"class="leancloud_visitors"  data-flag-title="加油！一定能找到好工作！">
            &nbsp; | &nbsp;   
            views
            </span>
          
        </div>
      </header>
    


    <div class="post-body">

      
      

      
        <span itemprop="articleBody"><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <meta http-equiv="Content-Style-Type" content="text/css">
  <meta name="generator" content="pandoc">
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pygments-css@1.0.0/github.min.css" type="text/css">
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
</head>
<body>
<a id="more"></a>
<h3 id="模型评估">模型评估</h3>
<h4 id="模型评估常用方法">模型评估常用方法</h4>
<p>一般情况来说，单一评分标准无法完全评估一个机器学习模型。只用good和bad偏离真实场景去评估某个模型，都是一种欠妥的评估方式。下面介绍常用的分类模型和回归模型评估方法。</p>
<p><strong>分类模型常用评估方法：</strong></p>
<table>
<thead>
<tr class="header">
<th align="center">指标</th>
<th align="center">描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Accuracy</td>
<td align="center">准确率</td>
</tr>
<tr class="even">
<td align="center">Precision</td>
<td align="center">精准度/查准率</td>
</tr>
<tr class="odd">
<td align="center">Recall</td>
<td align="center">召回率/查全率</td>
</tr>
<tr class="even">
<td align="center">P-R曲线</td>
<td align="center">查准率为纵轴，查全率为横轴，作图</td>
</tr>
<tr class="odd">
<td align="center">F1</td>
<td align="center">F1值</td>
</tr>
<tr class="even">
<td align="center">Confusion Matrix</td>
<td align="center">混淆矩阵</td>
</tr>
<tr class="odd">
<td align="center">ROC</td>
<td align="center">ROC曲线</td>
</tr>
<tr class="even">
<td align="center">AUC</td>
<td align="center">ROC曲线下的面积</td>
</tr>
</tbody>
</table>
<p>F1 是基于查准率与查全率的调和平均(harmonic mean)定义的： <span class="math inline">\(\frac{1}{F 1}=\frac{1}{2} \cdot\left(\frac{1}{P}+\frac{1}{R}\right)\)</span><br>
<span class="math display">\[
F 1=\frac{2 \times P \times R}{P+R}
 = =\frac{2 \times TP }{样例总数 + TP - TN}
\]</span><br>
用于综合考虑查准率、查全率的性能度量。</p>
<p><strong>回归模型常用评估方法：</strong></p>
<table>
<thead>
<tr class="header">
<th align="center">指标</th>
<th align="center">描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Mean Square Error (MSE, RMSE)</td>
<td align="center">均方误差</td>
</tr>
<tr class="even">
<td align="center">Absolute Error (MAE, RAE)</td>
<td align="center">绝对误差</td>
</tr>
<tr class="odd">
<td align="center">R-Squared</td>
<td align="center">R平方值</td>
</tr>
</tbody>
</table>
<h4 id="机器学习中的bias和variance有什么区别和联系">机器学习中的Bias和Variance有什么区别和联系</h4>
<p><strong>Bias(偏差)</strong>与<strong>Variance(方差)</strong>分别是用于衡量一个模型<strong>泛化误差</strong>的两个方面<br>
- <strong>Bias(偏差)：</strong> 指的是模型预测的<strong>期望值</strong>与<strong>真实值</strong>之间的差（描述模型的<strong>拟合能力</strong>）<br>
- <strong>Variance(方差)：</strong> 指的是模型预测的<strong>期望值</strong>与<strong>预测值</strong>之间的差的平方和（描述模型的<strong>稳定性</strong>）</p>
<ul>
<li>在<strong>监督学习</strong>中，模型的<strong>泛化误差</strong>可<strong>分解</strong>为偏差、方差与噪声之和。</li>
</ul>
<p><span class="math display">\[
Err(x) = Bias^2 + Variance + Irreducible\ Error
\]</span></p>
<ul>
<li><strong>噪声：</strong>表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度</li>
</ul>
<p><img src="/2019/05/08/Try-your-best!You-will-find-the-job/TIM截图20180817192259.png" height></p>
<h4 id="经验误差与泛化误差">经验误差与泛化误差</h4>
<ul>
<li><strong>经验误差</strong>（empirical error）：也叫训练误差（training error），模型在训练集上的误差。</li>
<li><strong>泛化误差</strong>（generalization error）：模型在新样本集（测试集）上的误差。</li>
</ul>
<h4 id="深度学习中的偏差与方差">深度学习中的偏差与方差</h4>
<ul>
<li>神经网络的拟合能力非常强，因此它的<strong>训练误差</strong>（偏差）通常较小；</li>
<li>但是过强的拟合能力会导致较大的方差，使模型的测试误差（<strong>泛化误差</strong>）增大；</li>
<li>因此深度学习的核心工作之一就是研究如何降低模型的泛化误差，这类方法统称为<strong>正则化方法</strong>。</li>
</ul>
<h4 id="偏差方差与boosting和bagging联系"><a href="#集成学习">偏差方差与Boosting和Bagging联系</a></h4>
<ol style="list-style-type: decimal">
<li><strong>Boosting</strong> 能提升弱分类器性能的原因是降低了<strong>偏差</strong></li>
<li><strong>Bagging</strong> 则是降低了<strong>方差</strong></li>
</ol>
<ul>
<li><strong>Boosting</strong> 方法：</li>
<li>Boosting 的<strong>基本思路</strong>就是在不断减小模型的<strong>训练误差</strong>（拟合残差或者加大错类的权重），加强模型的学习能力，从而减小偏差；</li>
<li>但 Boosting 不会显著降低方差，因为其训练过程中各基学习器是强相关的，缺少独立性。</li>
<li><strong>Bagging</strong> 方法：</li>
<li>对 <code>n</code> 个<strong>独立不相关的模型</strong>预测结果取平均，方差是原来的 <code>1/n</code>；</li>
<li>假设所有基分类器出错的概率是独立的，<strong>超过半数</strong>基分类器出错的概率会随着基分类器的数量增加而下降。</li>
</ul>
<h4 id="偏差与方差的计算公式">偏差与方差的计算公式</h4>
<ul>
<li><p>记在<strong>训练集 D</strong> 上学得的模型为<br>
<span class="math display">\[
  f(\boldsymbol{x} ; D)
  \]</span><br>
模型的<strong>期望预测</strong>为<br>
<span class="math display">\[
  \hat{f}(\boldsymbol{x})=\mathbb{E}_{D}[f(\boldsymbol{x} ; D)]
  \]</span></p></li>
<li><p><strong>偏差</strong>（Bias）<br>
<span class="math display">\[
  \operatorname{bias}^{2}(\boldsymbol{x})=(\hat{f}(\boldsymbol{x})-y)^{2}
  \]</span></p></li>
</ul>
<blockquote>
<p><strong>偏差</strong>度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力；</p>
</blockquote>
<ul>
<li><strong>方差</strong>（Variance）<br>
<span class="math display">\[
  \operatorname{var}(\boldsymbol{x})=\mathbb{E}_{D}\left[(f(\boldsymbol{x} ; D)-\hat{f}(\boldsymbol{x}))^{2}\right]
  \]</span></li>
</ul>
<blockquote>
<p><strong>方差</strong>度量了同样大小的<strong>训练集的变动</strong>所导致的学习性能的变化，即刻画了数据扰动所造成的影响（模型的稳定性）；</p>
</blockquote>
<ul>
<li><strong>噪声</strong><br>
<span class="math display">\[
  \varepsilon^{2}=\mathbb{E}_{D}\left[\left(y_{D}-y\right)^{2}\right]
  \]</span></li>
</ul>
<blockquote>
<p><strong>噪声</strong>则表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度。</p>
</blockquote>
<ul>
<li>“<strong>偏差-方差分解</strong>”表明模型的泛化能力是由算法的能力、数据的充分性、任务本身的难度共同决定的</li>
</ul>
<h4 id="偏差与方差的权衡过拟合与模型复杂度的权衡">偏差与方差的权衡（过拟合与模型复杂度的权衡）</h4>
<ul>
<li>给定学习任务，</li>
<li>当训练不足时，模型的<strong>拟合能力不够</strong>（数据的扰动不足以使模型产生显著的变化），此时<strong>偏差</strong>主导模型的泛化误差；</li>
<li>随着训练的进行，模型的<strong>拟合能力增强</strong>（模型能够学习数据发生的扰动），此时<strong>方差</strong>逐渐主导模型的泛化误差；</li>
<li>当训练充足后，模型的<strong>拟合能力过强</strong>（数据的轻微扰动都会导致模型产生显著的变化），此时即发生<strong>过拟合</strong>（训练数据自身的、非全局的特征也被模型学习了）</li>
<li>偏差和方差的关系和<strong>模型容量</strong>（模型复杂度）、<strong>欠拟合</strong>和<strong>过拟合</strong>的概念紧密相联<br>
<img src="/2019/05/08/Try-your-best!You-will-find-the-job/TIM截图20180817214034.png" height></li>
<li>当模型的容量增大（x 轴）时， 偏差（用点表示）随之减小，而方差（虚线）随之增大</li>
<li>沿着 x 轴存在<strong>最佳容量</strong>，<strong>小于最佳容量会呈现欠拟合</strong>，<strong>大于最佳容量会导致过拟合</strong>。</li>
</ul>
<h4 id="欠拟合与过拟合">欠拟合与过拟合</h4>
<ul>
<li><strong>欠拟合</strong>指模型不能在<strong>训练集</strong>上获得足够低的<strong>训练误差</strong></li>
<li><strong>过拟合</strong>指模型的<strong>训练误差</strong>与<strong>测试误差</strong>（泛化误差）之间差距过大</li>
<li>反映在<strong>评价指标</strong>上，就是模型在训练集上表现良好，但是在测试集和新数据上表现一般（<strong>泛化能力差</strong>）</li>
</ul>
<hr>
<h4 id="根据不同的坐标方式图解欠拟合与过拟合">根据不同的坐标方式，图解欠拟合与过拟合</h4>
<ol style="list-style-type: decimal">
<li><strong>横轴为训练样本数量，纵轴为误差</strong></li>
</ol>
<div class="figure">
<img src="/2019/05/08/Try-your-best!You-will-find-the-job/2.16.4.1.jpg">

</div>
<ul>
<li>模型欠拟合：在训练集以及测试集上同时具有较高的误差，此时模型的偏差较大；</li>
<li>模型过拟合：在训练集上具有较低的误差，在测试集上具有较高的误差，此时模型的方差较大。</li>
<li>模型正常：在训练集以及测试集上，同时具有相对较低的偏差以及方差。</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li><strong>横轴为模型复杂程度，纵轴为误差</strong></li>
</ol>
<div class="figure">
<img src="/2019/05/08/Try-your-best!You-will-find-the-job/2.16.4.2.png">

</div>
<p>​ 红线为测试集上的Error, 蓝线为训练集上的Error</p>
<ul>
<li>模型欠拟合：模型在点A处，在训练集以及测试集上同时具有较高的误差，此时模型的偏差较大。</li>
<li>模型过拟合：模型在点C处，在训练集上具有较低的误差，在测试集上具有较高的误差，此时模型的方差较大。</li>
<li>模型正常：模型复杂程度控制在点B处为最优。</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li><strong>横轴为正则项系数，纵轴为误差</strong></li>
</ol>
<div class="figure">
<img src="/2019/05/08/Try-your-best!You-will-find-the-job/2.16.4.3.png">

</div>
<p>​ 红线为测试集上的Error,蓝线为训练集上的Error</p>
<ul>
<li>模型欠拟合：模型在点C处，在训练集以及测试集上同时具有较高的误差，此时模型的偏差较大。</li>
<li>模型过拟合：模型在点A处，在训练集上具有较低的误差，在测试集上具有较高的误差，此时模型的方差较大。</li>
<li>模型正常：模型复杂程度控制在点B处为最优。</li>
</ul>
<h4 id="降低-过拟合-的方法">降低 过拟合 的方法</h4>
<ul>
<li><strong>数据增强</strong></li>
<li>图像：平移、旋转、缩放</li>
<li>利用<strong>生成对抗网络</strong>（GAN）生成新数据</li>
<li>NLP：利用机器翻译生成新数据</li>
<li><strong>增加正则化项</strong>（权值约束）</li>
<li>L1 正则化</li>
<li>L2 正则化</li>
<li><strong>增大正则化项系数</strong></li>
<li><strong>降低模型复杂度</strong></li>
<li>神经网络：减少网络层、神经元个数</li>
<li>决策树：降低树的深度、剪枝</li>
<li><strong>采用Dropout方法</strong></li>
<li>Dropout方法，通俗的讲就是在训练的时候让神经元以一定的概率不工作</li>
<li><strong>提前终止 early stopping</strong></li>
<li><strong>集成学习</strong></li>
<li>神经网络：Dropout</li>
<li>决策树：随机森林、GBDT</li>
</ul>
<h4 id="降低-欠拟合-的方法">降低 欠拟合 的方法</h4>
<ul>
<li><strong>加入新的特征</strong></li>
<li>交叉特征、多项式特征、…</li>
<li>深度学习：因子分解机、Deep-Crossing、自编码器</li>
<li><strong>增加模型复杂度</strong></li>
<li>线性模型：添加高次项</li>
<li>神经网络：增加网络层数、神经元个数</li>
<li><strong>减小正则化项的系数</strong></li>
<li>添加正则化项是为了限制模型的学习能力，减小正则化项的系数则可以放宽这个限制</li>
<li>模型通常更倾向于更大的权重，更大的权重可以使模型更好的拟合数据</li>
</ul>
<h4 id="l1l2-范数正则化">L1/L2 范数正则化</h4>
<h4 id="l1l2-范数的作用异同">L1/L2 范数的作用、异同</h4>
<p><strong>相同点</strong></p>
<ul>
<li>限制模型的学习能力——通过限制参数的规模，使模型偏好于<strong>权值较小</strong>的目标函数，防止过拟合。</li>
</ul>
<p><strong>不同点</strong></p>
<ul>
<li><strong>L1 正则化</strong>可以产生更<strong>稀疏</strong>的权值矩阵，可以用于特征选择，同时一定程度上防止过拟合；<strong>L2 正则化</strong>主要用于防止模型过拟合</li>
<li><strong>L1 正则化</strong>适用于特征之间有关联的情况；<strong>L2 正则化</strong>适用于特征之间没有关联的情况。</li>
</ul>
<h4 id="为什么-l1-和-l2-正则化可以防止过拟合">为什么 L1 和 L2 正则化可以防止过拟合？</h4>
<ul>
<li>L1 &amp; L2 正则化会使模型偏好于更小的权值。</li>
<li>更小的权值意味着<strong>更低的模型复杂度</strong>；添加 L1 &amp; L2 正则化相当于为模型添加了某种<strong>先验</strong>，限制了参数的分布，从而降低了模型的复杂度。</li>
<li>模型的复杂度降低，意味着模型对于噪声与异常点的抗干扰性的能力增强，从而提高模型的泛化能力。——直观来说，就是对训练数据的拟合刚刚好，不会过分拟合训练数据（比如异常点，噪声）——<strong>奥卡姆剃刀原理</strong></li>
</ul>
<h4 id="为什么-l1-正则化可以产生稀疏权值而-l2-不会">为什么 L1 正则化可以产生稀疏权值，而 L2 不会？</h4>
<ul>
<li><p>对目标函数添加范数正则化，训练时相当于在范数的约束下求目标函数 <code>J</code> 的最小值</p></li>
<li><p>带有<strong>L1 范数</strong>（左）和<strong>L2 范数</strong>（右）约束的二维图示</p></li>
</ul>
<div class="figure">
<img src="/2019/05/08/Try-your-best!You-will-find-the-job/TIM截图20180608171710.png">

</div>
<div class="figure">
<img src="/2019/05/08/Try-your-best!You-will-find-the-job/TIM截图20180608172312.png">

</div>
<ul>
<li>图中 <code>J</code> 与 <code>L1</code> 首次相交的点即是最优解。<code>L1</code> 在和每个坐标轴相交的地方都会有“<strong>顶点</strong>”出现，多维的情况下，这些顶点会更多；在顶点的位置就会产生稀疏的解。而 <code>J</code> 与这些“顶点”相交的机会远大于其他点，因此 <code>L1</code> 正则化会产生稀疏的解。</li>
<li><code>L2</code> 不会产生“<strong>顶点</strong>”，因此 <code>J</code> 与 <code>L2</code> 相交的点具有稀疏性的概率就会变得非常小。</li>
</ul>
<h4 id="交叉验证的主要作用">交叉验证的主要作用</h4>
<p>为了得到更为稳健可靠的模型，使用验证集对模型的泛化误差进行评估，得到模型泛化误差的近似值。当有多个模型可以选择时，我们通常选择“泛化误差”最小的模型。</p>
<p>常用的交叉验证方法：<strong>留一交叉验证</strong>、<strong>k折交叉验证</strong></p>
<h4 id="k-折交叉验证"><span class="math inline">\(k\)</span> 折交叉验证</h4>
<p><strong><span class="math inline">\(k\)</span> 折交叉验证：</strong>将含有 <span class="math inline">\(N\)</span> 个样本的数据集，分成 <span class="math inline">\(k\)</span> 份，每份含有 <span class="math inline">\(N/k\)</span> 个样本。选择其中1份作为测试集，另外 <span class="math inline">\(k-1\)</span> 份作为训练集。这样就可以获得 <span class="math inline">\(K\)</span> 组训练/测试集，从而可以进行 <span class="math inline">\(k\)</span> 次训练和测试，最终返回这 <span class="math inline">\(k\)</span> 个测试结果的均值，做为模型最终的泛化误差。一般 <span class="math inline">\(2 \leq k \leq 10\)</span> ，<span class="math inline">\(k\)</span> 最常用的取值是 10，此时称为<strong>10折交叉验证</strong>：</p>
<div class="figure">
<img src="/2019/05/08/Try-your-best!You-will-find-the-job/k-折交叉验证.png">

</div>
<p><strong>10次10折交叉验证：</strong> 则是如上重复做了10次，每次的10折交叉验证随机使用不同的划分。</p>
<blockquote>
<p>训练集和测试集必须从完整的数据集中均匀取样。均匀取样的目的是希望减少训练集、测试集与原数据集之间的偏差。当样本数量足够多时，通过随机取样，便可以实现均匀取样的效果。</p>
</blockquote>
<h4 id="precision和recall">Precision和Recall</h4>
<p>对于二分类问题，可将样例 根据其真实类别与学习器预测类别的组合划分为：</p>
<ul>
<li><strong>TP</strong> (True Positive)： 预测为真，实际为真</li>
<li><strong>FP</strong> (False Positive)： 预测为真，实际为假</li>
<li><strong>TN </strong>(True Negative)： 预测为假，实际为假</li>
<li><strong>FN</strong> (False Negative)：预测为假，实际为真</li>
</ul>
<p>令 TP、FP、TN、FN分别表示其对应的样例数，则显然有 <strong>TP + FP + TN + FN = 样例总数</strong>分类结果的 <strong>“混淆矩阵”</strong> 如下：</p>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center">实际为真 T</th>
<th align="center">实际为假 F</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>预测为正例 P</strong></td>
<td align="center"><strong>TP</strong> (预测为1，实际为1)</td>
<td align="center"><strong>FP</strong> (预测为1，实际为0)</td>
</tr>
<tr class="even">
<td align="center"><strong>预测为负例 N</strong></td>
<td align="center"><strong>FN</strong> (预测为0，实际为1)</td>
<td align="center"><strong>TN</strong> (预测为0，实际为0)</td>
</tr>
</tbody>
</table>
<div class="figure">
<img src="/2019/05/08/Try-your-best!You-will-find-the-job/precision_recall.png">

</div>
<p><span class="math display">\[
（查准率）Precision = \frac{TP}{TP  + FP} \\ \\ \\
{\color{Purple}{（预测的好瓜中有多少是真的好瓜）}}
\]</span></p>
<p><span class="math display">\[
（查全率）Recall  = \frac{TP}{TP + FN} \\ \\
{\color{Purple}{（所有真正的好瓜中有多少被真的挑出来了）}}
\]</span></p>
<h4 id="p-r曲线">P-R曲线</h4>
<p>一般来说，查准率高时，查全率往往偏低，而查全率高时，查准率往往偏低。通常只有在一些简单任务中，才可能使得查全率和查准率都很高。在很多情况，我们可以根据学习器的预测结果，得到对应预测的 confidence scores 得分(有多大的概率是正例)，按照得分对样例进行排序，排在前面的是学习器认为”最可能“是正例的样本，排在最后的则是学习器认为“最不可能”是正例的样本。每次选择当前第 <span class="math inline">\(i\)</span> 个样例的得分作为阈值 <span class="math inline">\((1 \leq i \leq 样例个数)\)</span>，计算当前预测的前 <span class="math inline">\(i\)</span> 为正例的查全率和查准率。然后以<strong>查全率为横坐标</strong>，<strong>查准率为纵坐标</strong>作图，就得到了我们的查准率-查全率曲线: <strong>P-R曲线</strong></p>
<h4 id="roc与auc">ROC与AUC</h4>
<p><strong>ROC</strong> 全称是“受试者工作特征”（Receiver Operating Characteristic）。ROC 曲线下的面积就是 <strong>AUC</strong>（Area Under the Curve）。AUC用于衡量“二分类问题”机器学习算法性能（泛化能力）。</p>
<blockquote>
<p>思想：和计算 P-R 曲线方法基本一致，只是这里计算的是 真正率(True Positive rate) 和 假正率(False Positive rate)，以 FPR 为横轴，TPR 为纵轴，绘制的曲线就是 ROC 曲线，ROC 曲线下的面积，即为 AUC</p>
</blockquote>
<p><span class="math display">\[
（真正率）TPR = \frac{TP}{TP + FN}
\]</span></p>
<p><span class="math display">\[
（假正率）FPR = \frac{FP}{FP + TN}
\]</span></p>
<h4 id="map">mAP</h4>
<p>接下来说说 <strong>AP</strong> 的计算，此处参考的是 <code>PASCAL  VOC  CHALLENGE</code> 的计算方法。首先按照 R-P曲线 计算方式计算 Precision 和 Recall。然后设定一组阈值，[0, 0.1, 0.2, …, 1]，计算 Recall 大于第 <span class="math inline">\(i\)</span> 个阈值的R-P集合中，对应的 Precision 的最大值。这样，我们就计算出了11个 Precision。<strong>mAP</strong> 即为这11个 Precision 的平均值。这种方法英文叫做 <code>11-point interpolated average precision</code><br>
相应的Precision-Recall曲线（这条曲线是单调递减的）如下：</p>
<div class="figure">
<img src="/2019/05/08/Try-your-best!You-will-find-the-job/mAP.png">

</div>
<p><strong>AP</strong> 衡量的是学出来的模型在每个类别上的好坏，<strong>mAP</strong> 衡量的是学出的模型在所有类别上的好坏，得到 AP 后 mAP 的计算就变得很简单了，就是取所有 AP 的平均值。</p>
<div class="figure">
<img src="/2019/05/08/Try-your-best!You-will-find-the-job/result.png">

</div>
<hr>
<h3 id="激活函数">激活函数</h3>
<h4 id="为什么需要激活函数">为什么需要激活函数？</h4>
<ul>
<li>激活函数对模型学习、理解非常复杂的、和非线性的函数具有重要作用</li>
<li>使用<strong>激活函数</strong>的目的是为了向网络中加入<strong>非线性因素</strong> 。从而加强网络的表示能力，解决<strong>线性模型</strong>无法解决的问题</li>
</ul>
<h4 id="为什么要使用非线性激活函数">为什么要使用非线性激活函数？</h4>
<blockquote>
<p><strong>神经网络的万能近似定理</strong>认为，神经网络具有至少一个非线性隐藏层，那么只要给予网络足够数量的隐藏单元，它就可以以任意的精度来近似拟合任何<strong>从一个有限维空间到另一个有限维空间</strong>的函数。</p>
</blockquote>
<ul>
<li>如果不使用非线性激活函数，那么每一层输出都是上层输入的<strong>线性组合</strong>；此时无论网络有多少层，其整体也将是线性的，就做不到用非线性来逼近任意函数，导致失去万能近似的性质</li>
<li>使用非线性激活函数 ，可以增强网络的表示能力，使它可以学习从输入到输出之间复杂的非线性的映射。而且，仅<strong>部分层是纯线性</strong>是可以接受的，这有助于<strong>减少网络中的参数</strong>。</li>
</ul>
<h4 id="什么时候可以用线性激活函数">什么时候可以用线性激活函数</h4>
<ul>
<li>输出层，大多使用线性激活函数</li>
<li>在隐含层可能会使用一些线性激活函数</li>
<li>一般用到的线性激活函数很少</li>
</ul>
<h4 id="常见的激活函数">常见的激活函数</h4>
<ul>
<li><span class="math inline">\(Sigmoid\)</span></li>
</ul>
<p><span class="math inline">\(Sigmod\)</span> 又叫作 <strong><span class="math inline">\(Logistic\)</span> 激活函数</strong>，它将实数值压缩进 0 到 1 的区间内，还可以在预测概率的输出层中使用。该函数将大的负数转换成 0，将大的正数转换成 1<br>
数学公式为：<br>
<span class="math display">\[
y = \sigma(x) = \frac{1}{1 + e^{-x}}   \\
y&#39; = y * (1 - y)
\]</span><br>
下图展示了 Sigmoid 函数及其导数：</p>
<div class="figure">
<img src="/2019/05/08/Try-your-best!You-will-find-the-job/sigmod_01.png" alt="Sigmoid激活函数">
<p class="caption">Sigmoid激活函数</p>
</div>
<div class="figure">
<img src="/2019/05/08/Try-your-best!You-will-find-the-job/sigmod_02.png" alt="Sigmoid激活函数导数">
<p class="caption">Sigmoid激活函数导数</p>
</div>
<ul>
<li><span class="math inline">\(Sigmoid\)</span> 函数的三个主要缺陷：</li>
<li><strong>梯度消失</strong>：Sigmoid 函数在输入取绝对值非常大的正值或负值时会出现 <strong>饱和</strong> 现象，在图像上表现为变得很平缓，此时函数会对输入的微小变化不敏感，即此时的梯度趋近于0，造成梯度消失，网络权重更新缓慢或不更新。</li>
<li><strong>计算成本高昂</strong>：<span class="math inline">\(exp()\)</span> 函数与其他非线性激活函数相比，计算成本高昂</li>
<li><strong>不以零为中心</strong>：Sigmoid 输出不以零为中心的</li>
</ul>
<hr>
<ul>
<li><span class="math inline">\(Tanh\)</span> 函数</li>
</ul>
<p><span class="math display">\[
\tanh (x)=2 \sigma(2 x)-1=\frac{\mathrm{e}^{x}-\mathrm{e}^{-x}}{\mathrm{e}^{x}+\mathrm{e}^{-x}} \\
\tanh ^{\prime}(x)=1-\tanh ^{2}(x)
\]</span></p>
<p><span class="math inline">\(Tanh\)</span> 函数又叫作<strong>双曲正切激活函数</strong>。与 <span class="math inline">\(Sigmoid\)</span> 函数类似，区别是值域为 <span class="math inline">\((-1, 1)\)</span> ，且 <span class="math inline">\(Tanh\)</span> 函数的输出以零为中心，因为区间在 <span class="math inline">\(-1\)</span> 到 <span class="math inline">\(1\)</span> 之间。</p>
<div class="figure">
<img src="/2019/05/08/Try-your-best!You-will-find-the-job/tanh_01.png" alt="Tanh函数">
<p class="caption">Tanh函数</p>
</div>
<div class="figure">
<img src="/2019/05/08/Try-your-best!You-will-find-the-job/tanh_02.png" alt="Tanh函数导数">
<p class="caption">Tanh函数导数</p>
</div>
<ul>
<li>缺点：</li>
<li><strong>梯度消失</strong>： <span class="math inline">\(Tanh\)</span> 函数也会有梯度消失的问题，因此在饱和时也会「杀死」梯度。</li>
<li><strong>计算成本高昂</strong>：<span class="math inline">\(exp()\)</span> 函数与其他非线性激活函数相比，计算成本高昂</li>
</ul>
<h4 id="为什么tanh收敛速度比sigmoid快">为什么Tanh收敛速度比Sigmoid快</h4>
<ul>
<li><span class="math inline">\(tanh^{&#39;}(x)=1-tanh(x)^{2}\in (0,1)\)</span></li>
<li><span class="math inline">\(s^{&#39;}(x)=s(x)*(1-s(x))\in (0,\frac{1}{4}]\)</span></li>
</ul>
<p>由上面两个公式可知 <span class="math inline">\(Tanh\)</span> 梯度消失的问题比 <span class="math inline">\(Sigmoid\)</span> 轻，所以 <span class="math inline">\(Tanh\)</span> 收敛速度比 <span class="math inline">\(Sigmoid\)</span> 快。</p>
<hr>
<ul>
<li><span class="math inline">\(Relu\)</span> 函数</li>
</ul>
<p><span class="math inline">\(ReLU\)</span> 是从底部开始半修正的一种函数，数学公式为：<br>
<span class="math display">\[
f(x) = max(0, x)
\]</span><br>
<img src="/2019/05/08/Try-your-best!You-will-find-the-job/ReLU_01.png"><br>
<img src="/2019/05/08/Try-your-best!You-will-find-the-job/ReLU_02.png"></p>
<ul>
<li>优点：</li>
<li><strong>加速网络训练：</strong>当输入 <span class="math inline">\(x &lt; 0\)</span> 时，输出为 0，当 <span class="math inline">\(x &gt; 0\)</span> 时，输出为 <span class="math inline">\(x\)</span>。该激活函数使网络更快速地收敛。</li>
<li><strong>避免梯度消失</strong>： <span class="math inline">\(ReLU\)</span> 的导数始终是一个常数，负半区为 0，正半区为 1，所以不会发生梯度消失现象。而 <span class="math inline">\(Sigmoid\)</span> 函数在输入取绝对值非常大的正值或负值时会出现<strong>饱和</strong>现象.</li>
<li><strong>减缓过拟合</strong>：<span class="math inline">\(ReLU\)</span> 在负半区的输出为 0。一旦神经元的激活值进入负半区，那么该激活值就不会产生梯度/不会被训练，造成了网络的稀疏性——<strong>稀疏激活</strong> 。这有助于减少参数的相互依赖，缓解过拟合问题的发生</li>
<li><strong>加速计算</strong>：<span class="math inline">\(ReLU\)</span> 的求导不涉及浮点运算，所以速度更快</li>
<li>缺点：</li>
<li><strong>不以零为中心</strong>：和 <span class="math inline">\(Sigmoid\)</span> 激活函数类似，<span class="math inline">\(ReLU\)</span> 函数的输出不以零为中心。</li>
<li>容易造成神经元死亡现象Deed ReLU <a href="https://blog.csdn.net/disiwei1012/article/details/79204243" target="_blank" rel="noopener">Reference</a>：比如对于一个神经元，当有一个比较大的梯度传递过来，导致该神经元的参数分布发生比较大的变化，变成一个低方差，中心在-0.1的高斯分布，这样以后，大部分数据输入该神经元之后为0，不能通过反向传播更新其参数，造成神经元死亡。这于是就引入了 <span class="math inline">\(Leaky ReLU\)</span> 来解决该问题。</li>
</ul>
<hr>
<ul>
<li><span class="math inline">\(Leaky\ ReLU\)</span></li>
</ul>
<p>该函数试图缓解 <code>dead ReLU</code> 问题。数学公式为：<br>
<span class="math display">\[
f(x) = max(0.1x, x)
\]</span><br>
<img src="/2019/05/08/Try-your-best!You-will-find-the-job/Leaky_ReLU_01.png"></p>
<p><span class="math inline">\(Leaky\ ReLU\)</span> 的概念是：当 <span class="math inline">\(x &lt; 0\)</span> 时，它得到 0.1 的正梯度。该函数一定程度上缓解了 <code>dead ReLU</code> 问题，但是使用该函数的结果并不连贯。尽管它具备 <span class="math inline">\(ReLU\)</span> 激活函数的所有特征，如计算高效、快速收敛、在正区域内不会饱和。</p>
<p><span class="math inline">\(Leaky\ ReLU\)</span> 可以得到更多扩展。不让 <span class="math inline">\(x\)</span> 乘常数项，而是让 <span class="math inline">\(x\)</span> 乘超参数，这看起来比 <span class="math inline">\(Leaky\ ReLU\)</span> 效果要好。该扩展就是 <span class="math inline">\(Parametric\ ReLU\)</span>。</p>
<hr>
<ul>
<li><span class="math inline">\(Parametric\ ReLU\)</span></li>
</ul>
<p><span class="math display">\[
f(x) = max(ax, x)
\]</span></p>
<p>其中 <span class="math inline">\(\alpha\)</span> 是一个可以学习的参数，因为你可以对它进行反向传播。这使神经元能够选择负区域最好的梯度，有了这种能力，它们可以变成 ReLU 或 Leaky ReLU。</p>
<h4 id="怎样理解-relu-0-时是非线性激活函数">怎样理解 ReLU（&lt; 0 时）是非线性激活函数</h4>
<p>从 <span class="math inline">\(ReLU\)</span> 的图像可以看出具有一下特点：</p>
<ul>
<li>单侧抑制</li>
<li>相对宽阔的兴奋边界</li>
<li>稀疏激活性</li>
</ul>
<p><span class="math inline">\(ReLU\)</span> 函数从图像上看，是一个分段线性函数，把所有的负值都变为 0，而正值不变，这样就成为单侧抑制。因为有了这单侧抑制，才使得神经网络中的神经元也具有了稀疏激活性。</p>
<hr>
<h3 id="损失函数代价函数">损失函数/代价函数</h3>
<blockquote>
<ol style="list-style-type: decimal">
<li>损失函数 <span class="math inline">\(| y_i-f(x_i)|\)</span> ，一般是针对单个样本 <span class="math inline">\(i\)</span></li>
<li>代价函数 <span class="math inline">\(\frac{1}{N}\sum_{i=1}^{N}{| y_i-f(x_i)}|\)</span> ，一般是针对总体<span class="math inline">\(N\)</span></li>
<li>目标函数 <span class="math inline">\(\frac{1}{N}\sum_{i=1}^{N}{| y_i-f(x_i)}| + 正则化项\)</span></li>
</ol>
<p><strong>(其实 <code>损失函数</code> 和 <code>代价函数</code> 可以理解成同一个东西)</strong></p>
</blockquote>
<p><strong>损失函数</strong>（loss function）是用来估量你模型的预测值 <span class="math inline">\(f(X)\)</span> 与真实值 <span class="math inline">\(Y\)</span> 的不一致程度，它是一个非负实值函数，通常使用 <span class="math inline">\(L(Y, f(X))\)</span> 来表示，损失函数越小，模型的鲁棒性就越好。训练数据集的平均损失称为<strong>经验风险</strong> 。<strong>结构风险最小化</strong> 是为了防止过拟合而提出来的策略，结构风险在经验风险基础上加上正则化项(regularizer)或惩罚项(penalty term)，[李航P8] 通常可以表示为如下形式：<br>
<span class="math display">\[
\theta^{*}=\arg \min _{\theta} \frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i} ; \theta\right)\right)+\lambda \Phi(\theta)
\]</span><br>
其中，前面的均值函数表示的是经验风险函数，<span class="math inline">\(L\)</span> 代表的是损失函数，后面的 <span class="math inline">\(Φ\)</span> 是正则化项，它可以是 <span class="math inline">\(L1\)</span>，也可以是 <span class="math inline">\(L2\)</span>，或者其他的正则函数。整个式子表示的意思是<strong>找到使目标函数最小时的 <span class="math inline">\(θ\)</span> 值</strong>。</p>
<h4 id="为什么需要损失代价函数">为什么需要损失/代价函数</h4>
<p>用于找到最优解的目标/假设函数（用于衡量假设函数的准确性）比如：为了得到逻辑回归模型的参数，需要一个代价函数，通过训练代价函数来得到参数。</p>
<h4 id="损失代价函数的作用及原理">损失/代价函数的作用及原理</h4>
<p>使用线性回归的例子说明，用一条直线，拟合给定的数据，使用均方误差损失函数，梯度下降法来拟合。</p>
<h4 id="为什么损失代价函数要非负">为什么损失/代价函数要非负</h4>
<p>目标函数存在一个下界，在优化过程当中，如果优化算法能够使目标函数不断减小，根据单调有界准则，这个优化算法就能证明是收敛有效的。只要设计的目标函数有下界，基本上都可以，代价函数非负更为方便。</p>
<h4 id="常用的损失函数">常用的损失函数</h4>
<p><strong>（1）0-1 损失函数</strong></p>
<p>如果 预测值 <span class="math inline">\(f(X)\)</span> 和目标值 <span class="math inline">\(Y\)</span> 相等，值为 0，如果不相等，值为 1：<br>
<span class="math display">\[
L(Y, f(X)) =
\begin{cases}
1,&amp; Y\ne f(X)\\
0,&amp; Y = f(X)
\end{cases}
\]</span><br>
一般的在实际使用中，相等的条件过于严格，可适当放宽条件：<br>
<span class="math display">\[
L(Y, f(X)) =
\begin{cases}
1,&amp; |Y-f(X)|\geqslant T\\
0,&amp; |Y-f(X)|&lt; T
\end{cases}
\]</span><br>
<strong>（2）绝对值损失函数（L1 Loss、LAE）</strong></p>
<p>L1范数损失函数，也称作<strong>最小绝对值偏差</strong>( least absolute deviations, <strong>LAD</strong>)，<strong>最小绝对值误差</strong>(least absolute errors, <strong>LAE</strong>)。目的是将估计值 <span class="math inline">\(f(X)\)</span> 和 目标值 <span class="math inline">\(Y\)</span> 的绝对差值的总和 <span class="math inline">\(L\)</span> 最小化：<br>
<span class="math display">\[
L(Y, f(X)) = |Y-f(X)|  \\ \color{green}{（此时的 \ X \ 是只有一个样本的输入）}
\]</span><br>
在实际应用中，<span class="math inline">\(X\)</span> 样本个数为 <span class="math inline">\(m\)</span> ，通常会使用 <strong>平均最小绝对值误差 (MAE)</strong> 做为 L1 Loss 形式 ：<br>
<span class="math display">\[
L(Y, f(X)) = \frac{1}{m}\sum_{i=1}^m|Y-f(X)|   \\ \color{green}{（此时的 \ X \ 有 \ m \ 个样本的输入）}
\]</span><br>
<strong>（3）平方损失函数 (L2 Loss、LSE、最小二乘法OLS)</strong></p>
<p>基于均方误差最小化(平方损失函数)来进行模型求解的方法称为: <span class="math inline">\(\mathbf{\color{green}{最小二乘法}}\)</span>. 在线性回归中，最小二乘法就是试图找到一条直线，使得所有样本到直线上的欧氏距离之和最小. (其假设样本和噪声都<strong>服从<span class="math inline">\(\mathbf{\color{blue}{高斯分布}}\)</span></strong>，然后通过极大似然估计(MLE)可以推导出最小二乘式子)<br>
即 <strong>OLS</strong> 是<strong>基于距离的</strong>，而这个距离就是我们用的最多的欧几里得距离。为什么它会选择使用欧式距离作为误差度量呢？ (即Mean squared error， MSE)，主要有以下几个原因：</p>
<blockquote>
<ol style="list-style-type: decimal">
<li>简单，计算方便</li>
<li>欧氏距离是一种很好的相似性度量标准</li>
<li>在不同的表示域变换后特征性质不变</li>
</ol>
</blockquote>
<ul>
<li>平方损失（Square loss）的标准形式如下：</li>
</ul>
<p><span class="math display">\[
L(Y,f(X))=(Y-f(X))^2.
\]</span><br>
- 当样本个数为 <span class="math inline">\(m\)</span> 时，此时损失函数变为：</p>
<p><span class="math display">\[
L(Y, f(X)) = \sum_{i = 1}^{m}(Y - f(X))
\]</span></p>
<p><span class="math inline">\(Y-f(X)\)</span> 表示的是 <strong>残差</strong>，整个式子表示的是 <strong>残差的平方和</strong>，而我们的目的就是最小化这个目标函数值 (注：该式子未加入正则项)，也就是<u>最小化残差的平方和</u>（residual sum of squares, RSS）<br>
而在实际应用中，通常会使用 <strong>均方误差 (MSE)</strong> 作为一项衡量指标，公式如下：<br>
<span class="math display">\[
MSE = \frac{1}{m}\sum_{i = 1}^{m}(Y - f(X))^2
\]</span><br>
<strong>（4）对数损失函数（逻辑回归）</strong><br>
<span class="math display">\[
L(Y,P(Y|X))=-logP(Y|X).
\]</span><br>
常见的<u>逻辑回归使用的就是对数损失函数</u>，而不是平方损失。逻辑回归它假设样本服从 <strong><span class="math inline">\(\mathbf{\color{blue}{伯努利分布（0-1分布）}}\)</span></strong>，进而求得满足该分布的似然函数，接着取对数求极值等。而逻辑回归并没有求似然函数的极值，而是把极大化当做是一种思想，进而推导出它的经验风险函数为：最小化负的似然函数，从损失函数的角度看， 就是 <span class="math inline">\(log\)</span> 损失函数。</p>
<p>损失函数 <span class="math inline">\(L(Y, P(Y|X))\)</span> 表达的是样本 <span class="math inline">\(X\)</span> 在分类 <span class="math inline">\(Y\)</span> 的情况下，使概率 <span class="math inline">\(P(Y|X)\)</span> 达到最大值（换言之，<strong>就是利用已知的样本分布，找到最有可能（即最大概率）导致这种分布的参数值；或者说什么样的参数才能使我们观测到目前这组数据的概率最大</strong>）因为 <span class="math inline">\(log\)</span> 函数是单调递增的，所以 <span class="math inline">\(logP(Y|X)\)</span> 也会达到最大值，因此在前面加上负号之后，最大化 <span class="math inline">\(P(Y|X)\)</span> 就等价于最小化 <span class="math inline">\(L\)</span> 了。</p>
<p><strong>逻辑回归</strong> 的 <span class="math inline">\(P(Y=y|x)\)</span> 表达式如下（为了将类别标签 <span class="math inline">\(y\)</span> 统一为1和0，下面将表达式分开表示）：<br>
<span class="math display">\[
P(Y=y | x)=\left\{\begin{array}{cc}{h_{\theta}(x)=g(f(x))=\frac{1}{1+\exp (-f(x)\}}} &amp; {, y=1} \\ {1-h_{\theta}(x)=1-g(f(x))=\frac{1}{1+\exp (f(x)\}}} &amp; {, y=0}\end{array}\right.
\]</span><br>
将它带入到上式，通过推导可以得到logistic的损失函数表达式，如下：<br>
<span class="math display">\[
L(y, P(Y=y | x))=\left\{\begin{array}{cc}{\log (1+\exp \{-f(x)\})} &amp; {, y=1} \\ {\log (1+\exp \{f(x)\})} &amp; {, y=0}\end{array}\right.
\]</span><br>
逻辑回归最后得到的目标式子如下：<br>
<span class="math display">\[
J(\theta)=-\frac{1}{m} \sum_{i=1}^{m}\left[y^{(i)} \log h_{\theta}\left(x^{(i)}\right)+\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right]
\]</span><br>
<strong>（5）指数损失函数</strong><br>
  <br>
指数损失函数的标准形式为：<br>
<span class="math display">\[
L(y|f(x))=exp[-yf(x)].
\]</span><br>
例如 <code>AdaBoost</code> 就是以指数损失函数为损失函数。</p>
<p><strong>（6）Hinge 损失函数</strong><br>
  <br>
Hinge 损失函数的标准形式如下：<br>
<span class="math display">\[
L(y)=max(0, 1-ty).
\]</span><br>
其中 <span class="math inline">\(y\)</span> 是预测值，范围为 <span class="math inline">\((-1,1), t\)</span> 为目标值，其为 <span class="math inline">\(-1 或 1\)</span>。<br>
在线性支持向量机中，最优化问题可等价于：<br>
<span class="math display">\[
\underset{w,b}{min}\sum_{i=1}^{N}(1-y_i(wx_i+b))+\lambda \lVert w^2 \rVert
\]</span></p>
<p><span class="math display">\[
\frac{1}{m}\sum_{i=1}^{N}l(wx_i+by_i))+\lVert w^2 \rVert
\]</span></p>
<p>其中 <span class="math inline">\(l(wx_i+by_i))\)</span> 是Hinge损失函数，<span class="math inline">\(\lVert w^2 \rVert\)</span> 可看做为正则化项。</p>
<p>（7）<strong>Softmax 函数</strong></p>
<p>函数定义为<br>
<span class="math display">\[
 \sigma(z)_j = \frac{e^{z_j}}{\sum_{k=1}^K e^{z_k}} 
\]</span><br>
Softmax 多用于多分类神经网络输出。</p>
<blockquote>
<p>Reference: <a href="http://www.csuldw.com/2016/03/26/2016-03-26-loss-function/" target="_blank" rel="noopener">机器学习-损失函数</a></p>
</blockquote>
<h4 id="熵条件熵kl散度交叉熵">熵、条件熵、KL散度、交叉熵</h4>
<blockquote>
<p>信息论背后的原理是：从不太可能发生的事件中能学到更多的有用信息。</p>
<ul>
<li>发生可能性较大的事件包含较少的信息。</li>
<li>发生可能性较小的事件包含较多的信息。</li>
<li>独立事件包含额外的信息 。</li>
</ul>
</blockquote>
<blockquote>
<p><strong>【通俗理解】</strong></p>
<ol style="list-style-type: decimal">
<li><strong>熵</strong>：可以表示一个事件A的自信息量，也就是A包含多少信息（表示随机变量不确定性的度量，所有可能发生的事件产生的信息量的期望）</li>
<li><strong>KL散度</strong>：可以用来表示从事件B的角度来看，事件A有多大不同</li>
<li><strong>交叉熵</strong>：可以用来表示从事件B的角度来看，如何描述事件A</li>
<li><strong>条件熵</strong>： H(Y|X) 表示已知随机变量X的条件下，随机变量Y的不确定性</li>
</ol>
<p><strong>【概括】</strong>：KL散度可以被用于计算代价，而在特定情况下最小化KL散度等价于最小化交叉熵。而交叉熵的运算更简单，所以用交叉熵来当做代价</p>
</blockquote>
<ol style="list-style-type: decimal">
<li><strong>熵(entropy)</strong>： <strong><code>在信息论中表示一个事件所包含的信息量，所有可能发生事件产生的信息量的期望 (对A时间中的随机变量进行编码所需要的最小字节数)</code></strong></li>
</ol>
<ul>
<li><strong>越不可能发生的事件信息量越大</strong>，比如“我不会死”这句话信息量就很大。<strong>而确定事件的信息量就很低</strong>， 比如“我是我妈生的”，信息量就很低甚至为0</li>
<li><strong>独立事件的信息量可叠加。</strong>比如“a. 张三今天喝了阿萨姆红茶，b. 李四前天喝了英式早茶”的信息量就应该恰好等于a+b的信息量，如果张三李四喝什么茶是两个独立事件。</li>
</ul>
<p><strong>自信息</strong>：对于事件 <span class="math inline">\(X = x\)</span> ，定义自信息 <code>self-information</code> 为： <span class="math inline">\(I(x) = -logP(x)\)</span> 自信息仅仅处理单个输出，但是如果计算自信息的期望，它就是熵.</p>
<p><strong>熵的定义</strong>：<br>
<span class="math display">\[
   S(x)=-\sum_{i= 1}^{n} P\left(x_{i}\right) \log P\left(x_{i}\right)
   \]</span><br>
<span class="math inline">\(x\)</span> 指不同的事件，比如喝茶。<span class="math inline">\(P(x_i)\)</span> 指的是某个事件发生的概率，比如喝红茶的概率。对于一个一定会发生的事件，其发生概率为1，<span class="math inline">\(S(x)=-\log (1) * 1=-0 * 1=0\)</span> ，信息量为0.</p>
<ol start="2" style="list-style-type: decimal">
<li><strong>KL散度/相对熵(Kullback-Leibler Divergence)：</strong> <strong><code>衡量两个事件(分布)之间的差异 (刻画了非真实分布B编码真实分布A带来的平均编码长度的增量)</code></strong></li>
</ol>
<p>对于给定的随机变量 <span class="math inline">\(X\)</span>，它的两个单独的概率分布函数 <span class="math inline">\(A(X) 和 B(X)\)</span> 的区别，可以用 <code>KL</code> 散度来度量。<strong>KL散度，有时候也叫KL距离，一般被用于计算两个分布之间的差异</strong>。看名字似乎跟计算两个点之间的距离也很像，但实则不然，因为KL散度<strong>不具备有对称性</strong>。在距离上的对称性指的是A到B的距离等于B到A的距离。</p>
<p><strong>KL散度的数学定义：</strong></p>
<ul>
<li>对于<strong>离散事件</strong>我们可以定义事件A和B的差别为(2.1)：</li>
</ul>
<p><span class="math display">\[
   D_{K L}(A \| B)=\sum_{i} P_{A}\left(x_{i}\right) \log \left(\frac{P_{A}\left(x_{i}\right)}{P_{B}\left(x_{i}\right)}\right)=\sum_{i} P_{A}\left(x_{i}\right) \log \left(P_{A}\left(x_{i}\right)\right)-P_{A}\left(x_{i}\right) \log \left(P_{B}\left(x_{i}\right)\right)
   \\ 事件 A 与 B之间的对数差 \ 在 A上的期望
   \]</span></p>
<ul>
<li>对于<strong>连续事件</strong>，那么我们只是把求和改为求积分而已(2.2)：</li>
</ul>
<p><span class="math display">\[
   D_{K L}(A \| B)=\int a(x) \log \left(\frac{a(x)}{b(x)}\right)
   \]</span></p>
<p>从公式中可以看出：</p>
<ul>
<li><strong>如果</strong> <span class="math inline">\(P_A=P_B\)</span> ，即两个事件/分布完全相同，那么KL散度等于0</li>
<li><strong>其中 <span class="math inline">\(\sum_{i} P_{A}\left(x_{i}\right) \log \left(P_{A}\left(x_{i}\right)\right)\)</span> 就是事件A的熵</strong></li>
<li>如果颠倒一下顺序求 <span class="math inline">\(D_{KL}(B||A)\)</span> ，那么就需要使用B的熵，答案就不一样了。<strong>所以KL散度来计算两个分布A与B的时候是不是对称的，有“坐标系”的问题</strong>，<span class="math inline">\(D_{K L}(A \| B) \neq D_{K L}(B \| A)\)</span></li>
</ul>
<blockquote>
<p>换句话说，KL散度由A自己的熵与B在A上的期望共同决定。当使用KL散度来衡量两个事件(连续或离散)，上面的公式意义就是<strong>求 A与B之间的对数差 在 A上的期望值</strong>。</p>
</blockquote>
<ol start="3" style="list-style-type: decimal">
<li><strong>交叉熵(cross entropy) = KL散度 + 熵</strong> <strong><code>描述两个事件之间的相互关系 (刻画了用非真实分布B来表示真实分布A中的样本的平均编码长度)</code></strong></li>
</ol>
<p>如果我们默认了用KL散度来计算两个分布间的不同，那还要交叉熵做什么？</p>
<p>事实上交叉熵和KL散度的公式非常相近，其实就是KL散度的后半部分(公式2.1)：A和B的交叉熵 = A与B的KL散度 + A的熵。<span class="math inline">\(D_{K L}(A \| B)=-S(A)+H(A, B)\)</span></p>
<ul>
<li><strong>KL散度的公式</strong></li>
</ul>
<p><span class="math display">\[
   D_{K L}(A \| B)=\sum_{i} P_{A}\left(x_{i}\right) \log \left(\frac{P_{A}\left(x_{i}\right)}{P_{B}\left(x_{i}\right)}\right)=\sum_{i} P_{A}\left(x_{i}\right) \log \left(P_{A}\left(x_{i}\right)\right)-P_{A}\left(x_{i}\right) \log \left(P_{B}\left(x_{i}\right)\right)
   \]</span></p>
<ul>
<li><strong>熵的公式</strong></li>
</ul>
<p><span class="math display">\[
   S(A)=-\sum_{i} P_{A}\left(x_{i}\right) \log P_{A}\left(x_{i}\right)
   \]</span></p>
<ul>
<li><strong>交叉熵公式</strong></li>
</ul>
<p><span class="math display">\[
   H(A, B)=-\sum_{i} P_{A}\left(x_{i}\right) \log \left(P_{B}\left(x_{i}\right)\right)
   \]</span></p>
<blockquote>
<p><strong>如果</strong> <span class="math inline">\(S(A)\)</span> <strong>是一个常量，那么</strong> <span class="math inline">\(D_{KL}(A||B) = H(A,B)\)</span> ，<strong>也就是说KL散度和交叉熵在特定条件下等价。</strong> <strong><code>比如我们的A为真实分布，B为非真实分布，即KL散度前者是真实分布的熵，后者是交叉熵，由于真实分布是固定的，所以信息熵的值是固定的。此时的KL散度 和 交叉熵是等价的。</code></strong></p>
</blockquote>
<ul>
<li><strong>交叉熵性质</strong>
<ul>
<li>和KL散度相同，交叉熵也不具备对称性：<span class="math inline">\(H(A, B) \neq H(B, A)\)</span></li>
<li>从名字上来看，Cross(交叉)主要是用于<strong>描述两个事件之间的相互关系</strong>，对自己求交叉熵等于熵。即 <span class="math inline">\(H(A, A)=S(A)\)</span> ，注意只是非负而不一定等于0。</li>
</ul></li>
<li><strong>交叉熵和KL散度的联系</strong>
<ul>
<li>不同之处：交叉熵中不包括“熵”的部分</li>
<li>相同之处：a. 都不具备对称性 b. 都是非负的</li>
<li>等价条件：当 A 固定不变时，那么最小化KL散度 $D_{KL}(A||B) $ 等价于最小化交叉熵 <span class="math inline">\(H(A,B)\)</span> 。<span class="math inline">\(D_{KL}(A||B) = H(A,B)\)</span></li>
</ul></li>
</ul>
<ol start="4" style="list-style-type: decimal">
<li><strong>条件熵(conditional entropy)</strong>： <strong><code>H(Y|X):表示已知随机变量X的条件下，随机变量Y的不确定性.</code></strong></li>
</ol>
<ul>
<li><strong>条件熵的公式</strong></li>
<li><strong><a href="https://zhuanlan.zhihu.com/p/26551798" target="_blank" rel="noopener">实例理解</a></strong></li>
</ul>
<p><span class="math display">\[
   \begin{aligned} H(Y | X) &amp;=\sum_{x \in X} p(x) H(Y | X=x) \\ &amp;=-\sum_{x \in X} p(x) \sum_{y \in Y} p(y | x) \log p(y | x) \\ &amp;=-\sum_{x \in X} \sum_{y \in Y} p(x, y) \log p(y | x) \end{aligned}
\]</span></p>
<ul>
<li><p><strong>总结</strong></p>
<ul>
<li><u>其实条件熵意思是按一个新的变量的每个值对原变量进行分类，</u>(比如上面这个题把嫁与不嫁按帅，不帅分成了俩类) <u>然后在每一个小类里面，都计算一个小熵，然后每一个小熵乘以各个类别的概率，然后求和。</u></li>
<li>我们用另一个变量对原变量分类后，原变量的不确定性就会减小了，因为新增了X的信息，可以感受一下。不确定程度减少了多少就是信息的增益。</li>
</ul></li>
<li><p><strong>性质</strong></p></li>
</ul>
<p><span class="math display">\[
H(X, Y)=H(Y | X)+H(X)
   \\ 即：描述 X 和 Y 所需要的信息是：描述 X 所需要的信息加上给定 X 条件下描述 Y 所需的额外信息。
\]</span></p>
<ol start="5" style="list-style-type: decimal">
<li><strong>机器如何“学习”</strong></li>
</ol>
<p>在机器学习中，我们希望在<strong>训练数据上模型学到的分布 <span class="math inline">\(P(model)\)</span> 和真实数据的分布 <span class="math inline">\(P(real)\)</span> 越接近越好，所以我们可以使其相对熵最小</strong>。但是我们没有真实数据的分布，所以只能希望模型学到的分布 <span class="math inline">\(P(model)\)</span> 和训练数据的分布 <span class="math inline">\(P(train)\)</span> 尽量相同。假设训练数据是从总体中独立同分布采样的，那么我们可以通过最小化训练数据的经验误差来降低模型的泛化误差。即：</p>
<ul>
<li>希望学到的模型的分布和真实分布一致，<span class="math inline">\(P(model)≃P(real)\)</span></li>
<li>但是真实分布不可知，假设训练数据是从真实数据中独立同分布采样的，<span class="math inline">\(P(train)≃P(real)\)</span></li>
<li>因此，我们希望学到的模型分布至少和训练数据的分布一致，<span class="math inline">\(P(train)≃P(model)\)</span></li>
</ul>
<p>由此非常理想化的看法是如果模型(左)能够学到训练数据(中)的分布，那么应该近似的学到了真数据(右)的分布：<span class="math inline">\(P(model)≃P(train)≃P(real)\)</span></p>
<ol start="6" style="list-style-type: decimal">
<li><strong>为什么交叉熵可以用作代价？</strong></li>
</ol>
<p>最小化模型分布 <span class="math inline">\(P(model)\)</span> 与训练数据上的分布 <span class="math inline">\(P(train)\)</span> 的差异，等价于最小化这两个分布间的KL散度，也就是最小化 <span class="math inline">\(D_{KL}(P(train)||P(model))\)</span></p>
<p>对比上面的KL散度公式</p>
<ul>
<li>A就是数据的真实分布：<span class="math inline">\(P(train)\)</span></li>
<li>B就是模型从训练数据上学到的分布：<span class="math inline">\(P(model)\)</span></li>
</ul>
<p>而训练数据的分布A是给定的，是固定的，即A的信息熵是固定的，此时求 <span class="math inline">\(D_KL(A||B)\)</span> 等价于求 <span class="math inline">\(H(A, B)\)</span> ，也就是 A 与 B 的交叉熵。<strong>得证，交叉熵可以用于计算“学习模型的分布”与“训练数据分布”之间的不同。当交叉熵最低时(等于训练数据分布的熵)，我们学到了“最好的模型”。</strong></p>
<blockquote>
<p>Reference: <a href="https://www.cnblogs.com/kyrieng/p/8694705.html" target="_blank" rel="noopener">参考一</a> <a href="https://www.zhihu.com/question/65288314/answer/244557337" target="_blank" rel="noopener">参考二</a> <a href="https://zhuanlan.zhihu.com/p/26486223" target="_blank" rel="noopener">参考三</a> <a href="https://zhuanlan.zhihu.com/p/32985487" target="_blank" rel="noopener">参考四</a> <a href="https://zhuanlan.zhihu.com/p/26551798" target="_blank" rel="noopener">参考五</a></p>
</blockquote>
<hr>
<h4 id="为什么用交叉熵代替二次代价均方误差函数">为什么用交叉熵代替二次代价(均方误差)函数</h4>
<p><strong>二次代价函数（quadratic cost）</strong></p>
<p>当使用 <code>sigmoid</code> 函数作为激活函数：<span class="math inline">\(\sigma(z)=\frac{1}{1+e^{-z}}\)</span> ，此时的二次代价函数为<br>
<span class="math display">\[
J(\theta) = \frac{1}{2m}\sum_{i = 1}^{m}(h(x^{(i)} - y^{(i)})^2)
\]</span><br>
假如使用梯度下降法（Gradient descent）来调整权值的参数大小，为了说明方便，我们用单个样本为例，此时二次代价函数为：<br>
<span class="math display">\[
J = \frac{(a - y)^2}{2}
\]</span><br>
<code>a = σ(z)</code> 表示该神经元的输入，<span class="math inline">\(y\)</span> 表示真实值，参数调整需要求损失函数的偏导：<br>
<span class="math display">\[
\begin{aligned} \frac{\partial J}{\partial w} &amp;=(a-y) \sigma^{\prime}(z) x \\ \frac{\partial J}{\partial b} &amp;=(a-y) \sigma^{\prime}(z) \end{aligned}
\]</span><br>
参数沿着梯度方向调整参数大小，不足的地方在于，当初始的误差越大，收敛得越缓慢 (sigmoid输入绝对值非常大的值，是饱和区域，梯度很小 <a href="#常见的激活函数">Reference</a>)</p>
<p><strong>交叉熵代价函数（cross-entropy）</strong><br>
<span class="math display">\[
J = -\frac{1}{n}\sum_x[y\ln a + (1-y)\ln{(1-a)}]
\]</span></p>
<p>其中, <code>a = σ(z)</code>最终求导得到更新权重时的偏导:<br>
<span class="math display">\[
\begin{aligned} \frac{\partial J}{\partial w} &amp;=\frac{1}{n} \sum_{x}(\sigma(z)-y)  x_{j} \\ \frac{\partial J}{\partial b} &amp;=\frac{1}{n} \sum_{x}(\sigma(z)-y) \end{aligned}
\]</span><br>
当误差越大时，梯度就越大，权值 <span class="math inline">\(w\)</span> 和偏置 <span class="math inline">\(b\)</span> 调整就越快，训练的速度也就越快，从而达到更快收敛的目。</p>
<hr>
<h3 id="梯度下降">梯度下降</h3>
<h4 id="机器学习中为什么需要梯度下降">机器学习中为什么需要梯度下降</h4>
<ul>
<li>梯度下降是迭代法的一种，可以用于求解最小二乘问题。</li>
<li>在求解机器学习算法的模型参数，即无约束优化问题时，主要有梯度下降法（Gradient Descent）和最小二乘法。</li>
<li>在求解损失函数的最小值时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数和模型参数值。</li>
<li>如果我们需要求解损失函数的最大值，可通过梯度上升法来迭代。梯度下降法和梯度上升法可相互转换。</li>
<li>在机器学习中，梯度下降法主要有随机梯度下降法和批量梯度下降法。</li>
</ul>
<h4 id="梯度下降法缺点">梯度下降法缺点</h4>
<ul>
<li>靠近极小值时收敛速度减慢。</li>
<li>直线搜索时可能会产生一些问题。</li>
<li>可能会“之字形”地下降。</li>
</ul>
<p>梯度概念需注意：</p>
<ul>
<li>梯度是一个向量，即有方向有大小。</li>
<li>梯度的方向是最大方向导数的方向。</li>
<li>梯度的值是最大方向导数的值。</li>
</ul>
<h4 id="梯度下降法直观理解">梯度下降法直观理解</h4>
<div class="figure">
<img src="/2019/05/08/Try-your-best!You-will-find-the-job/梯度下降.png">

</div>
<p>​ 形象化举例，由上图，假如最开始，我们在一座大山上的某处位置，因为到处都是陌生的，不知道下山的路，所以只能摸索着根据直觉，走一步算一步，在此过程中，每走到一个位置的时候，都会求解当前位置的梯度，沿着梯度的负方向，也就是当前最陡峭的位置向下走一步，然后继续求解当前位置梯度，向这一步所在位置沿着最陡峭最易下山的位置走一步。不断循环求梯度，就这样一步步地走下去，一直走到我们觉得已经到了山脚。当然这样走下去，有可能我们不能走到山脚，而是到了某一个局部的山势低处。<br>
​ 由此，从上面的解释可以看出，梯度下降不一定能够找到全局的最优解，有可能是一个局部的最优解。当然，如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解。</p>
<ol style="list-style-type: decimal">
<li>初始化参数，随机选取取值范围内的任意数；</li>
<li>迭代操作：<br>
a）计算当前梯度；<br>
b）修改新的变量；<br>
c）计算朝最陡的下坡方向走一步；<br>
d）判断是否需要终止，如否，返回a）；</li>
<li>得到全局最优解或者接近全局最优解。</li>
</ol>
<h4 id="梯度下降法算法描述">梯度下降法算法描述</h4>
<p><strong>1 . 确定优化模型的假设函数</strong><br>
举例，对于线性回归，假设函数为：<br>
<span class="math display">\[
h_\theta(x_1,x_2,...,x_n)=\theta_0+\theta_1x_1+...+\theta_nx_n
\]</span><br>
其中，<span class="math inline">\(\theta_j,x_j(j=0,1,2,...,n)\)</span>分别为模型参数、每个样本的特征值。<br>
<strong>2 . 损失函数</strong><br>
<span class="math display">\[
J(\theta_0,\theta_1,...,\theta_n)=\frac{1}{2m}\sum^{m}_{i=0}(h_\theta (x^{(i)})-y^{(i)})^2
\]</span><br>
<strong>3 . 相关参数初始化</strong><br>
主要初始化 <span class="math inline">\({\theta}_j\)</span>、算法迭代步长 ${} $、终止距离 ${} $。初始化时可以根据经验初始化，即 <span class="math inline">\({\theta}\)</span> 初始化为0，步长 <span class="math inline">\({\alpha}\)</span> 初始化为1。当前步长记为 <span class="math inline">\({\varphi}_j\)</span>。当然，也可随机初始化。</p>
<p><strong>4 . 迭代计算</strong></p>
<p>1）计算当前位置时损失函数的梯度，对 ${}_j $，其梯度表示为：<br>
<span class="math display">\[
\frac{\partial}{\partial \theta_j}J({\theta}_0,{\theta}_1,...,{\theta}_n)=\frac{1}{2m}\sum^{m}_{i=0}(h_\theta (x^{(i)})-y^{(i)})^2
\]</span><br>
2）计算当前位置下降的距离。<br>
<span class="math display">\[
{\varphi}_j={\alpha} \frac{\partial}{\partial \theta_j}J({\theta}_0,{\theta}_1,...,{\theta}_n)
\]</span><br>
3）判断是否终止</p>
<p>确定是否所有 <span class="math inline">\({\theta}_j\)</span> 梯度下降的距离 <span class="math inline">\({\varphi}_j\)</span> 都小于终止距离 <span class="math inline">\({\zeta}\)</span>，如果都小于 <span class="math inline">\({\zeta}\)</span>，则算法终止，当然的值即为最终结果，否则进入下一步。</p>
<p>4）更新所有的 <span class="math inline">\({\theta}_j\)</span>，更新后的表达式为<br>
<span class="math display">\[
\begin{aligned}
{\theta}_j
&amp; ={\theta}_j-\alpha \frac{\partial}{\partial \theta_j}J({\theta}_0,{\theta}_1,...,{\theta}_n) \\
&amp; =\theta_j - \alpha \frac{1}{m} \sum^{m}_{i=0}(h_\theta (x^{(i)}-y^{(i)})x^{(i)}_j)
\end{aligned}
\]</span><br>
5）令上式 <span class="math inline">\(x^{(i)}_0=1\)</span>，更新完毕后转入 1)<br>
​ 由此，可看出，当前位置的梯度方向由所有样本决定，上式中 <span class="math inline">\(\frac{1}{m}\)</span>、<span class="math inline">\(\alpha \frac{1}{m}\)</span> 的目的是为了便于理解。</p>
<h4 id="如何对梯度下降法进行调优">如何对梯度下降法进行调优</h4>
<p>实际使用梯度下降法时，各项参数指标不能一步就达到理想状态，对梯度下降法调优主要体现在以下几个方面：</p>
<ol style="list-style-type: decimal">
<li><strong>算法迭代步长 <span class="math inline">\(\alpha\)</span> 选择。</strong><br>
在算法参数初始化时，有时根据经验将步长初始化为1。实际取值取决于数据样本。可以从大到小，多取一些值，分别运行算法看迭代效果，如果损失函数在变小，则取值有效。如果取值无效，说明要增大步长。但步长太大，有时会导致迭代速度过快，错过最优解。步长太小，迭代速度慢，算法运行时间长。</li>
<li><strong>参数的初始值选择。</strong><br>
初始值不同，获得的最小值也有可能不同，梯度下降有可能得到的是局部最小值。如果损失函数是凸函数，则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。</li>
<li><strong>标准化处理。</strong><br>
由于样本不同，特征取值范围也不同，导致迭代速度慢。为了减少特征取值的影响，可对特征数据标准化，使新期望为0，新方差为1，可节省算法运行时间。</li>
</ol>
<h4 id="随机梯度和批量梯度区别">随机梯度和批量梯度区别</h4>
<ol style="list-style-type: decimal">
<li><strong>随机梯度下降（SGD）</strong></li>
</ol>
<p>其每次迭代，只用一个训练数据来更新 <span class="math inline">\(\theta\)</span> ，即代价函数对参数的偏导数为：<br>
<span class="math display">\[
\begin{aligned}
{\alpha} \frac{\partial}{\partial \theta_j}J({\theta}_0,{\theta}_1,...,{\theta}_n) &amp; = \frac{\partial}{\partial \theta_j} [\frac{1}{2}(h_{\theta}(x) - y)^2] \\
&amp; = 2 \cdot \frac{1}{2}(h_{\theta}(x) - y) \cdot \frac{\partial}{\partial \theta_j} (h_{\theta}(x) - y) \\
&amp; = (h_{\theta}(x) - y)\cdot x_j
\end{aligned}
\]</span><br>
即此时的参数更新为：d<br>
<span class="math display">\[
{\theta}_j =\theta_j - \alpha \frac{1}{m} (h_\theta (x^{(i)}-y^{(i)})x^{(i)}_j)
\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li><strong>批量梯度下降（BGD）</strong></li>
</ol>
<p>每次迭代，使用所有的数据来更新 <span class="math inline">\(\theta\)</span> ，此时代价函数对参数的偏导数为：<br>
<span class="math display">\[
\begin{aligned}
{\alpha} \frac{\partial}{\partial \theta_j}J({\theta}_0,{\theta}_1,...,{\theta}_n) &amp; = \frac{\partial}{\partial \theta_j} [\frac{1}{2m}\sum_{i = 1}^{m}(h_{\theta}(x^{(i)}) -y^{(i)})^2] \\
&amp; = \frac{1}{m} \sum_{i = 1}^{m}(h_{\theta}(x^{(i)}) -y^{(i)})\cdot x_j^{(i)}
\end{aligned} \\
\]</span><br>
即此时的参数更新为：<br>
<span class="math display">\[
{\theta}_j =\theta_j - \alpha \frac{1}{m} \sum_{i = 1}^{m} (h_\theta (x^{(i)}-y^{(i)})x^{(i)}_j)
\]</span><br>
<strong>小结：</strong>随机梯度下降法、批量梯度下降法相对来说都比较极端，简单对比如下：</p>
<table>
<colgroup>
<col width="17%">
<col width="82%">
</colgroup>
<thead>
<tr class="header">
<th>方法</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>批量梯度下降</td>
<td>a）采用所有数据来梯度下降。<br>b）批量梯度下降法在样本量很大的时候，训练速度慢。</td>
</tr>
<tr class="even">
<td>随机梯度下降</td>
<td>a）随机梯度下降用一个样本来梯度下降。<br>b）训练速度很快。<br>c）随机梯度下降法仅仅用一个样本决定梯度方向，导致解有可能不是全局最优。<br>d）收敛速度来说，随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解。</td>
</tr>
</tbody>
</table>
<p>下面介绍能结合两种方法优点的小批量梯度下降法。</p>
<ol start="3" style="list-style-type: decimal">
<li><strong>小批量梯度下降（Mini-batch GD）</strong></li>
</ol>
<p>对于总数为 <span class="math inline">\(m\)</span> 个样本的数据，根据样本的数据，选取其中的<span class="math inline">\(n(1&lt; n&lt; m)\)</span>个子样本来迭代。其参数 <span class="math inline">\(\theta\)</span> 按梯度方向更新 <span class="math inline">\(\theta_j\)</span> 公式如下：<br>
<span class="math display">\[
{\theta}_j =\theta_j - \alpha \frac{1}{m} \sum_{i = t}^{t+n-1} (h_\theta (x^{(i)}-y^{(i)})x^{(i)}_j)
\]</span></p>
<h4 id="各种梯度下降法性能比较">各种梯度下降法性能比较</h4>
<p>下表简单对比 <code>随机梯度下降（SGD）</code>、<code>批量梯度下降（BGD）</code>、<code>小批量梯度下降（Mini-batch GD）</code>、和 <code>Online GD</code> 的区别：</p>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="center">BGD</th>
<th align="center">SGD</th>
<th align="center">Mini-batch GD</th>
<th align="center">Online GD</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">训练集</td>
<td align="center">固定</td>
<td align="center">固定</td>
<td align="center">固定</td>
<td align="center">实时更新</td>
</tr>
<tr class="even">
<td align="left">单次迭代样本数</td>
<td align="center">整个训练集</td>
<td align="center">单个样本</td>
<td align="center">训练集的子集</td>
<td align="center">根据具体算法定</td>
</tr>
<tr class="odd">
<td align="left">算法复杂度</td>
<td align="center">高</td>
<td align="center">低</td>
<td align="center">一般</td>
<td align="center">低</td>
</tr>
<tr class="even">
<td align="left">时效性</td>
<td align="center">低</td>
<td align="center">一般</td>
<td align="center">一般</td>
<td align="center">高</td>
</tr>
<tr class="odd">
<td align="left">收敛性</td>
<td align="center">稳定</td>
<td align="center">不稳定</td>
<td align="center">较稳定</td>
<td align="center">不稳定</td>
</tr>
</tbody>
</table>
<p>BGD、SGD、Mini-batch GD，前面均已讨论过，这里介绍一下Online GD</p>
<p><strong>Online GD</strong> 与Mini-batch GD/SGD的区别在于，所有训练数据只用一次，然后丢弃。这样做的优点在于可预测最终模型的变化趋势。</p>
<p>Online GD在互联网领域用的较多，比如搜索广告的点击率（CTR）预估模型，网民的点击行为会随着时间改变。用普通的BGD算法（每天更新一次）一方面耗时较长（需要对所有历史数据重新训练）；另一方面，无法及时反馈用户的点击行为迁移。而Online GD算法可以实时的依据网民的点击行为进行迁移。</p>
<hr>
<h3 id="范数">范数</h3>
<blockquote>
<p><strong>什么是范数？</strong></p>
</blockquote>
<p>我们知道距离的定义是一个宽泛的概念，只要满足非负、自反、三角不等式就可以称之为距离。范数是一种强化了的距离概念，它在定义上比距离多了一条数乘的运算法则。有时候为了便于理解，我们可以把范数当作距离来理解。</p>
<p>在数学上，范数包括 <strong>向量范数</strong> 和 <strong>矩阵范数</strong>，向量范数表征向量空间中向量的大小，矩阵范数表征矩阵引起变化的大小。一种非严密的解释就是，对应向量范数，向量空间中的向量都是有大小的，这个大小如何度量，就是用范数来度量的，不同的范数都可以来度量这个大小，就好比米和尺都可以来度量远近一样；对于矩阵范数，学过线性代数，我们知道，通过运算 <span class="math inline">\(AX=B\)</span>，可以将向量 <span class="math inline">\(X\)</span> 变化为 <span class="math inline">\(B\)</span>，矩阵范数就是来度量这个变化大小的。这里简单地介绍以下几种向量范数的定义和含义</p>
<h4 id="l-p范数">L-P范数</h4>
<p>与闵可夫斯基距离的定义一样，L-P范数不是一个范数，而是一组范数，其定义如下：<br>
<span class="math display">\[
Lp=\sqrt[p]{\sum\limits_{i = 1}^n  x_i^p}，x=(x_1,x_2,\cdots,x_n)
\]</span><br>
根据 <span class="math inline">\(p\)</span> 的变化，范数也有着不同的变化，一个经典的有关 <span class="math inline">\(p\)</span> 范数的变化图如下：</p>
<div class="figure">
<img src="/2019/05/08/Try-your-best!You-will-find-the-job/范数.png">

</div>
<p>上图表示了 <span class="math inline">\(p\)</span> 从无穷到0变化时，三维空间中到原点的距离（范数）为1的点构成的图形的变化情况。以常见的 L-2范数(p=2)为例，此时的范数也即欧氏距离，空间中到原点的欧氏距离为1的点构成了一个球面。<br>
实际上，在 <span class="math inline">\(0≤p&lt;1\)</span> 时，<span class="math inline">\(L_p\)</span> 并不满足三角不等式的性质，也就不是严格意义下的范数。以<span class="math inline">\(p=0.5\)</span> ，二维坐标 <span class="math inline">\((1,4)、(4,1)、(1,9)\)</span> 为例，<span class="math inline">\(\sqrt[0.5]{(1+\sqrt{4})}+\sqrt[0.5]{(\sqrt{4}+1)}&lt;\sqrt[0.5]{(1+\sqrt{9})}\)</span> ，因此这里的 L-P 范数只是一个概念上的宽泛说法。</p>
<h4 id="l0范数">L0范数</h4>
<p>当 <span class="math inline">\(p=0\)</span> 时，也就是 L0 范数，由上面可知，L0范数并不是一个真正的范数，它<u>主要被用来度量向量中非零元素的个数</u>。用上面的L-P定义可以得到的L-0的定义为：<br>
<span class="math display">\[
||x||_{0}=\sqrt[0]{\sum\limits_1^nx_i^0}，x=(x_1,x_2,\cdots,x_n)
\]</span><br>
这里就有点问题了，我们知道非零元素的零次方为1，但零的零次方，非零数开零次方都是什么鬼，很不好说明 L0 的意义，所以在通常情况下，大家都用的是：</p>
<p>​ <span class="math inline">\(||x_0||\)</span> = # <span class="math inline">\(( i|x_i\neq 0)\)</span></p>
<blockquote>
<p>表示向量 <span class="math inline">\(x\)</span> 中非零元素的个数。</p>
</blockquote>
<p>对于L0范数，其优化问题为：<br>
<span class="math display">\[
min||x||_0 \\
s.t. Ax=b
\]</span><br>
在实际应用中，由于 L0 范数本身不容易有一个好的数学表示形式，给出上面问题的形式化表示是一个很难的问题，故被人认为是一个NP难问题。所以在实际情况中，L0的最优问题会被放宽到 L1 或 L2 下的最优化。</p>
<h4 id="l1范数">L1范数</h4>
<p>L1范数是我们经常见到的一种范数，它的定义如下：<br>
<span class="math display">\[
||x||_1=\sum_i|x_i|
\]</span></p>
<blockquote>
<p>表示向量 <span class="math inline">\(x\)</span> 中非零元素的绝对值之和</p>
</blockquote>
<p>L1范数有很多的名字，例如我们熟悉的<strong>曼哈顿距离</strong>、<strong>最小绝对误差</strong>等。使用L1范数可以度量两个向量间的差异，如绝对误差和（Sum of Absolute Difference）：<br>
<span class="math display">\[
SAD(x_1,x_2)=\sum_i|x_{1i}-x_{2i}|
\]</span><br>
对于 L1 范数，它的优化问题如下：<br>
<span class="math display">\[
min||x||_1 \\
s.t.Ax=b
\]</span><br>
由于 L1 范数的天然性质，对 L1 优化的解是一个稀疏解，因此 L1范数 也被叫做<strong>稀疏规则算子</strong>。通过L1 可以实现特征的稀疏，去掉一些没有信息的特征，例如在对用户的电影爱好做分类的时候，用户有100个特征，可能只有十几个特征是对分类有用的，大部分特征如身高体重等可能都是无用的，利用 L1 范数就可以过滤掉。</p>
<h4 id="l2范数">L2范数</h4>
<p>L2范数是我们最常见最常用的范数了，我们用的最多的度量距离<strong>欧氏距离</strong>就是一种 L2 范数，它的定义如下：<br>
<span class="math display">\[
||x||_2=\sqrt{\sum_ix_i^2}
\]</span></p>
<blockquote>
<p>表示向量元素的平方和 再开平方</p>
</blockquote>
<p>像 L1 范数一样，L2 也可以度量两个向量间的差异，如平方差和（Sum of Squared Difference）:<br>
<span class="math display">\[
SSD(x_1,x_2)=\sum_i(x_{1i}-x_{2i})^2
\]</span><br>
对于L2范数，它的优化问题如下：<br>
<span class="math display">\[
min||x||_2 \\
s.t.Ax=b
\]</span><br>
L2 范数通常会被用来做优化目标函数的正则化项，防止模型为了迎合训练集而过于复杂造成过拟合的情况，从而提高模型的泛化能力。</p>
<h4 id="l-范数">L-∞范数</h4>
<p>当 <span class="math inline">\(P=∞\)</span> 时，也就是 <span class="math inline">\(L∞\)</span> 范数，它主要被用来度量向量元素的最大值。用上面的 L-P 定义可以得到的 <span class="math inline">\(L∞\)</span> 的定义为：<br>
<span class="math display">\[
||x||_\infty=\sqrt[\infty]{\sum\limits_1^nx_i^\infty}，x=(x_1,x_2,\cdots,x_n)
\]</span><br>
与 L0 一样，在通常情况下，大家都用的是：<br>
<span class="math display">\[
||x||_\infty=max(|x_i|)
\]</span><br>
来表示 <span class="math inline">\(L∞\)</span></p>
<h4 id="欧式距离-与-余弦相似度">欧式距离 与 余弦相似度</h4>
<ol style="list-style-type: decimal">
<li><strong>区别</strong></li>
</ol>
<p>假设 2人对三部电影的评分分别是 <code>A = [3, 3, 3]</code> 和 <code>B = [5, 5, 5]</code><br>
那么2人的欧式距离是 <span class="math inline">\(\sqrt{12} = 3.46\)</span>， A、B的余弦相似度是1（方向完全一致）</p>
<blockquote>
<p>余弦值的范围是[-1, 1], 越接近于1，说明2个向量的方向越相近</p>
</blockquote>
<p>欧式距离和余弦相似度都能度量2个向量之间的相似度，但是欧式距离从2点之间的距离去考量，余弦相似从2个向量之间的夹角去考量。 从上例可以发出，2人对三部电影的评价趋势是一致的，但是欧式距离并不能反映出这一点，余弦相似则能够很好地反应。余弦相似可以很好地规避指标刻度的差异，最常见的应用是计算 <strong>文本的相似度</strong> 。</p>
<ol start="2" style="list-style-type: decimal">
<li><strong>联系</strong></li>
</ol>
<p>假设二维空间两个点： <span class="math inline">\(A(x_1, y_1),\ B(x_2,y_2)\)</span></p>
<ul>
<li><strong>欧氏距离（Euclidean Distance）</strong></li>
</ul>
<p><span class="math display">\[
euc = \sqrt{(x_1 - y_1)^2 + (x_2-y_2)^2}
\]</span></p>
<ul>
<li><strong>余弦定义</strong></li>
</ul>
<p><span class="math display">\[
\cos (\theta)=\frac{&lt;A, B&gt;}{|A||B|}
\]</span></p>
<p>然后归一化为<strong>单位向量，</strong> <span class="math inline">\(A\left(\frac{x_{1}}{\sqrt{x_{1}^{2}+y_{1}^{2}}}, \frac{y_{1}}{\sqrt{x_{1}^{2}+y_{1}^{2}}}\right), B\left(\frac{x_{2}}{\sqrt{x_{2}^{2}+y_{2}^{2}}}, \frac{y_{2}}{\sqrt{x_{2}^{2}+y_{2}^{2}}}\right)\)</span></p>
<p><strong>余弦相似度</strong>为：<span class="math inline">\(\cos =\frac{x_{1}}{\sqrt{x_{1}^{2}+y_{1}^{2}}} \times \frac{x_{2}}{\sqrt{x_{2}^{2}+y_{2}^{2}}}+\frac{y_{1}}{\sqrt{x_{1}^{2}+y_{1}^{2}}} \times \frac{y_{2}}{\sqrt{x_{2}^{2}+y_{2}^{2}}}，(分母是1省略了)\)</span></p>
<p><strong>欧式距离</strong>为：<span class="math inline">\(e u c=\sqrt{\left(\frac{x_{1}}{\sqrt{x_{1}^{2}+y_{1}^{2}}}-\frac{x_{2}}{\sqrt{x_{2}^{2}+y_{2}^{2}}}\right)^{2}+\left(\frac{y_{1}}{\sqrt{x_{1}^{2}+y_{1}^{2}}}-\frac{y_{2}}{\sqrt{x_{2}^{2}+y_{2}^{2}}}\right)^{2}}\)</span></p>
<p>简化后就是：<span class="math inline">\(e u c=\sqrt{2-2 \times \cos }\)</span> ，即化简后计算的欧式距离是关于余弦相似度的单调函数，可以认为归一化后，余弦相似与欧式距离效果是一致的（欧式距离越小等价于余弦相似度越大）</p>
<p>因此可以将 <strong>求余弦相似转为求欧式距离</strong> ，余弦相似的计算复杂度过高（需要两两比较 <span class="math inline">\(O(n^2)\)</span>），转为求欧式距离后，可以借助 <code>KDTree</code>（KNN算法用到）或者 <code>BallTree</code>（对高维向量友好）来降低复杂度。</p>
<blockquote>
<p><a href="https://www.zhihu.com/question/19640394" target="_blank" rel="noopener">欧氏距离和余弦相似度的区别是什么？</a> <a href="https://juejin.im/entry/5b14121fe51d4506d33ce9f4" target="_blank" rel="noopener">另一个总结</a> <a href="https://my.oschina.net/hunglish/blog/787596" target="_blank" rel="noopener">几种距离度量方法比较</a></p>
</blockquote>
<hr>
<h3 id="逻辑回归">逻辑回归</h3>
<h4 id="回归划分">回归划分</h4>
<p>广义线性模型家族里，依据因变量不同，可以有如下划分：</p>
<ul>
<li>如果是连续的，就是<strong>多重线性回归</strong></li>
<li>如果是二项分布(0-1分布)，就是<strong>逻辑回归</strong></li>
<li>如果是泊松(Poisson)分布，就是<strong>泊松回归</strong></li>
<li>如果是负二项分布，就是<strong>负二项回归</strong></li>
<li>逻辑回归的因变量可以是二分类的，也可以是多分类的，但是二分类的更为常用，也更加容易解释。所以实际中最常用的就是<strong>二分类的逻辑回归</strong>。</li>
</ul>
<h4 id="逻辑回归适用性">逻辑回归适用性</h4>
<ul>
<li><strong>用于概率预测</strong>。用于可能性预测时，得到的结果有可比性。比如根据模型进而预测在不同的自变量情况下，发生某病或某种情况的概率有多大。</li>
<li><strong>用于分类</strong>。实际上跟预测有些类似，也是根据模型，判断某人属于某病或属于某种情况的概率有多大，也就是看一下这个人有多大的可能性是属于某病。进行分类时，仅需要设定一个阈值即可，可能性高于阈值是一类，低于阈值是另一类。</li>
<li>寻找危险因素。寻找某一疾病的危险因素等。</li>
<li><strong>仅能用于线性问题</strong>。<strong>只有当目标和特征是线性关系时，才能用逻辑回归</strong>。在应用逻辑回归时注意两点：一是当知道模型是非线性时，不适用逻辑回归；二是当使用逻辑回归时，应注意选择和目标为线性关系的特征。</li>
<li>各特征之间不需要满足条件独立假设，但各个特征的贡献独立计算。</li>
</ul>
<h4 id="逻辑回归与朴素贝叶斯的区别">逻辑回归与朴素贝叶斯的区别</h4>
<ol style="list-style-type: decimal">
<li>逻辑回归是判别模型， 朴素贝叶斯是生成模型，所以生成和判别的所有区别它们都有。</li>
<li>朴素贝叶斯属于贝叶斯，逻辑回归是最大似然，两种概率哲学间的区别。</li>
<li>朴素贝叶斯需要条件独立假设。</li>
<li>逻辑回归需要求特征参数间是线性的。</li>
</ol>
<h4 id="逻辑回归-与-线性回归-的联系与区别">逻辑回归 与 线性回归 的联系与区别</h4>
<p><strong>联系：</strong> 线性回归与逻辑回归都是广义的线性回归</p>
<table>
<colgroup>
<col width="6%">
<col width="46%">
<col width="46%">
</colgroup>
<thead>
<tr class="header">
<th align="center">区别</th>
<th align="center">线性回归（liner regression）</th>
<th align="center">LR（logistics regression）</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">构建方法</td>
<td align="center">最小二乘法</td>
<td align="center">极大似然函数</td>
</tr>
<tr class="even">
<td align="center">解决问题</td>
<td align="center">主要用于预测，解决<strong>回归问题</strong><br>也可以用来分类，但是鲁棒性差</td>
<td align="center">解决<strong>分类问题</strong></td>
</tr>
<tr class="odd">
<td align="center">输出</td>
<td align="center">输出实数域上连续值</td>
<td align="center">输出值被S型函数映射到[0,1]<br>通过设置阈值转换成分类类别</td>
</tr>
<tr class="even">
<td align="center">拟合函数</td>
<td align="center"><span class="math inline">\(h_{\theta}(x)=\theta ^{T}x=\theta _{1}x _{1}+\theta _{2}x _{2}+...+\theta _{n}x _{n}\)</span></td>
<td align="center">$h_{}(x)=P(y=1</td>
</tr>
</tbody>
</table>
<ul>
<li>线性回归的拟合函数，是对 <span class="math inline">\(h_{\theta}(x)\)</span> 的输出变量 <span class="math inline">\(y\)</span> 的拟合（通过房屋大小，预测房价）</li>
<li>逻辑回归的拟合函数，通过已确定的参数，计算出输入变量 <span class="math inline">\(y = 1\)</span> 的可能性，即是对为1类样本的概率的拟合（疾病预测）</li>
</ul>
<h4 id="逻辑回归-与-svm-的联系与区别">逻辑回归 与 SVM 的联系与区别 <a href="https://www.cnblogs.com/zhizhan/p/5038747.html" target="_blank" rel="noopener">*</a></h4>
<p><strong>相同点：</strong></p>
<ul>
<li>都是<strong>分类算法</strong></li>
<li>都是<strong>监督学习算法</strong></li>
<li>都是<strong>判别模型</strong></li>
<li>如果不考虑核函数，LR 与 SVM 都是线性分类算法，也就是说他们的分类决策面都是线性的</li>
</ul>
<p><strong>不同点：</strong></p>
<ul>
<li><p>LR 和 SVM 的<strong>本质区别</strong>来自于 loss function的不同</p></li>
<li><p>LR 使用的是<strong>交叉熵损失函数/对数损失函数</strong> (cross entropy / log loss)</p></li>
<li><p><span class="math display">\[
J(\theta)=-\frac{1}{m} \sum_{i=1}^{m}\left[y^{(i)} \log h_{\theta}\left(x^{(i)}\right)+\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right]
\]</span></p></li>
<li><p>SVM 是 <strong>合页损失函数</strong> (hinge loss)<br>
<span class="math display">\[
\mathcal{L}(w, b, \alpha)=\frac{1}{2}\|w\|^{2}-\sum_{i=1}^{n} \alpha_{i}\left(y_{i}\left(w^{T} x_{i}+b\right)-1\right)
\]</span></p></li>
<li>LR 基于<u>概率理论</u>，假设样本为1的概率可以用sigmoid函数来表示，然后通过极大似然估计的方法估计出参数的值。SVM 基于<u>几何间隔最大化原理</u>，认为存在最大几何间隔的分类面为最优分类面。线性SVM依赖数据表达的距离测度，所以需要对数据先做normalization，LR不受其影响</li>
<li>LR 可以产生概率，SVM 不能产生概率</li>
<li>SVM 只考虑局部的边界线附近的点，而逻辑回归考虑全局（远离的点对边界线的确定也起作用）</li>
<li><p>SVM的损失函数自带正则（结构风险最小化），而LR需要额外在损失函数上添加正则项</p></li>
</ul>
<hr>
<h3 id="集成学习">集成学习</h3>
<h4 id="集成学习的基本思想">集成学习的基本思想</h4>
<ul>
<li>结合多个学习器组合成一个性能更好的学习器</li>
</ul>
<h4 id="集成学习为什么有效">集成学习为什么有效</h4>
<ul>
<li>不同的模型通常会在<strong>测试集</strong>上产生不同的误差；如果成员的误差是独立的，集成模型将显著地比其成员表现更好</li>
</ul>
<h4 id="boosting-方法">Boosting 方法</h4>
<ul>
<li>基于<strong>串行策略</strong>：基学习器之间存在依赖关系，新的学习器需要根据上一个学习器生成。</li>
<li><strong>基本思路</strong>：</li>
<li>先从<strong>初始训练集</strong>训练一个基学习器；初始训练集中各样本的权重是相同的；</li>
<li>根据上一个基学习器的表现，<strong>调整样本权重</strong>，使分类错误的样本得到更多的关注；</li>
<li>基于调整后的样本分布，训练下一个基学习器；</li>
<li>测试时，对各基学习器<strong>加权</strong>得到最终结果</li>
<li><strong>特点</strong>：</li>
<li>每次学习都会使用全部训练样本</li>
<li><strong>代表算法</strong>：</li>
<li><a href="#adaboost-算法">AdaBoost 算法</a></li>
<li><a href="#gbdt-算法">GBDT 算法</a></li>
</ul>
<h4 id="bagging-方法booststrap-aggregating">Bagging 方法（Booststrap AGGregatING）</h4>
<ul>
<li><p>基于<strong>并行策略</strong>：基学习器之间不存在依赖关系，可同时生成。</p></li>
<li><p><strong>基本思路</strong>：</p></li>
<li><p>利用<strong>自助采样法</strong>对训练集随机采样，重复进行 <code>T</code> 次;</p></li>
<li><p>基于每个采样集训练一个基学习器，并得到 <code>T</code> 个基学习器；</p></li>
<li><p>预测时，集体<strong>投票决策</strong>。</p>
<blockquote>
<p><strong>自助采样法</strong>：对 m 个样本的训练集，有放回的采样 m 次；此时，样本在 m 次采样中始终没被采样的概率约为 <code>0.368</code>，即每次自助采样只能采样到全部样本的 <code>63%</code> 左右。<br>
<span class="math display">\[
\lim _{m \rightarrow \infty}\left(1-\frac{1}{m}\right)^{m} \rightarrow \frac{1}{e} \approx 0.368
\]</span></p>
</blockquote></li>
<li><p><strong>特点</strong>：</p></li>
<li><p>训练每个基学习器时只使用一部分样本；</p></li>
<li><p>偏好<strong>不稳定</strong>的学习器作为基学习器；</p>
<blockquote>
<p>所谓不稳定的学习器，指的是对<strong>样本分布</strong>较为敏感的学习器。</p>
</blockquote></li>
</ul>
<h4 id="为什么使用决策树作为基学习器">为什么使用决策树作为基学习器</h4>
<ul>
<li><strong>类似问题</strong></li>
<li>基学习器有什么特点？</li>
<li>基学习器有什么要求？</li>
<li>使用决策树作为基学习器的原因：</li>
<li>决策树的表达能力和泛化能力，可以通过剪枝快速调整</li>
<li>决策树可以方便地将<strong>样本的权重</strong>整合到训练过程中 （适合 Boosting 策略）</li>
<li>决策树是一种<strong>不稳定</strong>的学习器。所谓不稳定，指的是数据样本的扰动会对决策树的结果产生较大的影响。 （适合 Bagging 策略）</li>
</ul>
<h4 id="为什么不稳定的学习器更适合作为基学习器">为什么不稳定的学习器更适合作为基学习器</h4>
<ul>
<li>不稳定的学习器容易受到<strong>样本分布</strong>的影响（方差大），很好的引入了<strong>随机性</strong>；这有助于在集成学习（特别是采用 <strong>Bagging</strong> 策略）中提升模型的<strong>泛化能力</strong></li>
<li>为了更好的引入随机性，有时会随机选择一个<strong>属性子集</strong>中的最优分裂属性，而不是全局最优（<strong>随机森林</strong>）</li>
</ul>
<h4 id="还有哪些模型也适合作为基学习器">还有哪些模型也适合作为基学习器</h4>
<ul>
<li><strong>神经网络</strong></li>
<li>神经网络也属于<strong>不稳定</strong>的学习器；</li>
<li>此外，通过调整神经元的数量、网络层数，连接方式初始权重也能很好的引入随机性和改变模型的表达能力和泛化能力。</li>
</ul>
<h4 id="boosting-方法中能使用线性分类器作为基学习器吗-bagging-呢">Boosting 方法中能使用线性分类器作为基学习器吗？ Bagging 呢？</h4>
<ul>
<li>Boosting 方法中<strong>可以使用</strong></li>
<li>Boosting 方法主要通过降低<strong>偏差</strong>的方式来提升模型的性能，而线性分类器本身具有方差小的特点，所以两者有一定相性</li>
<li>XGBoost 中就支持以线性分类器作为基学习器。</li>
<li>Bagging 方法中<strong>不推荐</strong></li>
<li>线性分类器都属于稳定的学习器（方差小），对数据不敏感；</li>
<li>甚至可能因为 Bagging 的采样，导致在训练中难以收敛，增大集成分类器的<strong>偏差</strong></li>
</ul>
<h4 id="boostingbagging-与-偏差方差-的关系"><a href="#%5B偏差方差与Boosting和Bagging联系%5D(#集成学习)">Boosting/Bagging 与 偏差/方差 的关系</a></h4>
<h4 id="bagging与dropout区别与联系"><a href>Bagging与Dropout区别与联系</a></h4>
<ul>
<li>在 Bagging 的情况下，所有模型都是独立的。而在 Dropout 的情况下，所有模型<strong>共享参数</strong>，其中每个模型继承父神经网络参数的不同子集。</li>
<li>在 Bagging 的情况下，每一个模型都会在其相应训练集上训练到收敛。而在 Dropout 的情况下，通常大部分模型都没有显式地被训练；取而代之的是，在单个步骤中我们训练一小部分的子网络，参数共享会使得剩余的子网络也能有好的参数设定。</li>
</ul>
<h4 id="adaboost-算法">AdaBoost 算法</h4>
<h4 id="gbdt-算法">GBDT 算法</h4>
<h3 id="归一化">归一化</h3>
<h3 id="batch-normalization批标准化">Batch Normalization(批标准化)</h3>
<h4 id="bn层作用以及如何使用bn层">BN层作用，以及如何使用BN层</h4>
<p><strong><a href="https://www.cnblogs.com/guoyaohua/p/8724433.html" target="_blank" rel="noopener">BN</a></strong> (Batch Normalization批标准化) 是一种<strong>正则化</strong>方法（减少泛化误差），主要作用有：</p>
<ul>
<li>加速网络的训练</li>
<li>缓解梯度消失</li>
<li>防止过拟合</li>
<li>增强模型的泛化能力</li>
<li>支持更大的学习率</li>
<li>降低了参数初始化的要求</li>
</ul>
<h4 id="动机">动机</h4>
<ul>
<li><strong>训练的本质是学习数据分布</strong>。如果训练数据与测试数据的分布不同会<strong>降低</strong>模型的<strong>泛化能力</strong>。因此，应该在开始训练前对所有输入数据做归一化处理。</li>
<li>而在神经网络中，因为<strong>每个隐层</strong>的参数不同，会使下一层的输入发生变化，从而导致每一批数据的分布也发生改变；<strong>致使</strong>网络在每次迭代中都需要拟合不同的数据分布，增大了网络的训练难度与<strong>过拟合</strong>的风险。</li>
</ul>
<h4 id="基本原理">基本原理</h4>
<p><strong>（1）训练阶段</strong></p>
<ul>
<li>BN 方法会针对<strong>每一批数据</strong>，在<strong>网络的每一层输入</strong>之前增加<strong>归一化</strong>处理，使输入的均值为 <code>0</code>，标准差为 <code>1</code>。<strong>目的</strong>是将数据限制在统一的分布下。</li>
<li>具体来说，针对每层的第 <code>k</code> 个神经元，计算<strong>这一批数据</strong>在第 <code>k</code> 个神经元的均值与标准差，然后将归一化后的值作为该神经元的激活值。</li>
</ul>
<p><span class="math display">\[
\boldsymbol{\hat{x}_{k}} = \frac{\boldsymbol{x_{k}}-\mathrm{E}\left[\boldsymbol{x_{k}}\right]}{\sqrt{\operatorname{Var}\left[\boldsymbol{x_{k}}\right]}}
\]</span></p>
<ul>
<li>BN 可以看作在各层之间加入了一个新的计算层，<strong>对数据分布进行额外的约束</strong>，从而增强模型的泛化能力；</li>
<li>但同时 BN 也降低了模型的拟合能力，破坏了之前学到的<strong>特征分布</strong>；</li>
<li>为了<strong>恢复数据的原始分布</strong>，BN 引入了一个<strong>重构变换</strong>来还原最优的输入数据分布，其中 <code>γ</code> 和 <code>β</code> 是我们要训练学习的参数。</li>
</ul>
<p><span class="math display">\[
\boldsymbol{y_{k}} \leftarrow \gamma \boldsymbol{\hat{x}_{k}}+\beta
\]</span></p>
<p><strong>小结：</strong></p>
<ul>
<li>以上过程可归纳为一个 <strong><code>BN(x)</code> 函数</strong>：</li>
</ul>
<p><span class="math display">\[
\large\begin{aligned}
\large\boldsymbol{y_i}=
\mathrm{BN}(\boldsymbol{x_i})
&amp;=\gamma\boldsymbol{\hat{x}_i} + \beta \\
&amp;=\gamma\frac{\boldsymbol{x_i}-\boldsymbol{\mathrm{E}[x_i]}}{\sqrt{\boldsymbol{\mathrm{Var}[x_i]}+\epsilon}}+\beta\end{aligned}
\]</span></p>
<ul>
<li>完整算法：</li>
</ul>
<div class="figure">
<img src="/2019/05/08/Try-your-best!You-will-find-the-job/BN.png">

</div>
<blockquote>
<p><strong>输入</strong>：上一层输出结果 $  = x_1, x_2, … , x_m$ ，学习参数 <span class="math inline">\(\gamma, \beta\)</span><br>
<strong>算法流程</strong>：</p>
<ol style="list-style-type: decimal">
<li>计算上一层输出数据的均值</li>
</ol>
<p><span class="math display">\[
\mu_{\mathcal{B}} = \frac{1}{m} \sum_{i=1}^m(x_i)
\]</span><br>
​ （其中，m是此次训练样本 batch 的大小）</p>
<ol start="2" style="list-style-type: decimal">
<li>计算上一层输出数据的标准差</li>
</ol>
<p><span class="math display">\[
\sigma_{\mathcal{B}}^2 = \frac{1}{m} \sum_{i=1}^m (x_i - \mu_{\mathcal{B}})^2
\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>归一化处理，目的是为了将数据限制在统一的分布之下</li>
</ol>
<p><span class="math display">\[
\hat x_i = \frac{x_i - \mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{B}}^2 + \epsilon}}
\]</span></p>
<p>（其中 $ $ 是为了避免分母为 0 而加进去的接近于 0 的很小值）</p>
<ol start="4" style="list-style-type: decimal">
<li>重构，对经过上面归一化处理得到的数据进行重构，还原最优的输入数据分布，作为该层神经元的激活值<br>
<span class="math display">\[
   y_i = \gamma \hat x_i + \beta
   \]</span><br>
（其中，$ , $ 为可学习参数）</li>
</ol>
</blockquote>
<p><strong>（2）测试阶段</strong></p>
<p>测试的时候，每次可能只会传入<strong>单个数据</strong>，此时的均值和标准差，模型会使用<strong>全局统计量</strong>代替<strong>批统计量</strong>；即使用训练时所有batch得到的一组组的均值和方差，计算其数学期望做为全局统计量。<br>
<span class="math display">\[
\begin{array}{c}{\mathrm{E}[x] \leftarrow \mathrm{E}\left[\mu_{i}\right]} \\ {\operatorname{Var}[x] \leftarrow \frac{m}{m-1} \mathrm{E}\left[\sigma_{i}^{2}\right]}\end{array}
\]</span></p>
<blockquote>
<p>其中 <span class="math inline">\(μ_i\)</span> 和 <span class="math inline">\(σ_i\)</span> 分别表示第 <span class="math inline">\(i\)</span> 轮 batch 保存的均值和标准差；<span class="math inline">\(m\)</span> 为 batch_size，系数 <span class="math inline">\(\frac{m}{m-1}\)</span> 用于计算<strong>无偏方差估计</strong> （原文称该方法为<strong>移动平均</strong>（moving averages））</p>
</blockquote>
<p>然后再将按照训练的流程，将输入数据，减去全局统计量均值除以标准差，再进行重构变换得到新的数据作为神经元的激活值。</p>
<ul>
<li>此时，<code>BN(x)</code> 调整为：</li>
</ul>
<p><span class="math display">\[
\large\begin{aligned}\mathrm{BN}(\boldsymbol{x_i})&amp;=\gamma\frac{\boldsymbol{x_i}-\boldsymbol{\mathrm{E}[x_i]}}{\sqrt{\boldsymbol{\mathrm{Var}[x_i]} + \epsilon}} + \beta\\&amp;=\frac{\gamma}{\sqrt{\boldsymbol{\mathrm{Var}[x_i]} + \epsilon}}\boldsymbol{x_i} + \left(\beta-\frac{\gamma\boldsymbol{\mathrm{E}[x_i]}}{\sqrt{\boldsymbol{\mathrm{Var}[x_i]} + \epsilon}}\right)\end{aligned}
\]</span></p>
<p>这样写的目的是为了减少计算量，推理阶段公式中的两个分式是固定值，可以预先计算好，这样推理阶段就可以直接使用。</p>
<ul>
<li><strong>完整算法：</strong></li>
</ul>
<div class="figure">
<img src="/2019/05/08/Try-your-best!You-will-find-the-job/BN_02.png">

</div>
<ul>
<li><strong>Reference:</strong> <a href="https://www.cnblogs.com/guoyaohua/p/8724433.html" target="_blank" rel="noopener">理解</a> <a href="https://blog.csdn.net/qq_25737169/article/details/79048516" target="_blank" rel="noopener">源码解读</a> <a href="https://zhuanlan.zhihu.com/p/34879333" target="_blank" rel="noopener">实战</a> <a href="https://www.zhihu.com/question/38102762" target="_blank" rel="noopener">知乎</a></li>
</ul>
<h4 id="为什么训练时不采用移动平均">为什么训练时不采用移动平均？</h4>
<ul>
<li>用 BN 的目的就是为了保证每批数据的分布稳定，使用训练时使用全局统计量反而违背了这个初衷</li>
<li>BN 的作者认为在训练时采用移动平均可能会与梯度优化存在冲突</li>
</ul>
<h4 id="bnlningn的异同">BN、LN、IN、GN的异同</h4>
<div class="figure">
<img src="/2019/05/08/Try-your-best!You-will-find-the-job/GN.png">

</div>
<div class="figure">
<img src="/2019/05/08/Try-your-best!You-will-find-the-job/BN_croped.png">

</div>
<p>深度网络中的数据维度一般为：[N, C, H, W] 或者[N, H, W, C]格式，分别对应上图两种排列方式。</p>
<p>其中：</p>
<ul>
<li>N ：batch_size</li>
<li>W ：feature map 的宽</li>
<li>H ：feature map 的高</li>
<li>C ：feature map 的通道数</li>
</ul>
<p><strong><code>BN</code>：</strong> 在batch的维度上进行norm，归一化维度为 <strong>[N, H, W]</strong>，BN对batch size有依赖，当batch size较大时，有不错的效果。而 LN、IN、GN 能够摆脱这种依赖，其中GN效果最好。</p>
<p><strong><code>LN</code>：</strong> 避开了batch维度，归一化的维度为 <strong>[C，H，W]</strong></p>
<p><strong><code>IN</code>：</strong> 归一化维度为 <strong>[H, W]</strong></p>
<p><strong><code>GN</code>：</strong> GN 介于 LN 和 IN 之间，其首先将channel分为许多组（group），对每一组做归一化，即先将feature的维度由[N, C, H, W] reshape为 [N*G，C//G , H, W]，归一化的维度为 <strong>[C//G , H, W]</strong> 。GN相当于特征的group归一化，其对batch_size更鲁棒</p>
<p>事实上，GN 的极端情况就是 LN 和 IN，分别对应G等于1和G等于C</p>
<hr>
<h3 id="caffe">Caffe</h3>
<h4 id="caffe卷积层的实现">Caffe卷积层的实现</h4>
<p>Caffe的卷积层实现，使用 <code>im2col</code> 操作，将数据以及卷积核分别转换成新的矩阵，然后将两对矩阵进行内积运算（inner product)。这样做，比原始的卷积操作速度更快。</p>
<div class="figure">
<img src="/2019/05/08/Try-your-best!You-will-find-the-job/caffe_conv_02.jpg">

</div>
<p>其中 <code>im2col</code> : 将一个大矩阵，重叠地划分为多个子矩阵，对每个子矩阵序列化成向量，最后得到另外一个矩阵。</p>
<div class="figure">
<img src="/2019/05/08/Try-your-best!You-will-find-the-job/caffe_conv_01.png">

</div>
<hr>
<h4 id="梯度下降与正规方程的比较">梯度下降与正规方程的比较</h4>
<table style="width:89%;">
<colgroup>
<col width="31%">
<col width="56%">
</colgroup>
<thead>
<tr class="header">
<th align="center">梯度下降(Gradient Descent)</th>
<th align="center">正规方程(Normal Equation)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">需要选择学习率 <span class="math inline">\(\alpha\)</span></td>
<td align="center">不需要</td>
</tr>
<tr class="even">
<td align="center">需要多次迭代</td>
<td align="center">一次运算得出</td>
</tr>
<tr class="odd">
<td align="center">当特征数量 n 大时，也能较好适用</td>
<td align="center">需要计算 <span class="math inline">\((X^{T}X)^{-1}\)</span><br>如果特征数量 n 较大则运算代价大，因为矩阵逆的计算时间复杂度为 <span class="math inline">\(O(n^3)\)</span> ，通常来说当n小于10000时还是可以接受的</td>
</tr>
<tr class="even">
<td align="center">适用于各种类型的模型</td>
<td align="center">只适用于线性模型，不适合逻辑回归等其他模型</td>
</tr>
</tbody>
</table>
<h3 id="面试题目">面试题目</h3>
<h4 id="广义逆阵"><a href="https://zh.wikipedia.org/wiki/%E5%B9%BF%E4%B9%89%E9%80%86%E9%98%B5" target="_blank" rel="noopener">广义逆阵</a></h4>
<p>关于矩阵的广义逆，下列表述不正确的是：</p>
<ul>
<li><strong>提出广义逆阵的原因</strong></li>
</ul>
<p>考虑以下的线性方程<br>
<span class="math display">\[
Ax = y
\]</span><br>
其中 <span class="math inline">\(A\)</span> 为 <span class="math inline">\(n \times m\)</span> 的矩阵，而 <span class="math inline">\(y \in \mathcal{R}(A), A\)</span> 的列空间。若矩阵 <span class="math inline">\(A\)</span> 为 <span class="math inline">\(A\)</span> 为可逆矩阵，则 <span class="math inline">\(x = A^{-1}y\)</span> 即为方程式的解。而若矩阵 <span class="math inline">\(A\)</span> 为可逆矩阵，则有：<br>
<span class="math display">\[
AA^{-1}A = A
\]</span><br>
假设矩阵 <span class="math inline">\(A\)</span> 不是可逆或是 <span class="math inline">\(n \neq m\)</span> ，需要一个合适的 <span class="math inline">\(m \times n\)</span> 矩阵 <span class="math inline">\(G\)</span> 使得下式成立：<br>
<span class="math display">\[
AGy = y
\]</span><br>
因为 <span class="math inline">\(Gy\)</span> 为线性系统 <span class="math inline">\(Ax = y\)</span> 的解。而同样的， <span class="math inline">\(m \times n\)</span> 的阶的矩阵 <span class="math inline">\(G\)</span> 也会使下式成立：<br>
<span class="math display">\[
AGA = A
\]</span><br>
因此可以用以下的方式定义<strong>广义逆阵</strong>：假设一个 <span class="math inline">\(n \times m\)</span> 的矩阵 <span class="math inline">\(A\)</span> ，<span class="math inline">\(m \times n\)</span> 的矩阵 <span class="math inline">\(G\)</span> 若可以使下式成立，矩阵 <span class="math inline">\(G\)</span> 即为 <span class="math inline">\(A\)</span> 的广义逆阵。<br>
<span class="math display">\[
AGA = A
\]</span></p>
<ul>
<li><strong>产生广义逆阵</strong></li>
</ul>
<p>以下是一种产生广义逆阵的方式[3]：</p>
<ol style="list-style-type: decimal">
<li>若 <span class="math inline">\(A=BC\)</span> 为其秩分解，则 <span class="math inline">\(G=C_{r}^{-}B_{l}^{-}\)</span> 为 <span class="math inline">\(A\)</span> 的广义逆阵，其中 <span class="math inline">\(C_{r}^{-}\)</span> 为 <span class="math inline">\(C\)</span> 的右逆矩阵，而 <span class="math inline">\(B_{l}^{-}\)</span> 为 <span class="math inline">\(B\)</span> 的左逆矩阵。</li>
<li>若 <span class="math inline">\(A=P{\begin{bmatrix}I_{r}&amp;0\\0&amp;0\end{bmatrix}}Q\)</span> ，其中 <span class="math inline">\(P\)</span> 及 <span class="math inline">\(Q\)</span> 为可逆矩阵，则 <span class="math inline">\(G=Q^{-1}{\begin{bmatrix}I_{r}&amp;U\\W&amp;V\end{bmatrix}}P^{-1}\)</span> 是 <span class="math inline">\(A\)</span> 的广义逆阵，其中 <span class="math inline">\(U,V\)</span> 及 <span class="math inline">\(W\)</span> 均为任意矩阵。</li>
<li>令 <span class="math inline">\(A\)</span> 为秩为 <span class="math inline">\(r\)</span> 的矩阵，在不失一般性的情形下，令 <span class="math inline">\(A={\begin{bmatrix}B&amp;C\\D&amp;E\end{bmatrix}}\)</span> ，其中 <span class="math inline">\(B_{r\times r}\)</span> 为 <span class="math inline">\(A\)</span> 的可逆子矩阵，则 <span class="math inline">\(G={\begin{bmatrix}B^{-1}&amp;0\\0&amp;0\end{bmatrix}}\)</span> 为 <span class="math inline">\(A\)</span> 的广义逆阵。</li>
</ol>
<ul>
<li><strong>广义逆阵的种类</strong></li>
</ul>
<p>彭若斯条件可以用来定义不同的广义逆阵：针对 <span class="math inline">\(A \in \mathbb{R}^{n \times m} 及 A^{\mathrm{g}} \in \mathbb{R}^{m \times n}\)</span> ，<br>
<span class="math display">\[
\begin{array}{l}{\text { 1.) } A A^{\mathrm{g}} A=A} \\ {\text { 2.) } A^{\mathrm{g}} A A^{\mathrm{g}}=A^{\mathrm{g}}} \\ {\text { 3.) }\left(A A^{\mathrm{g}}\right)^{\mathrm{T}}=A A^{\mathrm{g}}} \\ {\text { 4.) }\left(A^{\mathrm{g}} A\right)^{\mathrm{T}}=A^{\mathrm{g}} A}\end{array}
\]</span><br>
若 <span class="math inline">\(A^{\mathrm{g}}\)</span> 满足条件(1.)，即为 <span class="math inline">\(A\)</span> 的广义逆阵，若满足条件 (1.) 和 (2.)，则为 <span class="math inline">\(A\)</span> 的广义反身逆阵。若四个条件都满足，则为 <span class="math inline">\(A\)</span> 的<a href="https://zh.wikipedia.org/wiki/%E6%91%A9%E5%B0%94%EF%BC%8D%E5%BD%AD%E8%8B%A5%E6%96%AF%E5%B9%BF%E4%B9%89%E9%80%86" target="_blank" rel="noopener">摩尔－彭若斯广义逆</a>。</p>
<hr>
<h3 id="待整理">待整理</h3>
<h4 id="为什么用smoothl1-loss而不用-l2-loss">为什么用SmoothL1 loss而不用 L2 loss</h4>
<p><span class="math inline">\(Smooth_{L_1} Loss\)</span> 对于离群点更加鲁棒，其相比于 <span class="math inline">\(L2\)</span> 损失函数，其对离群点，异常值(outlier)不敏感，可控制梯度的量级使训练时不容易跑飞，从梯度的角度进行分析。</p>
<ul>
<li><a href="https://www.zhihu.com/question/58200555" class="uri" target="_blank" rel="noopener">https://www.zhihu.com/question/58200555</a></li>
</ul>
<h4 id="x1卷积核作用">1x1卷积核作用</h4>
<p>进行<strong>升维</strong>和<strong>降维</strong>的作用，也就是通过控制卷积核（通道数）实现。这个可以帮助减少模型参数，也可以对不同特征进行尺寸的归一化；同时也可以用于不同channel上特征的融合。如，RPN网络中，网络输出，接入了两个并行的1x1的卷积层，来回归anchor的所属类别以及需要做的平移缩放参数。conv5_3 输出[38x50x512]，输入到cls_layer 得到 [38x50x18]，以及 loc_layer 得到 [38x50x4].</p>
<h4 id="svm-和-softmax-区别">SVM 和 Softmax 区别</h4>
<h4 id="仿射变换和透视变化区别">仿射变换和透视变化区别</h4>
<h4 id="透视变换矩阵的形状">透视变换矩阵的形状</h4>
<h4 id="常用的算子">常用的算子</h4>
<h4 id="梯度消失和梯度爆炸的原因和解决方法">梯度消失和梯度爆炸的原因和解决方法</h4>
<h4 id="svmlr-和-softmax-区别">SVM、LR 和 Softmax 区别</h4>
<h4 id="svm寻参问题">SVM寻参问题</h4>
<h4 id="svm核函数解释">SVM核函数解释</h4>
<h4 id="softmax-公式以及反向传播">Softmax 公式，以及反向传播</h4>
<h4 id="交叉熵公式以及方向传播">交叉熵公式，以及方向传播</h4>
<h4 id="交叉熵损失-可以用-l2损失替换么">交叉熵损失 可以用 L2损失替换么</h4>
<h4 id="先把yolo看完500问中有检测论文需要看的顺序">先把YOLO看完（500问中有检测论文需要看的顺序）</h4>
<h4 id="简述-yolo-和-ssd">简述 YOLO 和 SSD</h4>
<h4 id="yolov2"><a href="https://zhuanlan.zhihu.com/p/25167153" target="_blank" rel="noopener">YOLOv2</a></h4>
<h4 id="batch_size-和-learning-rate的关系怎么平衡和调整二者">batch_size 和 learning rate的关系（怎么平衡和调整二者）</h4>
<h4 id="inception-v1-v4的演变">Inception v1-v4的演变</h4>
<h4 id="简述-cnn-的演变">简述 CNN 的演变</h4>
<h4 id="roi-pooling-和-roi-align">ROI Pooling 和 ROI Align</h4>
<h4 id="优化算法区别以及各自的优势">优化算法，区别以及各自的优势</h4>
<h4 id="cnn为什么有效-1-2">CNN为什么有效 <a href="https://lguduy.github.io/2017/07/02/CNN%E4%B8%BA%E4%BB%80%E4%B9%88work/" target="_blank" rel="noopener">1</a> <a href="https://www.zhihu.com/question/39022858" target="_blank" rel="noopener">2</a></h4>
<h4 id="cnn再图像上表现好的原因">CNN再图像上表现好的原因</h4>
<h4 id="对迁移学习的理解为什么能work">对迁移学习的理解，为什么能work</h4>
<h4 id="实现一个卷积操作"><a href="https://blog.csdn.net/huachao1001/article/details/79120521" target="_blank" rel="noopener">实现一个卷积操作</a></h4>
<h4 id="模型精简加速">模型精简加速</h4>
<h4 id="图像处理中的常用算子"><a href="https://blog.csdn.net/yyywww666/article/details/78117595" target="_blank" rel="noopener">图像处理中的常用算子</a></h4>
<h4 id="卷积与反卷积">卷积与反卷积</h4>
<h4 id="lstm-与-rnn-模型">LSTM 与 RNN 模型</h4>
<h4 id="lstm的结构其相对于rnn的好处">LSTM的结构，其相对于RNN的好处</h4>
<h4 id="bn原理与实战"><a href="https://zhuanlan.zhihu.com/p/34879333" target="_blank" rel="noopener">BN原理与实战</a></h4>
<hr>
<h3 id="面经集合">面经集合</h3>
<ul>
<li><a href="https://blog.csdn.net/liuxiao214/article/details/83043170" target="_blank" rel="noopener">超级多的面试相关题目，基本自己要都会才行！</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzUxNjcxMjQxNg==&amp;mid=2247489308&amp;idx=3&amp;sn=2ab79b30a0617acd27b4d0d008048a72&amp;chksm=f9a26593ced5ec85cde15405b917a4ad649bd3e17e1b6986220d9f0c3fb5440924e9f0ffdf9a&amp;mpshare=1&amp;scene=1&amp;srcid=&amp;pass_ticket=GNqW1dUZqNOlXRVxSJ2ZJyI8kYIhyz3BdG1xJe%2FAyL46O%2FfHd%2BkyZhc51dt3UTV1#rd" target="_blank" rel="noopener">跟自己很相关的一个面经,已经剪切到印象笔记之中</a></li>
<li><a href="https://blog.csdn.net/francislucien2017/article/details/87936928" target="_blank" rel="noopener">2019春 计算机视觉方向实习面试总结 （商汤 / 搜狗 / 纽劢 / 普华永道）</a></li>
<li><a href="https://www.jianshu.com/p/58855c6971e5" target="_blank" rel="noopener">2019秋招CV算法面经</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MjM5ODU3OTIyOA==&amp;mid=2650667647&amp;idx=1&amp;sn=142c2aa04e2af0f700a0c8bce08c1de0&amp;chksm=bec1c70c89b64e1a8e6fb091da6c74c152469469d2e28ee3301078602e8032eba8df75fc0b81&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">机器学习工程师面试!!!</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MjM5ODU3OTIyOA==&amp;mid=2650667685&amp;idx=1&amp;sn=b7e119cd87dd36361202aac99b73d8a9&amp;chksm=bec1c7d689b64ec0bbac4edec8b7102f38c751b21bd3789970527c13ccbb48c3403c74913180&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">深度学习面试你必须知道这些答案</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/32981626" target="_blank" rel="noopener">大佬面试总结：图像处理/CV/ML/DL到HR面</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MjM5ODU3OTIyOA==&amp;mid=2650668193&amp;idx=1&amp;sn=2efc45adea26c8ebd6efc5e14509180c&amp;chksm=bec1c1d289b648c43537fa297504f2cddf1568586da02becca515671fb095d7e591c33594c66&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">深度学习面试题目</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MjM5ODU3OTIyOA==&amp;mid=2650666049&amp;idx=1&amp;sn=94d9aca0a894418f4c66e9b5363e0498&amp;chksm=bec1c93289b640240af8f55c301a63897d0c52d3ea7b96da969ff160c17b2d52c4a04204634b&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">面试官如何判断面试者的机器学习水平？</a></li>
<li><a href="https://blog.csdn.net/yyywww666/article/details/78117595" target="_blank" rel="noopener">2017百度计算机视觉题目</a></li>
<li><a href="https://www.nowcoder.com/discuss/112562" target="_blank" rel="noopener">腾讯 MIG / 网易互娱 / 今日头条 从春招到秋招的面经</a></li>
<li><a href="https://www.nowcoder.com/discuss/128148" target="_blank" rel="noopener">计算机视觉岗位面经</a></li>
<li><a href="https://www.nowcoder.com/discuss/36815" target="_blank" rel="noopener">机器学习岗面试</a></li>
<li><a href="https://www.nowcoder.com/discuss/3453" target="_blank" rel="noopener">机器学习-相似性度量</a></li>
<li><a href="https://blog.csdn.net/zmazon/article/details/8262185" target="_blank" rel="noopener">优秀程序员不得不知道的20个位运算技巧</a></li>
</ul>
<hr>
<h3 id="c问题">C++问题</h3>
<ul>
<li>C++虚函数机制</li>
<li>C++中static作用</li>
<li>C++ 中的 new / delete 和 C 里的 malloc / free 的区别；</li>
<li><a href="https://www.cnblogs.com/clover-toeic/p/3853132.html" target="_blank" rel="noopener">结构体对齐</a></li>
<li>红黑树</li>
</ul>
<h4 id="c面向对象的三大特性">C++面向对象的三大特性</h4>
<ul>
<li><strong>封装</strong></li>
</ul>
<p>封装最好理解了。封装是面向对象的特征之一，是对象和类概念的主要特性。封装，也就是把客观事物封装成抽象的类，并且类可以把自己的数据和方法只让可信的类或者对象操作，对不可信的进行信息隐藏。</p>
<ul>
<li><strong>继承</strong></li>
</ul>
<p>面向对象编程 (OOP) 语言的一个主要功能就是“继承”。继承是指这样一种能力：它可以使用现有类的所有功能，并在无需重新编写原来的类的情况下对这些功能进行扩展。<br>
通过继承创建的新类称为“子类”或“派生类”。<br>
被继承的类称为“基类”、“父类”或“超类”。<br>
继承的过程，就是从一般到特殊的过程。<br>
要实现继承，可以通过“继承”（Inheritance）和“组合”（Composition）来实现。</p>
<p>继承概念的实现方式有三类：实现继承、接口继承和可视继承。</p>
<ul>
<li><strong>多态</strong></li>
</ul>
<p>多态性（polymorphisn）<strong>父类引用指向子类对象</strong>。是允许你将父对象设置成为和一个或更多的他的子对象相等的技术，赋值之后，父对象就可以根据当前赋值给它的子对象的特性以不同的方式运作。简单的说，就是一句话：允许将子类类型的指针赋值给父类类型的指针。<br>
实现多态有二种方式，<strong>覆盖</strong>，<strong>重载</strong>。<br>
（1）<strong>覆盖</strong>，是指子类重新定义父类的虚函数的做法。<br>
（2）<strong>重载</strong>，是指允许存在多个同名函数，而这些函数的参数表不同（或许参数个数不同，或许参数类型不同，或许两者都不同）</p>
<h3 id="python问题">Python问题</h3>
<ul>
<li>Python的多线程和多进程，Python伪多线程，那什么时候应该用它？（有空闲等待的情况）讲一下Java线程池（举了Android多线程的例子）</li>
<li>Python tuple和list的区别（只读和读写，什么时候用只读的容器？）</li>
<li>tensorflow while_loop和python for循环的区别，什么情况下for更优？</li>
</ul>
<hr>
<h3 id="面试算法题目">面试算法题目</h3>
<ul>
<li><p>给定一个词典，和两个单词A, B. 每次只能改变一个字母，求A在词典中变换为B所需的最小次数</p></li>
<li><p>数组形式的<code>a[i][j][k]</code> 改用指针形式来访问</p></li>
<li><p>有序数组，旋转后查找一个数</p></li>
<li><p>判断一个数是不是2的指数次方的值</p></li>
</ul>
<p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">如果是<span class="number">2</span>的倍数，那么该数的二进制位只有<span class="number">1</span>位为<span class="number">1</span>，所以只需要使用 n =  n &amp; (n - <span class="number">1</span>) 即可，其可以掉最右边的一个<span class="number">1</span>，若此时，n为<span class="number">0</span>，则符合，否则，不符合。</span><br></pre></td></tr></table></figure></p>
<ul>
<li><p>最长公共子串 (商汤)</p></li>
<li><p>找到一个数组中唯一一个出现奇数次数的数（商汤）</p></li>
<li><p>给两个有序数组，求第3大的数（头条）</p></li>
</ul>
<hr>
<h3 id="概率智力题">概率智力题</h3>
<ul>
<li>两个人拿金币，一次只能拿一个或者两个，最后拿光的人赢，怎样才能保证赢？</li>
<li>最大子矩阵和</li>
<li>有n瓶水，有一瓶有毒，用几只老鼠可以找出</li>
</ul>
<h3 id="自己整理的思维导图要每天看">自己整理的思维导图要每天看</h3>
<p><a href="https://blog.csdn.net/woaidapaopao/article/details/77806273" class="uri" target="_blank" rel="noopener">https://blog.csdn.net/woaidapaopao/article/details/77806273</a></p>
</body>
</html>
</span>
      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Machine-Learning/" rel="tag">#Machine Learning</a>
          
            <a href="/tags/Deeping-Learning/" rel="tag">#Deeping Learning</a>
          
            <a href="/tags/job/" rel="tag">#job</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/05/23/Softmax-and-Softmax-Loss-gradient-calcule/" rel="prev">
                <i class="fa fa-chevron-left"></i> 【转载】Softmax和Softmax-Loss函数及梯度计算
              </a>
            
          </div>

          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/04/29/Set-of-cpp/" rel="next">
                C++中的set <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>

 </div>

        

        
          <div class="comments" id="comments">
            
          </div>
        
      

        
          
  
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">
      
      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table Of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview" sidebar-panel >
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" src="/images/avatar.jpg" alt="SmileLingyong" itemprop="image"/>
          <p class="site-author-name" itemprop="name">SmileLingyong</p>
        </div>
        <p class="site-description motion-element" itemprop="description">向上，向阳！</p>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">23</span>
              <span class="site-state-item-name">posts</span>
            </a>
          </div>

          <div class="site-state-item site-state-categories">
            <a href="/categories">
              <span class="site-state-item-count">12</span>
              <span class="site-state-item-name">categories</span>
              </a>
          </div>

          <div class="site-state-item site-state-tags">
            <a href="/tags">
              <span class="site-state-item-count">18</span>
              <span class="site-state-item-name">tags</span>
              </a>
          </div>

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="smilelingyong@gmail.com" target="_blank">
                  <i class="fa fa-e-mail"></i> E-Mail
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://github.com/SmileLingyong" target="_blank">
                  <i class="fa fa-github"></i> Github
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://blog.csdn.net/forever__1234" target="_blank">
                  <i class="fa fa-csdn"></i> CSDN
                </a>
              </span>
            
          
        </div>

        
        

        <div class="links-of-author motion-element">
          
        </div>

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel-active">
          <div class="post-toc-indicator-top post-toc-indicator"></div>
          <div class="post-toc">
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#模型评估"><span class="nav-text">模型评估</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#模型评估常用方法"><span class="nav-text">模型评估常用方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#机器学习中的bias和variance有什么区别和联系"><span class="nav-text">机器学习中的Bias和Variance有什么区别和联系</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#经验误差与泛化误差"><span class="nav-text">经验误差与泛化误差</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#深度学习中的偏差与方差"><span class="nav-text">深度学习中的偏差与方差</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#偏差方差与boosting和bagging联系"><span class="nav-text">偏差方差与Boosting和Bagging联系</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#偏差与方差的计算公式"><span class="nav-text">偏差与方差的计算公式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#偏差与方差的权衡过拟合与模型复杂度的权衡"><span class="nav-text">偏差与方差的权衡（过拟合与模型复杂度的权衡）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#欠拟合与过拟合"><span class="nav-text">欠拟合与过拟合</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#根据不同的坐标方式图解欠拟合与过拟合"><span class="nav-text">根据不同的坐标方式，图解欠拟合与过拟合</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#降低-过拟合-的方法"><span class="nav-text">降低 过拟合 的方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#降低-欠拟合-的方法"><span class="nav-text">降低 欠拟合 的方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#l1l2-范数正则化"><span class="nav-text">L1/L2 范数正则化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#l1l2-范数的作用异同"><span class="nav-text">L1/L2 范数的作用、异同</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么-l1-和-l2-正则化可以防止过拟合"><span class="nav-text">为什么 L1 和 L2 正则化可以防止过拟合？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么-l1-正则化可以产生稀疏权值而-l2-不会"><span class="nav-text">为什么 L1 正则化可以产生稀疏权值，而 L2 不会？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#交叉验证的主要作用"><span class="nav-text">交叉验证的主要作用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#k-折交叉验证"><span class="nav-text">\(k\) 折交叉验证</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#precision和recall"><span class="nav-text">Precision和Recall</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#p-r曲线"><span class="nav-text">P-R曲线</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#roc与auc"><span class="nav-text">ROC与AUC</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#map"><span class="nav-text">mAP</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#激活函数"><span class="nav-text">激活函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么需要激活函数"><span class="nav-text">为什么需要激活函数？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么要使用非线性激活函数"><span class="nav-text">为什么要使用非线性激活函数？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#什么时候可以用线性激活函数"><span class="nav-text">什么时候可以用线性激活函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#常见的激活函数"><span class="nav-text">常见的激活函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么tanh收敛速度比sigmoid快"><span class="nav-text">为什么Tanh收敛速度比Sigmoid快</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#怎样理解-relu-0-时是非线性激活函数"><span class="nav-text">怎样理解 ReLU（&lt; 0 时）是非线性激活函数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#损失函数代价函数"><span class="nav-text">损失函数/代价函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么需要损失代价函数"><span class="nav-text">为什么需要损失/代价函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#损失代价函数的作用及原理"><span class="nav-text">损失/代价函数的作用及原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么损失代价函数要非负"><span class="nav-text">为什么损失/代价函数要非负</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#常用的损失函数"><span class="nav-text">常用的损失函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#熵条件熵kl散度交叉熵"><span class="nav-text">熵、条件熵、KL散度、交叉熵</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么用交叉熵代替二次代价均方误差函数"><span class="nav-text">为什么用交叉熵代替二次代价(均方误差)函数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度下降"><span class="nav-text">梯度下降</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#机器学习中为什么需要梯度下降"><span class="nav-text">机器学习中为什么需要梯度下降</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#梯度下降法缺点"><span class="nav-text">梯度下降法缺点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#梯度下降法直观理解"><span class="nav-text">梯度下降法直观理解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#梯度下降法算法描述"><span class="nav-text">梯度下降法算法描述</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#如何对梯度下降法进行调优"><span class="nav-text">如何对梯度下降法进行调优</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#随机梯度和批量梯度区别"><span class="nav-text">随机梯度和批量梯度区别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#各种梯度下降法性能比较"><span class="nav-text">各种梯度下降法性能比较</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#范数"><span class="nav-text">范数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#l-p范数"><span class="nav-text">L-P范数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#l0范数"><span class="nav-text">L0范数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#l1范数"><span class="nav-text">L1范数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#l2范数"><span class="nav-text">L2范数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#l-范数"><span class="nav-text">L-∞范数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#欧式距离-与-余弦相似度"><span class="nav-text">欧式距离 与 余弦相似度</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#逻辑回归"><span class="nav-text">逻辑回归</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#回归划分"><span class="nav-text">回归划分</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#逻辑回归适用性"><span class="nav-text">逻辑回归适用性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#逻辑回归与朴素贝叶斯的区别"><span class="nav-text">逻辑回归与朴素贝叶斯的区别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#逻辑回归-与-线性回归-的联系与区别"><span class="nav-text">逻辑回归 与 线性回归 的联系与区别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#逻辑回归-与-svm-的联系与区别"><span class="nav-text">逻辑回归 与 SVM 的联系与区别 *</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#集成学习"><span class="nav-text">集成学习</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#集成学习的基本思想"><span class="nav-text">集成学习的基本思想</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#集成学习为什么有效"><span class="nav-text">集成学习为什么有效</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#boosting-方法"><span class="nav-text">Boosting 方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#bagging-方法booststrap-aggregating"><span class="nav-text">Bagging 方法（Booststrap AGGregatING）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么使用决策树作为基学习器"><span class="nav-text">为什么使用决策树作为基学习器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么不稳定的学习器更适合作为基学习器"><span class="nav-text">为什么不稳定的学习器更适合作为基学习器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#还有哪些模型也适合作为基学习器"><span class="nav-text">还有哪些模型也适合作为基学习器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#boosting-方法中能使用线性分类器作为基学习器吗-bagging-呢"><span class="nav-text">Boosting 方法中能使用线性分类器作为基学习器吗？ Bagging 呢？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#boostingbagging-与-偏差方差-的关系"><span class="nav-text">Boosting/Bagging 与 偏差/方差 的关系</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#bagging与dropout区别与联系"><span class="nav-text">Bagging与Dropout区别与联系</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#adaboost-算法"><span class="nav-text">AdaBoost 算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#gbdt-算法"><span class="nav-text">GBDT 算法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#归一化"><span class="nav-text">归一化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#batch-normalization批标准化"><span class="nav-text">Batch Normalization(批标准化)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#bn层作用以及如何使用bn层"><span class="nav-text">BN层作用，以及如何使用BN层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#动机"><span class="nav-text">动机</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#基本原理"><span class="nav-text">基本原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么训练时不采用移动平均"><span class="nav-text">为什么训练时不采用移动平均？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#bnlningn的异同"><span class="nav-text">BN、LN、IN、GN的异同</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#caffe"><span class="nav-text">Caffe</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#caffe卷积层的实现"><span class="nav-text">Caffe卷积层的实现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#梯度下降与正规方程的比较"><span class="nav-text">梯度下降与正规方程的比较</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#面试题目"><span class="nav-text">面试题目</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#广义逆阵"><span class="nav-text">广义逆阵</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#待整理"><span class="nav-text">待整理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么用smoothl1-loss而不用-l2-loss"><span class="nav-text">为什么用SmoothL1 loss而不用 L2 loss</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#x1卷积核作用"><span class="nav-text">1x1卷积核作用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#svm-和-softmax-区别"><span class="nav-text">SVM 和 Softmax 区别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#仿射变换和透视变化区别"><span class="nav-text">仿射变换和透视变化区别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#透视变换矩阵的形状"><span class="nav-text">透视变换矩阵的形状</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#常用的算子"><span class="nav-text">常用的算子</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#梯度消失和梯度爆炸的原因和解决方法"><span class="nav-text">梯度消失和梯度爆炸的原因和解决方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#svmlr-和-softmax-区别"><span class="nav-text">SVM、LR 和 Softmax 区别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#svm寻参问题"><span class="nav-text">SVM寻参问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#svm核函数解释"><span class="nav-text">SVM核函数解释</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#softmax-公式以及反向传播"><span class="nav-text">Softmax 公式，以及反向传播</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#交叉熵公式以及方向传播"><span class="nav-text">交叉熵公式，以及方向传播</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#交叉熵损失-可以用-l2损失替换么"><span class="nav-text">交叉熵损失 可以用 L2损失替换么</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#先把yolo看完500问中有检测论文需要看的顺序"><span class="nav-text">先把YOLO看完（500问中有检测论文需要看的顺序）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#简述-yolo-和-ssd"><span class="nav-text">简述 YOLO 和 SSD</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#yolov2"><span class="nav-text">YOLOv2</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#batch_size-和-learning-rate的关系怎么平衡和调整二者"><span class="nav-text">batch_size 和 learning rate的关系（怎么平衡和调整二者）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#inception-v1-v4的演变"><span class="nav-text">Inception v1-v4的演变</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#简述-cnn-的演变"><span class="nav-text">简述 CNN 的演变</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#roi-pooling-和-roi-align"><span class="nav-text">ROI Pooling 和 ROI Align</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#优化算法区别以及各自的优势"><span class="nav-text">优化算法，区别以及各自的优势</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#cnn为什么有效-1-2"><span class="nav-text">CNN为什么有效 1 2</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#cnn再图像上表现好的原因"><span class="nav-text">CNN再图像上表现好的原因</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#对迁移学习的理解为什么能work"><span class="nav-text">对迁移学习的理解，为什么能work</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#实现一个卷积操作"><span class="nav-text">实现一个卷积操作</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#模型精简加速"><span class="nav-text">模型精简加速</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#图像处理中的常用算子"><span class="nav-text">图像处理中的常用算子</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#卷积与反卷积"><span class="nav-text">卷积与反卷积</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#lstm-与-rnn-模型"><span class="nav-text">LSTM 与 RNN 模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#lstm的结构其相对于rnn的好处"><span class="nav-text">LSTM的结构，其相对于RNN的好处</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#bn原理与实战"><span class="nav-text">BN原理与实战</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#面经集合"><span class="nav-text">面经集合</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#c问题"><span class="nav-text">C++问题</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#c面向对象的三大特性"><span class="nav-text">C++面向对象的三大特性</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#python问题"><span class="nav-text">Python问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#面试算法题目"><span class="nav-text">面试算法题目</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#概率智力题"><span class="nav-text">概率智力题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#自己整理的思维导图要每天看"><span class="nav-text">自己整理的思维导图要每天看</span></a></li></ol></div>
            
          </div>
          <div class="post-toc-indicator-bottom post-toc-indicator"></div>
        </section>
      

    </div>
  </aside>


        
	  </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner"> <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="icon-next-heart fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">SmileLingyong</span>
</div>

<div class="powered-by">
  Powered by <a class="theme-link" href="http://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="#">
    FreeSky
  </a>(Reserved)

  
  <span id="busuanzi_container_site_uv">
     &nbsp; | &nbsp;  用户量: <span id="busuanzi_value_site_uv"></span>
  </span>
  <span id="busuanzi_container_site_pv">
    &nbsp; | &nbsp;  总访问量: <span id="busuanzi_value_site_pv"></span>
  </span>

  
</div>


<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
 </div>
    </footer>

    <div class="back-to-top"></div>
  </div>

  <script type="text/javascript" src="/others/jquery/index.js?v=2.1.3"></script>

  
  
  
    
    

  


  
  
  <script type="text/javascript" src="/others/fancybox/source/jquery.fancybox.pack.js"></script>
  <script type="text/javascript" src="/js/fancy-box.js?v=0.4.5.2"></script>


  <script type="text/javascript" src="/js/helpers.js?v=0.4.5.2"></script>
  

  <script type="text/javascript" src="/others/velocity/velocity.min.js"></script>
  <script type="text/javascript" src="/others/velocity/velocity.ui.min.js"></script>

  <script type="text/javascript" src="/js/motion_global.js?v=0.4.5.2" id="motion.global"></script>




  <script type="text/javascript" src="/js/nav-toggle.js?v=0.4.5.2"></script>
  <script type="text/javascript" src="/others/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  
<script type="text/javascript" src="/js/bootstrap.scrollspy.js?v=0.4.5.2" id="bootstrap.scrollspy.custom"></script>


<script type="text/javascript" id="sidebar.toc.highlight">
  $(document).ready(function () {
    var tocSelector = '.post-toc';
    var $tocSelector = $(tocSelector);
    var activeCurrentSelector = '.active-current';

    $tocSelector
      .on('activate.bs.scrollspy', function () {
        var $currentActiveElement = $(tocSelector + ' .active').last();

        removeCurrentActiveClass();
        $currentActiveElement.addClass('active-current');

        $tocSelector[0].scrollTop = $currentActiveElement.position().top;
      })
      .on('clear.bs.scrollspy', function () {
        removeCurrentActiveClass();
      });

    function removeCurrentActiveClass () {
      $(tocSelector + ' ' + activeCurrentSelector)
        .removeClass(activeCurrentSelector.substring(1));
    }

    function processTOC () {
      getTOCMaxHeight();
      toggleTOCOverflowIndicators();
    }

    function getTOCMaxHeight () {
      var height = $('.sidebar').height() -
                   $tocSelector.position().top -
                   $('.post-toc-indicator-bottom').height();

      $tocSelector.css('height', height);

      return height;
    }

    function toggleTOCOverflowIndicators () {
      tocOverflowIndicator(
        '.post-toc-indicator-top',
        $tocSelector.scrollTop() > 0 ? 'show' : 'hide'
      );

      tocOverflowIndicator(
        '.post-toc-indicator-bottom',
        $tocSelector.scrollTop() >= $tocSelector.find('ol').height() - $tocSelector.height() ? 'hide' : 'show'
      )
    }

    $(document).on('sidebar.motion.complete', function () {
      processTOC();
    });

    $('body').scrollspy({ target: tocSelector });
    $(window).on('resize', function () {
      if ( $('.sidebar').hasClass('sidebar-active') ) {
        processTOC();
      }
    });

    onScroll($tocSelector);

    function onScroll (element) {
      element.on('mousewheel DOMMouseScroll', function (event) {
          var oe = event.originalEvent;
          var delta = oe.wheelDelta || -oe.detail;

          this.scrollTop += ( delta < 0 ? 1 : -1 ) * 30;
          event.preventDefault();

          toggleTOCOverflowIndicators();
      });
    }

    function tocOverflowIndicator (indicator, action) {
      var $indicator = $(indicator);
      var opacity = action === 'show' ? 0.4 : 0;
      $indicator.velocity ?
        $indicator.velocity('stop').velocity({
          opacity: opacity
        }, { duration: 100 }) :
        $indicator.stop().animate({
          opacity: opacity
        }, 100);
    }

  });
</script>

<script type="text/javascript" id="sidebar.nav">
  $(document).ready(function () {
    var html = $('html');
    var TAB_ANIMATE_DURATION = 200;
    var hasVelocity = $.isFunction(html.velocity);

    $('.sidebar-nav li').on('click', function () {
      var item = $(this);
      var activeTabClassName = 'sidebar-nav-active';
      var activePanelClassName = 'sidebar-panel-active';
      if (item.hasClass(activeTabClassName)) {
        return;
      }

      var currentTarget = $('.' + activePanelClassName);
      var target = $('.' + item.data('target'));

      hasVelocity ?
        currentTarget.velocity('transition.slideUpOut', TAB_ANIMATE_DURATION, function () {
          target
            .velocity('stop')
            .velocity('transition.slideDownIn', TAB_ANIMATE_DURATION)
            .addClass(activePanelClassName);
        }) :
        currentTarget.animate({ opacity: 0 }, TAB_ANIMATE_DURATION, function () {
          currentTarget.hide();
          target
            .stop()
            .css({'opacity': 0, 'display': 'block'})
            .animate({ opacity: 1 }, TAB_ANIMATE_DURATION, function () {
              currentTarget.removeClass(activePanelClassName);
              target.addClass(activePanelClassName);
            });
        });

      item.siblings().removeClass(activeTabClassName);
      item.addClass(activeTabClassName);
    });

    $('.post-toc a').on('click', function (e) {
      e.preventDefault();
      var targetSelector = escapeSelector(this.getAttribute('href'));
      var offset = $(targetSelector).offset().top;
      hasVelocity ?
        html.velocity('stop').velocity('scroll', {
          offset: offset  + 'px',
          mobileHA: false
        }) :
        $('html, body').stop().animate({
          scrollTop: offset
        }, 500);
    });

    // Expand sidebar on post detail page by default, when post has a toc.
    var $tocContent = $('.post-toc-content');
    var $aboutContent = $('#posts-about');
    if (isDesktop() && CONFIG.sidebar === 'post') {
      if ($tocContent.length > 0 && $tocContent.html().trim().length > 0 && $aboutContent.length === 0) {
        displaySidebar();
      }
    }
  });
</script>



  <script type="text/javascript">
    $(document).ready(function () {
      if (CONFIG.sidebar === 'always') {
        displaySidebar();
      }
      if (isMobile()) {
        FastClick.attach(document.body);
      }

      motionIntegrator.bootstrap();
    });
  </script>

  
  

  
  

  
  <script type="text/javascript" src="/js/lazyload.js"></script>
  <script type="text/javascript">
    $(function () {
      $("#posts").find('img').lazyload({
        placeholder: "/images/loading.gif",
        effect: "fadeIn"
      });
    });
  </script>
  
     <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
<script>AV.initialize("9QoQXWnRR4zwSFuxRv52kUpi-gzGzoHsz", "zlgcRzgHF7AHu8TKLJUwCAjw");</script>
<script>
function showTime(Counter) {
  var query = new AV.Query(Counter);
  $(".leancloud_visitors").each(function() {
    var url = $(this).attr("id").trim();
    query.equalTo("url", url);
    query.find({
      success: function(results) {
        if (results.length == 0) {
          var content = $(document.getElementById(url)).text() + ': 0';
          $(document.getElementById(url)).text(content);
          return;
        }
        for (var i = 0; i < results.length; i++) {
          var object = results[i];
          var content = $(document.getElementById(url)).text() + ': ' + object.get('time');
          $(document.getElementById(url)).text(content);
        }
      },
      error: function(object, error) {
        console.log("Error: " + error.code + " " + error.message);
      }
    });

  });
}

function addCount(Counter) {
  var Counter = AV.Object.extend("Counter");
  url = $(".leancloud_visitors").attr('id').trim();
  title = $(".leancloud_visitors").attr('data-flag-title').trim();
  var query = new AV.Query(Counter);
  query.equalTo("url", url);
  query.find({
    success: function(results) {
      if (results.length > 0) {
        var counter = results[0];
        counter.fetchWhenSave(true);
        counter.increment("time");
        counter.save(null, {
          success: function(counter) {
            var content = $(document.getElementById(url)).text() + ': ' + counter.get('time');
            $(document.getElementById(url)).text(content);
          },
          error: function(counter, error) {
            console.log('Failed to save Visitor num, with error message: ' + error.message);
          }
        });
      } else {
        var newcounter = new Counter();
        newcounter.set("title", title);
        newcounter.set("url", url);
        newcounter.set("time", 1);
        newcounter.save(null, {
          success: function(newcounter) {
              console.log("newcounter.get('time')="+newcounter.get('time'));
            var content = $(document.getElementById(url)).text() + ': ' + newcounter.get('time');
            $(document.getElementById(url)).text(content);
          },
          error: function(newcounter, error) {
            console.log('Failed to create');
          }
        });
      }
    },
    error: function(error) {
      console.log('Error:' + error.code + " " + error.message);
    }
  });
}
$(function() {
  var Counter = AV.Object.extend("Counter");
  if ($('.leancloud_visitors').length == 1) {
    addCount(Counter);
  } else if ($('.post-title-link').length > 1) {
    showTime(Counter);
  }
}); 
</script>
  
</body>
</html>
